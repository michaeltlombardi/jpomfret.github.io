<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>powershell on Jess Pomfret</title><link>https://jpomfret.github.io/categories/powershell/</link><description>Recent content in powershell on Jess Pomfret</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Thu, 07 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jpomfret.github.io/categories/powershell/index.xml" rel="self" type="application/rss+xml"/><item><title>Run ActiveDirectory PowerShell commands against another domain</title><link>https://jpomfret.github.io/ad-powershell-domain/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/ad-powershell-domain/</guid><description>&lt;img src="https://jpomfret.github.io/ad-powershell-domain/cover.jpg" alt="Featured image of post Run ActiveDirectory PowerShell commands against another domain" />&lt;p>Active Directory groups are used all over our IT estates. They can be used to simplify managing SQL Server access (&lt;a class="link" href="https://jesspomfret.com/sql-server-permissions-via-ad/" target="_blank" rel="noopener"
>Discover SQL Server Permissions hidden via AD Group Membership&lt;/a>) as well as for other applications. One of my favourite commands from the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/activedirectory/?view=windowsserver2022-ps" target="_blank" rel="noopener"
>ActiveDirectory PowerShell module&lt;/a> is &lt;code>Get-AdUser&lt;/code>, specifically when used in the following snippet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-ADUser&lt;/span> &lt;span class="n">UserName&lt;/span> &lt;span class="n">-Properties&lt;/span> &lt;span class="n">MemberOf&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="nb">Select-Object&lt;/span> &lt;span class="n">-ExpandProperty&lt;/span> &lt;span class="n">MemberOf&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This snippet will list all the groups the user is in. Super useful for troubleshooting permissions issues or if you’re onboarding a new employee and want to see what groups their peers are in. But what happens if your environment consists of multiple domains, and you have a query about a user in another domain?&lt;/p>
&lt;p>Well good news - here’s the answer!&lt;/p>
&lt;p>First we need to know a little about the other domain, specifically the name of a domain controller in that domain. We can find that out by running the following in a console:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="n">PS&lt;/span>&lt;span class="p">&amp;gt;&lt;/span> &lt;span class="n">nltest&lt;/span> &lt;span class="p">/&lt;/span>&lt;span class="n">dclist&lt;/span>&lt;span class="err">:&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Get&lt;/span> &lt;span class="n">list&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">DCs&lt;/span> &lt;span class="k">in&lt;/span> &lt;span class="n">domain&lt;/span> &lt;span class="s1">&amp;#39;otherdomain.com&amp;#39;&lt;/span> &lt;span class="n">from&lt;/span> &lt;span class="s1">&amp;#39;\\\\DC1.otherdomain.com&amp;#39;&lt;/span>&lt;span class="p">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">PDC&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC3&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">The&lt;/span> &lt;span class="n">command&lt;/span> &lt;span class="n">completed&lt;/span> &lt;span class="n">successfully&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The results are shown above, and in this example you can see there are three domain controllers. We can pick any for the next step. Once we have the domain controller we just need to add the &lt;code>-Server&lt;/code> parameter to our original snippet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-ADUser&lt;/span> &lt;span class="n">AnotherUserName&lt;/span> &lt;span class="n">-Server&lt;/span> &lt;span class="n">DC1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="n">-Properties&lt;/span> &lt;span class="n">MemberOf&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Select-Object&lt;/span> &lt;span class="n">-ExpandProperty&lt;/span> &lt;span class="n">MemberOf&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This will now return all the groups that &lt;code>AnotherUserName@otherdomain.com&lt;/code> is in.&lt;/p>
&lt;p>For full transparency, I used this tip a lot at a previous job but today when I needed it I couldn’t remember how to get the name of the AD controller. So this blog is for future Jess, because let’s be honest, I’ll be back here soon.&lt;/p></description></item><item><title>Log Shipping – Pre-stage database backups with dbatools</title><link>https://jpomfret.github.io/log-ship-staged/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/log-ship-staged/</guid><description>&lt;img src="https://jpomfret.github.io/log-ship-staged/cover.jpg" alt="Featured image of post Log Shipping – Pre-stage database backups with dbatools" />&lt;p>Log shipping is a SQL Server feature used for disaster-recovery where the transaction log backups are ‘shipped’ from your production instance to a secondary instance. This enables you to cutover to this secondary server in the event of a disaster where you lose your primary instance. Log shipping is not a new feature but is still quite popular.&lt;/p>
&lt;p>Recently I was tasked with setting up Log Shipping for a reasonably large production database. I didn’t want to initialize the secondary database with a new full backup as I was already taking full and log backups of the database. In this case we have the option of initialising the database by restoring the full &amp;amp; log backups up to the latest point in time and then configuring log shipping.&lt;/p>
&lt;h2 id="which-backups-to-restore">Which backups to restore?&lt;/h2>
&lt;p>In order for us to stage the database on the secondary at the point in time where we can configure log shipping we need to get the last full backup and any log backups taken since then.  If we were using differential backups as part of our strategy we would need the last full, the latest differential, and any log backups since then.&lt;/p>
&lt;p>This could be a lot of backup files to find, put in the right order, and then restore (with no recovery) onto the secondary server. dbatools makes this so easy! We can use &lt;code>Get-DbaDbBackupHistory&lt;/code> with the &lt;code>-Last&lt;/code> switch to get the latest backup chain. Then by piping that to &lt;code>Restore-DbaDatabase&lt;/code> we can automatically restore each piece of the puzzle. First, the full backups and then any differentials or log backups we need to get us to the point in time we are now.&lt;/p>
&lt;p>Get-DbaDbBackupHistory -SqlInstance mssql1 -Database productionDb -Last |
Restore-DbaDatabase -SqlInstance mssql2 -NoRecovery -UseDestinationDefaultDirectories&lt;/p>
&lt;p>Depending on how long the restores take you might have new log backups to apply to the secondary database, like those that have been taken on the primary since we ran the last command.  Again, we can use dbatools to help us with this. I will execute the same call to &lt;code>Get-DbaDbBackupHistory&lt;/code> to get the last backup chain, but instead of piping it straight to &lt;code>Restore-DbaDatabase&lt;/code> I will use &lt;code>Out-GridView&lt;/code> with the &lt;code>-PassThru&lt;/code> switch to effectively create a GUI window where I can select the backups I want to restore (any since the last log backup we applied), and then pass them on down the pipeline to be restored by &lt;code>Restore-DbaDatabase&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-DbaDbBackupHistory&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql1&lt;/span> &lt;span class="n">-Database&lt;/span> &lt;span class="n">productionDb&lt;/span> &lt;span class="n">-Last&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Out-GridView&lt;/span> &lt;span class="n">-PassThru&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Restore-DbaDatabase&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql2&lt;/span> &lt;span class="n">-NoRecovery&lt;/span> &lt;span class="n">-UseDestinationDefaultDirectories&lt;/span> &lt;span class="n">-Continue&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="check-on-what-we-restored">Check on what we restored&lt;/h2>
&lt;p>Once the restores are complete we can view what was restored using &lt;code>Get-DbaDbRestoreHistory&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-DbaDbRestoreHistory&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql2&lt;/span> &lt;span class="n">-Database&lt;/span> &lt;span class="n">productionDb&lt;/span> &lt;span class="n">-Last&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>At this point our secondary database has been initalised and we’re ready to set up log shipping. You can use the GUI in SSMS for this, or I’d recommend taking a look at dbatools offering &lt;code>Invoke-DbaDbLogShipping&lt;/code>.&lt;/p></description></item><item><title>Collating index usage stats across Availability Group replicas</title><link>https://jpomfret.github.io/collating-index-usage-stats-across-availability-group-replicas/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/collating-index-usage-stats-across-availability-group-replicas/</guid><description>&lt;p>One of the benefits available to us when using SQL Server Availability Groups is that we can offload read activity to a secondary replica. This can be useful if we need to run reports against our OLTP databases. Instead of this taking up valuable resources on the primary instance we can make use of the otherwise idle secondary replica.&lt;/p>
&lt;p>Note: This could affect your licensing standpoint, so ensure you’re in compliance on that front.&lt;/p>
&lt;p>Last week, I was working on a project to analyse indexes on a database that was part of an availability group. The main goal was to find unused indexes that could be removed, but I was also interested in gaining an overall understanding of how the system was indexed.&lt;/p>
&lt;p>Unused indexes not only take up disk space, but they also add overhead to write operations and require maintenance which can add additional load on your system.  We can also use this analysis to look for a high number of lookups which could indicate we need to adjust indexes slightly.&lt;/p>
&lt;p>&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: dbatools does have a command called &lt;code>Find-DbaDbUnusedIndex&lt;/code> to just look for unused indexes – however since I wanted to collect overall usage as well it wasn’t appropriate in this situation.&lt;/p>
&lt;p>dbatools has a command &lt;code>Get-DbaHelpIndex&lt;/code> which returns detailed information on our indexes which we can then use to complete the necessary analysis. To run this against a single database we could use the following code:&lt;/p>
&lt;p>Get-DbaHelpIndex -SqlInstsance mssql1 -Database AdventureWorks | Out-GridView&lt;/p>
&lt;p>In the above example I’ve used &lt;code>Out-GridView&lt;/code> to popup the results in a nice, easy to view GUI. I love using this output option to get a feel for the results. You can also filter and sort to help do some initial analysis to help get an understanding of your data.&lt;/p>
&lt;p>This is perfect – except I mentioned this database was in an AG. Oh, and it is set up to take advantage of using that read-only replica to run reporting against. That means the whole picture of the index usage is spread across two instances. We might find a totally unused index on our primary replica, a great candidate to be dropped, unless it’s heavily used by reports on the secondary.&lt;/p>
&lt;p>Remember, the secondary replica is just a read-only copy – so the indexes needed on the secondary must be created on the primary.&lt;/p>
&lt;p>In this situation we need to combine the index stats for both replicas into one easy to use result set – for this we can make use of PowerShell’s PSCustomObject to join these two result sets. In the code below I’ve set up a few variables at the top, and then run &lt;code>Get-DbaHelpIndex&lt;/code> against both instances. We then set up a variable to catch the results in &lt;code>$export&lt;/code> and use foreach-object to loop through the results for the primary instance. As we loop through, we’re looking for the matching index on the secondary replica before adding properties from both sides to the PSCustomObject.&lt;/p>
&lt;p>Finally, we lean on the ImportExcel module to export the results to an excel spreadsheet – if you haven’t checked this module out yet I highly recommend it.&lt;/p>
&lt;p>&lt;a class="link" href="https://gist.github.com/jpomfret/a9afa22c4d1129fecc4ea3e6cde1b51c" target="_blank" rel="noopener"
>https://gist.github.com/jpomfret/a9afa22c4d1129fecc4ea3e6cde1b51c&lt;/a>&lt;/p>
&lt;p>Looking at our results spreadsheet we can now easily review the index usage across both replicas and make sure that any indexes we identify as unused, truly are unused.&lt;/p></description></item><item><title>T-SQL Tuesday #143: Short code examples</title><link>https://jpomfret.github.io/t-sql-tuesday-#143-short-code-examples/</link><pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#143-short-code-examples/</guid><description>&lt;p>&lt;a class="link" href="https://johnmccormack.it/2021/10/t-sql-tuesday-143-short-code-examples/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Well folks, it’s Wednesday here in the UK, which means I’m a day late to get my blog post in for T-SQL Tuesday. However, if I was in Hawaii it would be still Tuesday so let&amp;rsquo;s go for it&amp;hellip;&lt;/p>
&lt;p>I used a handy short script this morning and I figured it was worth a quick, late entry! Hopefully John Mccormack (&lt;a class="link" href="https://johnmccormack.it/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/actualjohn" target="_blank" rel="noopener"
>t&lt;/a>), will forgive me for stretching the deadline!&lt;/p>
&lt;p>First of all, shout out to John for hosting the monthly blog party, he has got a great prompt and I’m really excited to see the wrap-up post as I’m sure it’ll be full of great little code snippets.&lt;/p>
&lt;blockquote>
&lt;p>T-SQL Tuesday this month is going back to basics and its all about code. I’d like to know &lt;strong>“What are your go to handy short scripts”?&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>This morning I was working on pulling together some information which included whether certain accounts were in the local administrator’s group on some remote servers. I had the perfect snippet saved in my code repo so I was quickly able to answer that question – and then I realised I should share that with you all.&lt;/p>
&lt;p>The following PowerShell snippet uses the &lt;code>net localgroup&lt;/code> command line tool to retrieve the results and parse them so we just get the account names.  The final line includes the &lt;code>-ComputerName&lt;/code> parameter so you can easily run it against remote machines.&lt;/p>
&lt;p>Invoke-Command -ScriptBlock { net localgroup administrators |
Where-Object { $_ -AND $_ -notmatch &amp;ldquo;command completed successfully&amp;rdquo; } |
Select -skip 4
} -ComputerName mssql1&lt;/p>
&lt;p>Hope this comes in handy, and sorry again John for sneaking in late.&lt;/p></description></item><item><title>Searching Stored Procedures for a pattern made easy with dbatools</title><link>https://jpomfret.github.io/searching-stored-procedures-for-a-pattern-made-easy-with-dbatools/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/searching-stored-procedures-for-a-pattern-made-easy-with-dbatools/</guid><description>&lt;p>Well folks, after starting the year off on a strong foot it’s been a while since I’ve published any blog posts. Hope you didn’t miss me too much, but I’m back now and I’ve got a useful dbatools snippet for you today.  Last week at the day job I had a situation where I needed to find all stored procedures that referenced a certain string, across any database on a specific server.  This is a pretty trivial task in SSMS when you’re just talking about one database. For example, if we’re looking for any reference to ‘Person’ perhaps we could run this T-SQL within the context of the database:&lt;/p>
&lt;p>select o.name, sc.text, o.type
from sys.objects o
inner join sys.syscomments sc
on sc.id = o.object_id
where text like &amp;lsquo;%Person%&amp;rsquo;
and o.type = &amp;lsquo;P&amp;rsquo; &amp;ndash; filtered for just stored procedures&lt;/p>
&lt;p>You can see I’ve found one procedure in my TestDb that references the ‘Person’ table, so it has been returned.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/TSQL.jpg"
loading="lazy"
alt="T-SQL code to find procedures with &amp;lsquo;person&amp;rsquo; in"
>&lt;/p>
&lt;p>However, if I want to search all databases on the server I now need to start thinking about a cursor, or using something like &lt;code>sp_MSforeachdb&lt;/code> to iterate over the databases.  A quick warning here- &lt;code>sp_MSforeachdb&lt;/code> is an undocumented procedure and there are some known issues with it.&lt;/p>
&lt;p>The natural next step here when we’re thinking about handling multiple databases is to switch to PowerShell and use dbatools.&lt;/p>
&lt;h2 id="find-the-dbatools-command-for-the-job">Find the dbatools command for the job&lt;/h2>
&lt;p>When we’re looking for the command we need within dbatools to fulfil our needs I cannot recommend &lt;code>Find-DbaCommand&lt;/code> highly enough.  This command will search all other commands for the pattern you pass in.  Today we know we want to find references in stored procedures so let’s see if there is a command that will help.&lt;/p>
&lt;p>Find-DbaCommand *stored*proc*&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/findCommand-1024x368.jpg"
loading="lazy"
alt="Find-DbaCommand helping us to find what we need"
>&lt;/p>
&lt;p>Looks like &lt;code>Get-DbaDbStoredProcedure&lt;/code> is what we need here.  Since this is our first time using this particular command I always have a quick look through the help content. I highly recommend running &lt;code>Get-Help Get-DbaDbStoredProcedure -ShowWindow&lt;/code>, this will open a separate window from your console and allow you to keep that open to refer back to if needed.  The last section of the help gives us several examples on how to use this command- let’s run a simple one against a test database to see what we get. I’m also going to pipe the output to &lt;code>Select-Object&lt;/code> so I can just sample the first 2 results.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Select-Object -First 2&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/twoProcs-1024x539.jpg"
loading="lazy"
alt="Get-DbaDbStoredProcedure results"
>&lt;/p>
&lt;p>This is handy, but this output doesn’t look like it’s going to help answer the question and find references of a string within the stored procedure code. There’s more than meets the eyes though.&lt;/p>
&lt;h2 id="sql-server-management-objects-smo">SQL Server Management Objects (SMO)&lt;/h2>
&lt;p>dbatools deals mostly with SQL Server Management Objects (SMO), which means that what you see in the output for commands is not always all there is available.  SMO is a hierarchy of objects which can be easily traversed from the output of the commands.  You can tell that we’re dealing with SMO instead of standard PowerShell objects by using &lt;code>Get-Member&lt;/code> and looking at the TypeName.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Get-Member&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/getMember.jpg"
loading="lazy"
alt="using Get-Member to see what&amp;rsquo;s available"
>&lt;/p>
&lt;p>&lt;code>Get-Member&lt;/code> is also really useful for looking to see what’s available from the object that is returned. In the screenshot above you can see multiple methods that can be used. If you run this in your console you’ll also get a list of all available properties.  That’s a hint for what we need for our scenario.&lt;/p>
&lt;h2 id="find-our-string-within-all-stored-procedures-in-any-database">Find our string within all Stored Procedures in any database&lt;/h2>
&lt;p>Now that we know the &lt;code>Get-DbaDbStoredProcedure&lt;/code> command is going to return SMO StoredProcedure objects we can look at some of the properties not returned by default.  We already saw one option for this- using &lt;code>Get-Member&lt;/code> will list all the properties available to us.  Another option is to select all the properties for the first result.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Select-Object -First 1 *&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/SelectAll-1024x315.png"
loading="lazy"
alt="Select all the properties from Get-DbaDbStoredProcedure"
>&lt;/p>
&lt;p>In the output you can see there are a lot of properties available that weren’t returned by default, this includes &lt;code>TextBody&lt;/code> which is what we need to search for our reference string.  All we need to do now is pipe the output of the command through to &lt;code>Where-Object&lt;/code> to find what we need:&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -ExcludeSystemSp | Where-Object TextBody -like &amp;lsquo;*Person*&amp;rsquo;&lt;/p>
&lt;p>You’ll notice two more changes to the code above. I dropped the &lt;code>Database&lt;/code> parameter, opening the search up to the whole server. I also added &lt;code>ExcludeSystemSp&lt;/code>, which means I’m only interested in user defined stored procedures. It is important to note if you have a lot of stored procedures this command could take a little while to run.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/SearchAllSPs-1024x237.jpg"
loading="lazy"
alt="search the TextBody for key words"
>&lt;/p>
&lt;p>PowerShell also supports other &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_comparison_operators?view=powershell-7.1" target="_blank" rel="noopener"
>comparison operators&lt;/a> by default, including `match` which can be used to find regex patterns within your procedures.  This opens up a lot more possibilities when looking for more complicated patterns within your database.&lt;/p>
&lt;h2 id="so-many-more-options">So many more options…&lt;/h2>
&lt;p>Today we were only looking for results on one SQL Server instance, but since dbatools makes handling multiple SQL instances easy we could also widen the search and instead search our entire estate for references to a certain string.&lt;/p>
&lt;p>We also only looked at Stored Procedures today, but if you do a little research with &lt;code>Find-DbaCommand&lt;/code>, &lt;code>Get-Help&lt;/code> and &lt;code>Get-Member&lt;/code> you’ll soon find what you need to search through functions, views and more.&lt;/p>
&lt;p>Happy searching!&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 3</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-3/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-3/</guid><description>&lt;p>This is now the third post in a series on Azure tags. You can read the other two posts here to get up to speed with where we’ve been, however that isn’t required for this post .&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://jesspomfret.com/azure-tags-part1/" target="_blank" rel="noopener"
>Keeping track of Azure resources with tags – Part 1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://jesspomfret.com/azure-tags-part2/" target="_blank" rel="noopener"
>Keeping track of Azure resources with tags – Part 2&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In part one I discussed how useful Azure tags can be, and specifically about how adding a ‘dateCreated’ tag can help you keep track of your resources, and how to find resources with certain tags using PowerShell.  Part 2 and 3 are based around the fact that adding the ‘dateCreated’ tag is a great idea, but relying on a human to remember to add it is less than ideal. In part 2 we looked at using Azure Policy to automatically add the tag. Today’s post will cover another option using Azure Functions.&lt;/p>
&lt;p>Azure Functions gives us a way of running serverless code, written in a number of different languages, triggered by specific events or timings.  Looking through the &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview" target="_blank" rel="noopener"
>documentation&lt;/a> there are many use cases from processing files to analysing IoT workstreams.  Our use case is to run a PowerShell script that tags any resources that are missing the ‘dateCreated’.&lt;/p>
&lt;h2 id="step-1---create-a-function">Step 1 - Create a function&lt;/h2>
&lt;p>Azure Functions live within a function app, so the first thing we have to do is create this logical container. At this level we’ll decide on a ‘Function App name’, I’ve called mine ‘resourceTagJp’, and choosing ‘Code’ for the publish option we can then choose PowerShell as our language of choice. There are some other options for selecting a storage account and configuring ‘Application Insights’, but for now I’ve left those all as the defaults.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/functionApp.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/functionApp.png"
loading="lazy"
alt="create a function app pane"
>&lt;/a>&lt;/p>
&lt;p>Once the ‘Function App’ is created we are ready to create our function.  On the left hand pane choose ‘Functions’ and then ‘Add’. This will open a pane for you to choose how to develop the function, either in the portal or on your local machine in VSCode, for example, and the template to base your function off of.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/CreateFunction-1.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/CreateFunction-1.png"
loading="lazy"
alt="Create an Azure Function"
>&lt;/a>&lt;/p>
&lt;p>One of the simplest options is to choose ‘Timer Trigger’, which as expected will execute the function code based on a schedule. The schedule is set using a cron expression. For it to run once an hour at the top of the hour we’ll use the following:&lt;/p>
&lt;p>0 0 * * * *&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/cronSchedule.png"
loading="lazy"
alt="cron schedule popup"
>&lt;/p>
&lt;p>Once the function is created we’ll choose ‘Code + Test’ on the left hand pane of the portal to actually add the function code. The code for my function is going to be pretty simple, but if you are writing more complicated functions there is a &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions" target="_blank" rel="noopener"
>VSCode extension&lt;/a> that can be used to develop and test functions locally before publishing them to Azure.&lt;/p>
&lt;p>We have three files within our function:&lt;/p>
&lt;ul>
&lt;li>Readme.md – for documentation in markdown&lt;/li>
&lt;li>Function.json – the config file, currently contains our timer binding information&lt;/li>
&lt;li>Run.ps1 – the main function code, in PowerShell as that’s what we chose&lt;/li>
&lt;/ul>
&lt;p>The code for the function is below and makes use of the &lt;code>Update-AzTag&lt;/code> cmdlet to add the ‘dateCreated’ tag. In this example, since I’m using PowerShell, I can easily format the date to be exactly how I want it to be displayed. If you read part 2 in this series, that was a downfall of using Azure Policy, I only had one datetime format option. The &lt;code>Update-AzTag&lt;/code> also has a &lt;code>-Merge&lt;/code> parameter which ensures any tags already on the resource aren’t overwritten by this function.&lt;/p>
&lt;p># Input bindings are passed in via param block.
param($Timer)&lt;/p>
&lt;h1 id="select-the-subscription">Select the subscription&lt;/h1>
&lt;p>$null = Select-AzSubscription -SubscriptionId (Get-AzSubscription -SubscriptionName &amp;lsquo;MSDN Platforms&amp;rsquo;)&lt;/p>
&lt;h1 id="tag-any-resources-that-are-missing-tags">tag any resources that are missing tags&lt;/h1>
&lt;p>$res = Get-AzResource -ResourceGroupName functionTest | Where { $_.tags.keys -notcontains &amp;lsquo;dateCreated&amp;rsquo; }
$res.Foreach{
Update-AzTag -ResourceId $psitem.ResourceId -Tag @{&amp;lsquo;dateCreated&amp;rsquo; = (Get-Date -Format &amp;ldquo;yyyy-MM-dd&amp;rdquo;)} -Operation Merge
}&lt;/p>
&lt;p>That’s all it is to create our function – however, it doesn’t currently have the authorisation to view or update resources.&lt;/p>
&lt;h2 id="step-2---add-a-managed-identity">Step 2 - Add a managed identity&lt;/h2>
&lt;p>To provision access to allow our function to work we can make use of &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/app-service/overview-managed-identity?tabs=dotnet" target="_blank" rel="noopener"
>Azure Managed Identities&lt;/a>. These are similar to Managed Service Accounts in that there is no need to set or rotate passwords. This means we can configure our function with a managed identity and then forget about it- we know it’ll remain secure and the password/secret will be rotated often.&lt;/p>
&lt;p>There are two options for managed identities: system-assigned or user-assigned. The system-assigned identity is tied directly to your application – if we delete the function the identity will also be deleted, and that’s fine for this purpose.&lt;/p>
&lt;p>Configuring the system-assigned identity is pretty straightforward. On our function pane under identity, change the status to ‘On’.  After a couple of minutes the identity will be deployed and you’ll see the option to assign ‘Azure role assignments’.&lt;/p>
&lt;p>Clicking on ‘Azure role assignments’ will open a pane where you can assign whatever permissions your function will need to run. This can be scoped at the subscription or resource group level. For this example my function is just tagging anything within the ‘functionTest’ resource group so I can set the permissions to that scope.  I have chosen the ‘contributor’ role as that gives us enough permissions to view and tag resources.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/ManagedIdentity.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/ManagedIdentity.png"
loading="lazy"
alt="Setting up a system assigned managed identity for our function"
>&lt;/a>&lt;/p>
&lt;h2 id="step-3---test-our-function">Step 3 - Test our function&lt;/h2>
&lt;p>We have created our function and set up the managed identity to enable the function to access our resources, now it’s time to make sure it’s working as expected. From the ‘Code + Test’ page there is a ‘Test/Run’ at the top that brings out the pane on the right. In that pane pressing run will simulate the time trigger being met and our function executing.&lt;/p>
&lt;p>In the console you can see exactly what the function does and any output you’ve configured – in this example you can see a storage account was tagged.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/TestFunction-1.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/TestFunction-1.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>We can also test this function by creating an untagged resource in the ‘functionTest’ resource group and waiting for the timer to be triggered.&lt;/p>
&lt;p>However we test our function we can see the tag is now on our storage account and we no longer have to rely on a human to remember the tag when they create resources.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/storageAccountFunctionTagged.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/storageAccountFunctionTagged.png"
loading="lazy"
alt="Showing a storage account has been tagged with our specified date format"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This has been as much a learning experience for me as it hopefully has been for you. My journey into Azure is still pretty new but I’m enjoying the adventure.&lt;/p>
&lt;p>To wrap this up, having a tagging strategy is important and there are multiple ways to ensure that tagging strategy is followed. Both Azure Policy and Azure Functions give us a good option for automatically tagging resources that are missing tags. If you’re using Terraform to deploy Azure resources &lt;a class="link" href="https://jqmartin.info/2021/03/02/terraform-timestamps-and-tagging/" target="_blank" rel="noopener"
>John Martin has written about adding the a tag for date created to all resources as they are deployed&lt;/a>, which is definitely worth a read.&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 2</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-2/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-2/</guid><description>&lt;p>Last week, in &lt;a class="link" href="https://jesspomfret.com/azure-tags-part1/" target="_blank" rel="noopener"
>Part 1&lt;/a>, we talked about how to easily keep track of our resources with tags. There are many strategies for tagging your resources but I specifically focused on adding a ‘dateCreated’ tag so we could see when resources were created – since this isn’t available by default.  During that post we identified the biggest issue we had was that we were relying on a human to remember to add the ‘dateCreated’ tag for every resource they created. I’ve got two ideas on how to fix that – today we’ll look at the first option, using &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/governance/policy/overview" target="_blank" rel="noopener"
>Azure Policy&lt;/a>.&lt;/p>
&lt;p>Azure Policy is a way of comparing your Azure estate to defined requirements. You can either use predefined definitions (of which there are many) or create your own specific rules.  These definitions can be assigned to certain scopes (subscriptions, resource groups). Azure Policy then reports on whether you’re in the expected state and in some cases can alter resources to ensure you are.&lt;/p>
&lt;h2 id="step-1--define-a-policy">&lt;strong>Step 1 – Define a policy&lt;/strong>&lt;/h2>
&lt;p>In our example, all resources should have a ‘dateCreated’ tag, and if Azure Policy finds the tag is missing, it should add that tag with the current date.&lt;/p>
&lt;p>There are a few steps to set up our policy for ensuring the ‘dateCreated’ tag exists on all resources. First we need to write a policy definition in JSON.  Don’t panic yet though – there are a lot of examples in the &lt;a class="link" href="https://github.com/Azure/azure-policy" target="_blank" rel="noopener"
>Azure/azure-policy&lt;/a> GitHub repo that we can start with.  By browsing the repo you can find the ‘&lt;a class="link" href="https://github.com/Azure/azure-policy/tree/master/samples/Tags/add-tag" target="_blank" rel="noopener"
>add-tag&lt;/a>’ policy which is the perfect base for us to build off of.  &lt;/p>
&lt;p>When viewing that GitHub page there is a ‘Deploy to Azure’ button- clicking that (presuming you’re logged into the portal) will take you straight to the ‘New Policy Definition’ wizard where we can modify our policy to meet our needs and save it. &lt;/p>
&lt;p>You’ll need to choose a ‘Definition Location’ (which is the subscription this policy should reside in), name your policy, and edit the description if needed. The GitHub template has already specified this will go in the ‘Tags’ category,but you can change that if you’re keeping custom policies in a new category. Then we get to the policy rule, it’s JSON time.&lt;/p>
&lt;p>Since we imported the sample from GitHub the JSON is almost exactly what we need. The first section defines the rules and the second section defines parameters.  In this case we’re going to remove the parameter section and change the tag name and values expected to be static. This means that the policy will only ever be used for adding the specific ‘dateCreated’ tag.  The reason for this is we’re going to add some logic to the tag value so it contains the current date. (It is possible that this can be achieved with parameters, but I couldn’t get it to work. Please let me know in the comments if you know differently). &lt;/p>
&lt;p>The JSON below is pretty simple. There are two main sections: the condition to be met and then the operation to carry out if the conditions are met.  In the conditions section we’re looking to see if the dateCreated tag exists, if it doesn’t we’ll move onto the second section of the JSON. This defines what to do about it, and in this case it’s pretty simple, we’ll modify the target and add the dateCreated tag.  The tag value is dynamic and uses a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions" target="_blank" rel="noopener"
>resource manager template function&lt;/a> to get the current date.  There are a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/governance/policy/concepts/definition-structure#policy-functions" target="_blank" rel="noopener"
>few restrictions&lt;/a> on using functions within policy definitions, one being that we can’t overload the uctNow function with a format parameter so we’ll only be able to get a date in ISO 8601 (yyyyMMddTHHmmssZ) format.&lt;/p>
&lt;p>{
&amp;ldquo;mode&amp;rdquo;: &amp;ldquo;Indexed&amp;rdquo;,
&amp;ldquo;policyRule&amp;rdquo;: {
&amp;ldquo;if&amp;rdquo;: {
&amp;ldquo;field&amp;rdquo;: &amp;ldquo;tags[&amp;lsquo;dateCreated&amp;rsquo;]&amp;rdquo;,
&amp;ldquo;exists&amp;rdquo;: &amp;ldquo;false&amp;rdquo;
},
&amp;ldquo;then&amp;rdquo;: {
&amp;ldquo;effect&amp;rdquo;: &amp;ldquo;modify&amp;rdquo;,
&amp;ldquo;details&amp;rdquo;: {
&amp;ldquo;roleDefinitionIds&amp;rdquo;: [
&amp;ldquo;/providers/microsoft.authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c&amp;rdquo;
],
&amp;ldquo;operations&amp;rdquo;: [
{
&amp;ldquo;operation&amp;rdquo;: &amp;ldquo;add&amp;rdquo;,
&amp;ldquo;field&amp;rdquo;: &amp;ldquo;tags[&amp;lsquo;dateCreated&amp;rsquo;]&amp;rdquo;,
&amp;ldquo;value&amp;rdquo;: &amp;ldquo;[utcNow()]&amp;rdquo;
}
]
}
}
}
}&lt;/p>
&lt;p>The final decision to make for our new policy definition is the ‘Role definition’. Since our policy has remediation actions, the operation to add tags if needed, a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview" target="_blank" rel="noopener"
>managed identity&lt;/a> will be created for the policy.  The ‘role definition’ is the permissions that will be granted to that managed identity.  The default is ‘Contributor’ which assigns ‘full access to manage all resources’. This can be changed based on what your policy needs access to.&lt;/p>
&lt;h2 id="step-2--assign-the-policy">&lt;strong>Step 2 – Assign the policy&lt;/strong>&lt;/h2>
&lt;p>We’ve now defined our policy, but the second part of this process is to assign that policy a scope. This outlines what this policy applies to. The scope can be a subscription or resource group, and can be more finely tuned by excluding specific resources that you don’t want to apply the policy too.  Then it’s as easy as selecting the policy we defined, setting an ‘Assignment name’, which could be a combination of policy name and assigned score, and adding an optional description.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/assignPolicy.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/assignPolicy.png"
loading="lazy"
alt="Assign policy screen in Azure"
>&lt;/a>&lt;/p>
&lt;h2 id="step-3--test-the-policy">&lt;strong>Step 3 – Test the policy&lt;/strong>&lt;/h2>
&lt;p>The easiest way to test our policy is to create a resource without the ‘dateCreated’ tag and see what happens. We have scoped our policy assignment to the ‘policyTest’ subscription so I’ll run the following PowerShell to create a new storage account that’s missing my required tag.&lt;/p>
&lt;p>New-AzStorageAccount -ResourceGroupName policyTest -AccountName mystorageaccountjp77 -Location uksouth -SkuName Standard_GRS&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/createStorageAccount.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/createStorageAccount-1024x124.png"
loading="lazy"
alt="create storage account with New-AzStorageAccount"
>&lt;/a>&lt;/p>
&lt;p>You can see there is no &lt;code>-Tags&lt;/code> parameter specified, so this storage account was created without any tags. If we now run &lt;code>Get-AzStorageAccount&lt;/code> we can see it has the &amp;lsquo;dateCreated&amp;rsquo; tag.&lt;/p>
&lt;p>Get-AzStorageAccount -ResourceGroupName policyTest | Select-Object StorageAccountName, Tags&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/storageAccountTagged.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/storageAccountTagged-1024x129.png"
loading="lazy"
alt="Get-AzStorageAccount results show the tags"
>&lt;/a>&lt;/p>
&lt;p>You can also see the tag in the portal view of our storage account.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/storageAccountTaggedPortal.png"
loading="lazy"
>&lt;/p>
&lt;p>Finally, if we check out the Azure Policy we can see that we are in compliance. All resources in the ‘policyTest’ subscription have the required ‘dateCreated’ tag.  We can also see the specific resources that are in compliance, in our case just the storage account we created.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/policyCompliance.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/policyCompliance-1024x390.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>This is just one option for automatically assigning the ‘dateCreated’ tag on all new resources. In this case we scoped the policy to a specific resource group but have assigned it at the subscription level to cover all resources.  Note – this is specifically to tag resources. If you want to also tag resource groups the policy definition will need to be altered slightly.&lt;/p>
&lt;p>One downside of this method is I haven’t found a way to control the format of the date in the tag value. This isn’t a big concern but does lack a little flexibility, especially if we wanted to add the date in a different time zone.&lt;/p>
&lt;p>Next week we’ll look at adding the same functionality using Azure Functions to auto tag new resources.&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 1</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-1/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-1/</guid><description>&lt;p>I’ve been working on some Azure exams recently, and I personally learn best by fiddling with things.  The &lt;a class="link" href="https://docs.microsoft.com/en-us/learn/" target="_blank" rel="noopener"
>Microsoft learn&lt;/a> content is excellent, and I’d highly recommend that for any of the Azure exams I’ve taken so far.  However, I also like to build things myself and experiment a little with all the available options.&lt;/p>
&lt;p>One of the vital parts of this learning and experimenting needs to be cleaning up after myself.  We all know the risks of leaving things running in Azure- it’s likely to drain your training budget pretty quickly.  To be fair, this is also a good lesson for real world scenarios. Getting used to turning off or scaling down resources based on need is a good way to reduce your Azure spend.&lt;/p>
&lt;p>This brings me to one morning last week. I logged in to the portal and got a pop up that my credit was down to under $5, which is not what I was expecting. I started looking around and wondering what I’d left running – it isn’t always easy to spot though.&lt;/p>
&lt;p>Luckily, John Martin (&lt;a class="link" href="https://jqmartin.info/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/jqmtweets" target="_blank" rel="noopener"
>t&lt;/a>) has instilled in me the importance of adding a tag for creation date on all resources, as it’s not tracked automatically. This means we can easily see what we last deployed and what we might have forgotten about.&lt;/p>
&lt;p>In Azure, tags are just key value pairs that can be applied to resources and subscriptions to add metadata. You can use them to organise resources by environment, cost centre, business criticality, and anything else that might be important to your individual situation. There is a limit of 50 tags per resource. If you’re getting close to having 50 tags per resource you might need to rethink your tagging strategy to reduce the complexity.&lt;/p>
&lt;p>You can view tags through the portal either through the dedicated ‘Tags’ pane, on each individual resource, or on the ‘Cost Management’ area. Here you can view your Azure spend broken down by tags, which can be very useful. You can also view tags using either the Azure CLI or PowerShell. I usually opt for PowerShell, so let’s have a look at how we can view resources with certain tags using the &lt;a class="link" href="https://www.powershellgallery.com/packages/Az/" target="_blank" rel="noopener"
>Az module&lt;/a>.  If you don’t already have the module installed you can run &lt;code>Install-Module az&lt;/code> to get started. More details on prerequisites and options available can be found in the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-5.6.0" target="_blank" rel="noopener"
>Install Azure PowerShell with PowerShellGet&lt;/a> docs.&lt;/p>
&lt;h2 id="find-resources-with-a-certain-tag">&lt;strong>Find resources with a certain tag&lt;/strong>&lt;/h2>
&lt;p>I already mentioned I add a ‘dateCreated’ tag to all resources, but when I’m playing around in Azure and working through training courses I also add a ‘training’ tag and set the value to ‘true’.  This is an easy way for me to find all the resources I’ve created while training and clean them up.&lt;/p>
&lt;p>We can easily list these resources in PowerShell using the following one liner:&lt;/p>
&lt;p>Get-AzResource -TagName training -TagValue &amp;rsquo;true&amp;rsquo; |
Select-Object Name, ResourceGroupName,ResourceType, Tags&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/trainingTrue-1024x198.jpg"
loading="lazy"
alt="results of Get-AzResource"
>&lt;/p>
&lt;p>If we’re ready to clean them up we can pipe the results from &lt;code>Get-AzResource&lt;/code> straight into &lt;code>Remove-AzResource&lt;/code>, ensuring we haven’t got anything left running and costing us credit.&lt;/p>
&lt;p>Get-AzResource -TagName training -TagValue &amp;rsquo;true&amp;rsquo; | Remove-AzResource&lt;/p>
&lt;h2 id="resources-created-in-the-last-x-days">&lt;strong>Resources created in the last x days&lt;/strong>&lt;/h2>
&lt;p>Another useful snippet since we’re adding ‘dateCreated’ tags to all our resources is to get any resources that have been created in the last few days. Since I have formatted my dates as yyyy-MM-dd in my tags I can easily convert them into dates with PowerShell and then filter based on them.&lt;/p>
&lt;p>Get-AzResource -TagName dateCreated |
Select-Object Name, ResourceType, @{l=&amp;lsquo;dateCreated&amp;rsquo;;e={get-date($_.Tags[&amp;lsquo;dateCreated&amp;rsquo;])}} |
Where-Object dateCreated -gt (get-date).AddDays(-7)&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/resourcesCreatedinLastXDays-1024x327.jpg"
loading="lazy"
alt="List of resources created in last 7 days in PowerShell"
>&lt;/p>
&lt;h2 id="resources-without-a-tag">&lt;strong>Resources without a tag&lt;/strong>&lt;/h2>
&lt;p>The final tag related snippets I have for today is to make sure all our resources have the &amp;lsquo;dateCreated&amp;rsquo; tag. Since currently I’m manually adding these tags there is a chance I forget or get lazy and some resources make it through without the tags.&lt;/p>
&lt;p>We can find these tags by interrogating the key values of the tags:&lt;/p>
&lt;p>Get-AzResource | where {$_.tags.Keys -notcontains &amp;lsquo;dateCreated&amp;rsquo;} |
Select-Object Name, ResourceType, Tags&lt;/p>
&lt;p>Once we know which resources are missing tags we can easily update them using &lt;code>Update-AzTag&lt;/code>, the operation parameter that controls what should happen if there are existing tags. Merge will ensure we don’t overwrite the current tags.&lt;/p>
&lt;p>$resources = Get-AzResource -ResourceGroupName missingtag |
Where-Object {$_.tags.Keys -notcontains &amp;lsquo;dateCreated&amp;rsquo;}&lt;/p>
&lt;p>Update-AzTag -ResourceId $resources.ResourceId -Tag @{&amp;lsquo;dateCreated&amp;rsquo; = (Get-Date -Format &amp;ldquo;yyyy-MM-dd&amp;rdquo;)} -Operation Merge&lt;/p>
&lt;p>The main problem with this whole idea is we are relying on whoever creates the resources to both remember to create the tag and put the values of the tag in a standard format.  Next week we’ll look at a couple of ways to automate this process.&lt;/p></description></item><item><title>Quickly Execute a Folder of SQL Scripts against a SQL Server</title><link>https://jpomfret.github.io/quickly-execute-a-folder-of-sql-scripts-against-a-sql-server/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/quickly-execute-a-folder-of-sql-scripts-against-a-sql-server/</guid><description>&lt;p>Another week and another useful dbatools snippet for you today.  Last week at work I was given a folder of 1,500 scripts – each containing a create table statement. Can you imagine having to open each file in Management Studio to be able to execute it? Thank goodness we have PowerShell and &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> on our side.&lt;/p>
&lt;p>The code for this example is pretty short, but there are a couple of things to point out. &lt;/p>
&lt;p>First, I used &lt;code>Connect-DbaInstance&lt;/code> to create a server object to use to run the queries.  This means that we’re efficiently reusing the connection rather than opening a new one for each file we want to execute. &lt;/p>
&lt;p>Second, I’m using the foreach method which takes each script file returned from the &lt;code>Get-ChildItem&lt;/code> call, and executes &lt;code>Invoke-DbaQuery&lt;/code>.  With this we can use the &lt;code>-File&lt;/code> parameter to pass in the sql file and that’s really all we need.  This will loop through each file running the sql scripts.&lt;/p>
&lt;p>$SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
$destinationDatabase = &amp;lsquo;AdventureWorks2021&amp;rsquo;
$folderPath = &amp;lsquo;.\output\AdventureWorks2017&amp;rsquo;&lt;/p>
&lt;h1 id="create-a-connection-to-the-server-that-we-will-reuse---can-use-sqlcredential-for-alternative-creds">Create a connection to the server that we will reuse - can use SqlCredential for alternative creds&lt;/h1>
&lt;p>$sqlInst = Connect-DbaInstance -SqlInstance $SqlInstance&lt;/p>
&lt;p>(Get-ChildItem $folderPath).Foreach{
Invoke-DbaQuery -SqlInstance $sqlInst -Database $destinationDatabase -File $psitem.FullName
}&lt;/p>
&lt;p>That’s really all we need for this blog post, but in order to set this up for a demo I did use a few other dbatools commands. I’ve posted the script above, along with the setup scripts on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/08_ExecuteFolderOfScripts.ps1" target="_blank" rel="noopener"
>GitHub&lt;/a>. This includes creating a new database, scripting out all the tables into individual script files, and ensuring all the schemas and other dependencies were ready in the new database.&lt;/p>
&lt;p>Thanks for reading, and hope this is a useful snippet. It sure saved me a lot of time this week.&lt;/p></description></item><item><title>Troubleshooting SPN Troubles - Cannot generate SSPI context</title><link>https://jpomfret.github.io/troubleshooting-spn-troubles-cannot-generate-sspi-context/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/troubleshooting-spn-troubles-cannot-generate-sspi-context/</guid><description>&lt;p>I was working in my lab environment this weekend, playing with some SQL Servers that I had built with PowerShell DSC a while ago.  I had installed SQL Server with mostly defaults, including not changing the engine and agent service accounts.  For the blog post I thought I was going to write next, I wanted to change these to be active directory accounts – it did not go smoothly, and I figured this might be useful to document for future Jess, or anyone else who might stumble across this problem.&lt;/p>
&lt;h2 id="create-the-issue-change-sql-server-service-accounts">Create the Issue: Change SQL Server Service Accounts&lt;/h2>
&lt;p>First off, I created two new Active Directory users that I’ll use for my service accounts. The below code will create a prompt for each account for the password to be entered.&lt;/p>
&lt;p>$engSvcAccount = &amp;lsquo;svc-dscsvr1-eng2&amp;rsquo;
$agSvcAccount = &amp;lsquo;svc-dscsvr1-ag2&amp;rsquo;&lt;/p>
&lt;p>$EngSvcAccount = @{
Name = $engSvcAccount
UserPrincipalName = $engSvcAccount
AccountPassword = (Get-Credential -Credential EnterPassword).Password
PasswordNeverExpires = $true
Enabled = $true
}
New-AdUser @EngSvcAccount&lt;/p>
&lt;p>$AgentSvcAccount = @{
Name = $agSvcAccount
UserPrincipalName = $agSvcAccount
AccountPassword = (Get-Credential -Credential EnterPassword).Password
PasswordNeverExpires = $true
Enabled = $true
}
New-AdUser @AgentSvcAccount&lt;/p>
&lt;p>We can view the current SQL services with &lt;code>Get-DbaService&lt;/code>. This is useful to see what account they are currently running under, as well as the service names.&lt;/p>
&lt;p>Get-DbaService -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/GetDbaService.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaService.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>There is also a command for updating service accounts in dbatools. I will note, sometimes I have issues with the command being able to update the accounts and I’m not sure why. It worked perfectly in this scenario though running the following.&lt;/p>
&lt;p>This again creates a prompt to enter the service account password, before setting the service &amp;lsquo;StartName&amp;rsquo;.&lt;/p>
&lt;p>Update-DbaServiceAccount -ComputerName dscsvr1 -ServiceName MSSQLSERVER -ServiceCredential (Get-Credential -Credential &amp;ldquo;Pomfret\$engSvcAccount&amp;rdquo; )
Update-DbaServiceAccount -ComputerName dscsvr1 -ServiceName SQLSERVERAGENT -ServiceCredential (Get-Credential -Credential &amp;ldquo;Pomfret\$agSvcAccount&amp;rdquo; )&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/UpdateDbaServiceAccount.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/UpdateDbaServiceAccount.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>If I rerun &lt;code>Get-DbaService&lt;/code> I can see all looks good. &lt;code>StartName&lt;/code> shows my new accounts and the services for both engine and agent are running.&lt;/p>
&lt;p>Get-DbaService -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/GetDbaService_post.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaService_post.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="the-issue">The Issue&lt;/h2>
&lt;p>At this point I was still planning on writing a blog post on a totally different topic. I ran the following to determine what databases I already had on dscsvr1:&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance dscsvr1 -ExcludeSystem | Format-Table&lt;/p>
&lt;p>But instead of getting a quick answer to my question, I just got the following error:&lt;/p>
&lt;p>&lt;em>WARNING: [15:19:49][Get-DbaDatabase] Error occurred while establishing connection to dscsvr1 | The target principal name is incorrect. Cannot generate SSPI context.&lt;/em>&lt;/p>
&lt;p>I checked a few things as I started troubleshooting:&lt;/p>
&lt;ul>
&lt;li>Are the services running – we already checked this with &lt;code>Get-DbaService&lt;/code> and they are&lt;/li>
&lt;li>Was it firewall related – inbound rules were in place, and I was able to previously connect&lt;/li>
&lt;li>Was it certificate related – I’m not forcing encryption, and there are no certificates set up&lt;/li>
&lt;li>Was it Service Principal Name (SPN) related – bingo&lt;/li>
&lt;/ul>
&lt;p>I have seen this happen before when changing service accounts for SQL services. I’m not an Active Directory expert, and I’m certainly not a Kerberos expert – in fact I’m as surprised as you that Kerberos has actually appeared on this blog. &lt;/p>
&lt;p>What I do know is that due to permissions, the SPNs needed were not able to be registered for the new service accounts.  The easiest way to investigate SPN issues is with dbatools, saving us again!&lt;/p>
&lt;p>&lt;code>Test-DbaSpn&lt;/code> works out exactly what SPNs are needed for our SQL instances and determines if they are in place.&lt;/p>
&lt;p>Test-DbaSpn -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/Test-DbaSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Test-DbaSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>This shows we should have two SPNs set for the default instance (MSSQLSERVER) on DscSvr1.  The ‘IsSet’ column shows they aren’t set – and this is why we can’t connect to our instance remotely.&lt;/p>
&lt;h2 id="lets-fix-it">Let&amp;rsquo;s Fix It&lt;/h2>
&lt;p>The fix for this issue seems simple- register the required SPNs.  dbatools again tries to make this as easy as possible for us. We can take the output from &lt;code>Test-DbaSpn&lt;/code> and pipe it straight into &lt;code>Set-DbaSpn&lt;/code> and dbatools will take care of the rest – won’t it?&lt;/p>
&lt;p>Test-DbaSpn -ComputerName dscsvr1 | Set-DbaSpn&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/FailedToSetSPN.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/FailedToSetSPN.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>As you can see from the warning message, dbatools wasn’t able to set our required SPNs either. It complains about &amp;lsquo;A constraint violation occurred&amp;rsquo;.&lt;/p>
&lt;p>The reason is each SPN can only be registered once, and these SPNs were created for the previous service accounts and never cleaned up due to a lack of permissions.&lt;/p>
&lt;p>We now need to use the &lt;code>setspn&lt;/code> command line tool that is built into Windows and available when you have AD windows features installed, but the output from the dbatools command is still very useful for building the inputs for &lt;code>setspn&lt;/code>.&lt;/p>
&lt;p>First we’ll try and register the required SPNs manually. For this we’ll use the -a parameter on &lt;code>setspn&lt;/code>, the format being:&lt;/p>
&lt;p>setspn -a &amp;laquo;SPN&amp;raquo; &amp;laquo;ServiceAccount&amp;raquo;&lt;/p>
&lt;p>So we’ll run the following, getting the SPN from the ‘RequiredSPN’ column of the &lt;code>Test-DbaSpn&lt;/code> output. You’ll notice there are two required SPNs, one without a port specified and one with 1433 – we’ll want to fix both.&lt;/p>
&lt;p>setspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\svc-dscsvr1-eng2&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/duplicateSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/duplicateSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see in the output, the problem is highlighted – ‘Duplicate SPN found’.  The useful part of this output is on the second line.  I’ve highlighted the current owner of the SPN – we need this to be able to resolve the problem.  Not surprising, it is the computer account since I was previously running SQL Server as the default &lt;code>NT SERVICE\MSSQLSERVER&lt;/code> account.&lt;/p>
&lt;p>Now we know what the duplicate is we can remove it, again using setspn, but this time with the -d parameter. The format is:&lt;/p>
&lt;p>setspn -d &amp;laquo;SPN&amp;raquo; &amp;laquo;ServiceAccount&amp;raquo;&lt;/p>
&lt;p>We’ll run the following two commands to clear up both old SPNs:&lt;/p>
&lt;p>setspn -d MSSQLSvc/DscSvr1.pomfret.com DSCSVR1
setspn -d MSSQLSvc/DscSvr1.pomfret.com:1433 DSCSVR1&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/DeleteSPN.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/DeleteSPN.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Finally we can add the required SPNs. We can either use dbatools with the code we tried earlier or &lt;code>setspn&lt;/code>.&lt;/p>
&lt;p>setspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\svc-dscsvr1-eng2
setspn -a MSSQLSvc/DscSvr1.pomfret.com:1433 Pomfret\svc-dscsvr1-eng2&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/addSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/addSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see the output now states ‘Updated object’ which means we were successful. If we try and view the databases again now we should see the output we were expecting.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/Get-DbaDatabase.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Get-DbaDatabase.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>As I mentioned, this was not at all what I was expecting to write about – but I hope it’ll be useful if you ever find yourself in this situation while trying to change service accounts for SQL Server.&lt;/p>
&lt;p>I was in the end able to resolve the permissions problems for my service accounts by following this great blog post &amp;lsquo;&lt;a class="link" href="http://www.alexandreviot.net/2014/09/30/sql-server-could-not-register-the-service-principal-name-spn/" target="_blank" rel="noopener"
>SQL Server - Could not register the Service Principal Name&lt;/a>&amp;rsquo;. Once I applied these permissions when I changed service accounts they were able to delete and recreate the required SPNs.&lt;/p>
&lt;p>Another great blog post for more reading on SPNs is this post by &lt;a class="link" href="https://www.twitter.com/pittfurg" target="_blank" rel="noopener"
>Drew Furgiuele&lt;/a> on how to use the &lt;a class="link" href="https://dbatools.io/schwifty/" target="_blank" rel="noopener"
>dbatools SPN commands&lt;/a>.&lt;/p></description></item><item><title>T-SQL Tuesday #135: The outstanding tools of the trade that make your job awesome</title><link>https://jpomfret.github.io/t-sql-tuesday-#135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/</guid><description>&lt;p>&lt;a class="link" href="https://www.bronowski.it/blog/2021/02/t-sql-tuesday-135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>It’s time for February’s monthly blog party. This month is hosted by Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/MikeyBronowski" target="_blank" rel="noopener"
>t&lt;/a>) and he’s asking us to write about our ‘tools of the trade’.  He’s looking for those tools that make our lives easier, ones we couldn’t imagine going without. Thanks for hosting Mikey, can’t wait to read everyone’s contributions and add some tools to my toolbelt.&lt;/p>
&lt;p>I’m going to split this into a couple of sections. I’m sure you can all guess what’s up first though…&lt;/p>
&lt;h2 id="powershell">PowerShell&lt;/h2>
&lt;p>If I could only choose one tool for my toolbelt it would be PowerShell, which is actually probably cheating because there are so many options to import modules and add functionality.  I’m going to highlight five modules I use a lot below.&lt;/p>
&lt;ol>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbatools" target="_blank" rel="noopener"
>dbatools&lt;/a> – If you’ve read much of my blog before, or seen me present, it should be no surprise that dbatools is number one.  I use dbatools every day, whether it’s to check diskspace, update database owners, or a plethora of other uses.  In fact I previously wrote a post ‘&lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-101/" target="_blank" rel="noopener"
>The Multitool of my DBA toolbox&lt;/a>’ that highlights five great use cases.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a> – A close friend of dbatools, dbachecks combines Pester and dbatools to create an amazing infrastructure testing module.  This is perfect for creating a morning checks dashboard, or quickly checking certain parts of your estate. For example, in my post ‘&lt;a class="link" href="https://jesspomfret.com/dbachecks-importexcel/" target="_blank" rel="noopener"
>dbachecks meets ImportExcel&lt;/a>’ we check up on backups and database status before exporting to create an Excel report.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/pester/Pester" target="_blank" rel="noopener"
>Pester&lt;/a> – Originally designed for unit/integration testing, I personally use this framework to test anything you can write in PowerShell. It quickly provides a clear and easy to read answer for whether everything is as expected. I’ve written about it previously to ‘&lt;a class="link" href="https://jesspomfret.com/pester-test-cluster-role-owners/" target="_blank" rel="noopener"
>Pester test your Cluster Role Owners&lt;/a>’.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a> – This module lets you work with Excel objects, without having Excel installed.  Easily read data from spreadsheets into PowerShell, or export data to create detailed reports with a few lines of code. Our host for this T-SQL Tuesday has written a great series on this module, if you’re looking for inspiration. &lt;a class="link" href="https://www.bronowski.it/blog/tag/importexcel/" target="_blank" rel="noopener"
>importexcel Archives - Mikey Bronowski - Blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/PowershellFrameworkCollective/psframework" target="_blank" rel="noopener"
>PSFramework&lt;/a> – Finally, I want to highlight PSFramework.  Portions of this module are used within both dbatools and dbachecks.  It provides great options for both setting configuration options that can be then used in your modules as well as for creating great logging. I’ve switched to using Write-PSFMessage instead of Write-Host\Verbose\Output as it provides a lot more flexibility as well as writing to a physical log file.&lt;/li>
&lt;/ol>
&lt;p>I also recently wrote about &lt;a class="link" href="https://jesspomfret.com/psreadline-search-history/" target="_blank" rel="noopener"
>PowerShell’s interactive search functionality&lt;/a>, and after a poll on Twitter was pretty shocked by how few people knew about it.  I recommend checking it out, as it is a really handy built in feature.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/jpomfret/status/1357014555638042624" target="_blank" rel="noopener"
>https://twitter.com/jpomfret/status/1357014555638042624&lt;/a>&lt;/p>
&lt;h2 id="microsoft-excel">Microsoft Excel&lt;/h2>
&lt;p>Since I’ve written a lot about PowerShell previously, I wanted to highlight some other tools that I depend on. I’ve always been a fan of Excel, my personal life is full of spreadsheets – most decisions end with a spreadsheet (lucky for me, my wife is also a big fan of Excel!).  I often find myself copying data into Excel to keep track of work, or to quickly analyse data.  It’s also a great way of sharing data with a clear structure.  I’m also a big fan of shortcuts – so here’s a few I use often.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ctrl+;&lt;/strong> - Insert today’s date into the current cell – really useful, and avoids you having to remember we’re now in 2021!&lt;/li>
&lt;li>&lt;strong>Ctrl+l&lt;/strong> – Select a cell within a dataset, press Ctrl+l (lowercase L), press enter. Your data is transformed into a table.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ctrlL.gif"
loading="lazy"
alt="Gif showing using Ctrl&amp;#43;L in Excel to create a table"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ctrl+D&lt;/strong> – Fill down, this will copy the contents of the cell above into your current cell.  Also smart enough to handle continuation of formulas.&lt;/li>
&lt;li>&lt;strong>Ctrl+R&lt;/strong> – Fill right, same as above but it’ll copy the contents of the cell to your left into your current cell.&lt;/li>
&lt;li>&lt;strong>Ctrl+Up/Down arrow&lt;/strong> – This will move your cursor to either the first value in the current column, or the last.  I use this a lot for navigating around worksheets/tables.&lt;/li>
&lt;li>&lt;strong>F2&lt;/strong> – This edits a cell&amp;rsquo;s contents. It puts your cursor at the end of the value, but you can now use your arrow keys to move about in the cell. It also stops you accidentally overwriting what was already in the cell.&lt;/li>
&lt;/ul>
&lt;h2 id="my-bike">My Bike&lt;/h2>
&lt;p>My final tool is my bike. Not technical at all, but a tool I use to keep fit and have some fun.  I love cycling, and in the current times it’s my best option for fitness (I’m in England – we’re deep into lockdown 3 and gyms are closed). &lt;/p>
&lt;p>Honestly, I have a really hard time working out at home. I enjoy going to the gym, seeing some friendly faces and having someone tell me what to do for an hour.  It’s not the same at home, and my mood is instantly affected by not being active.&lt;/p>
&lt;p>However, I’m happy to go out for a ride, and living in the South of England the weather is reasonably kind all year round.  Previously, living in Ohio there weren’t many options for winter bike riding, unless you had fat tyres and loved the snow!  I’m also lucky to be close to the South Downs (pictured below), as well as plenty of country lanes to explore.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Bike_SouthDowns-1024x768.jpg"
loading="lazy"
alt="My bike on the south downs"
>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Thanks for reading and hope you’ve enjoyed digging through my toolbox. Thanks again to Mikey for hosting. I always enjoy participating in these T-SQL Tuesday’s, partly because it gives me a prompt to write about, partly because it’s fun to see what everyone else wrote about.&lt;/p>
&lt;p>Stay safe folks.&lt;/p></description></item><item><title>Easily Search PowerShell Command History With PSReadLine</title><link>https://jpomfret.github.io/easily-search-powershell-command-history-with-psreadline/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/easily-search-powershell-command-history-with-psreadline/</guid><description>&lt;p>I was listening to a &lt;a class="link" href="http://runasradio.com/Shows/Show/760" target="_blank" rel="noopener"
>podcast last week about PowerShell&lt;/a>, when one of the hosts mentioned having to ‘up arrow’ back through your history to find a command you wanted to rerun.  This made me realise that I should write this quick post on using &lt;a class="link" href="https://github.com/PowerShell/PSReadLine" target="_blank" rel="noopener"
>PSReadLine&lt;/a>’s interactive search function.  This tip is a serious time saver and I rely on it heavily.&lt;/p>
&lt;p>The great news is that if you are using Windows PowerShell on Windows 10 or if you’re using PowerShell 6+, PSReadLine is already installed and you can immediately start using this tip.  If you don’t have the module though, it’s easy enough to install from the PowerShell Gallery:    &lt;/p>
&lt;p>Install-Module PSReadLine&lt;/p>
&lt;p>The main goal of this module is to enhance the command line experience for users. There is a lot of great stuff in this module, some of which you’re probably already using, without even realising it’s coming from PSReadLine. A fun experiment for this is to open a new console, run &lt;code>Remove-Module PSReadline&lt;/code> and then see what’s missing. The two biggest things I notice (on top of the interactive search) is there’s no history available from previous sessions, and no colour coding to show the differences between variables, parameters and input.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/removePSReadLine.jpg"
loading="lazy"
alt="Showing no colour coding after removing PSReadLine"
>&lt;/p>
&lt;p>The screenshot above is using Windows Terminal and PowerShell 7, but the same thing happens using older versions of PowerShell. Shout out to &lt;a class="link" href="http://twitter.com/cl" target="_blank" rel="noopener"
>Chrissy LeMaire&lt;/a> for the beautiful &lt;a class="link" href="https://blog.netnerds.net/2020/07/my-windows-terminal-retro-theme/" target="_blank" rel="noopener"
>Windows Terminal theme&lt;/a>.&lt;/p>
&lt;p>As I mentioned earlier, when you’re running PowerShell in a console it tracks the commands you run, building up a history of all the things you’ve executed.  This is really useful if you want to slightly change the command you just ran, perhaps fixing a typo, or piping the output to another command. The problem comes when you know you ran something recently, and you start ‘up arrowing’ furiously through the history trying to find what you’re looking for.&lt;/p>
&lt;p>You can also view the history of your current session by executing &lt;code>history&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/history-1.jpg"
loading="lazy"
alt="powershell command history "
>&lt;/p>
&lt;p>PSReadLine makes looking for a specific command in the history haystack even easier with ‘Bash/zsh style interactive history search’.  I have created a gif below to demonstrate this, but it’s as simple as pressing ‘Ctrl+R’ from the console. That will create a second line under your prompt where you can start typing your search terms.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/interactiveSearch.jpg"
loading="lazy"
alt="Interactive back search"
>&lt;/p>
&lt;p>As you type PSReadLine is going back through your history to find the most recent command that matches what you’ve typed so far. As you continue typing it hones in on the exact command you’re looking for.  To look further back in your history, press ‘Ctrl+R’ again to find the next time you used that search term in a command.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/psreadline-1.gif"
loading="lazy"
>&lt;/p>
&lt;p>If you press `Ctrl+R` one too many times, you can press `Ctrl+S` to search forward, basically taking you forward in time one search result.&lt;/p>
&lt;p>Hope you find this as useful as I do. As I mentioned I rely heavily on interactive search to rerun commands I’ve used before.&lt;/p></description></item><item><title>Discover SQL Server Permissions hidden via AD Group Membership</title><link>https://jpomfret.github.io/discover-sql-server-permissions-hidden-via-ad-group-membership/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/discover-sql-server-permissions-hidden-via-ad-group-membership/</guid><description>&lt;p>When granting permissions to SQL Server resources we have a few options. One option is to grant permissions to Active Directory groups instead of individual users.  This has several benefits, for example, improved security over using SQL logins, and the ability to create a separation of duties when controlling database access.&lt;/p>
&lt;p>However, it does add an extra step when trying to determine who has what access to your SQL Servers. It can also make troubleshooting permission issues more challenging.  This post is going to aim to simplify this by combining dbatools and ActiveDirectory PowerShell modules to provide a clear solution.&lt;/p>
&lt;h2 id="setup">&lt;strong>Setup&lt;/strong>&lt;/h2>
&lt;p>This is obviously not needed in our actual environments, this is just how I prepared my lab so I could demonstrate how we can solve this problem.  Feel free to skip ahead to the solution if you already have plenty of AD groups to investigate.&lt;/p>
&lt;p>The full code sample is available in my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/07_PermisssionsGrantedViaADGroups.ps1" target="_blank" rel="noopener"
>GitHub demos repo&lt;/a>.&lt;/p>
&lt;p>First, I created some AD users and groups to use in my lab. This is easily achieved with the AD PowerShell module using &lt;code>New-AdUser&lt;/code> and &lt;code>New-AdGroup&lt;/code>. Then, using &lt;code>Add-AdGroupMember&lt;/code>, I added two users into the newly created group.&lt;/p>
&lt;p># setup - create some AD users\groups uing the ActiveDirectory module&lt;/p>
&lt;h1 id="create-several-new-ad-users">create several new ad users&lt;/h1>
&lt;p>(&amp;lsquo;pomfretJ&amp;rsquo;,&amp;lsquo;smithA&amp;rsquo;, &amp;lsquo;jonesP&amp;rsquo;,&amp;lsquo;barnesR&amp;rsquo;).foreach{New-AdUser $_}&lt;/p>
&lt;h1 id="view-newly-created-users">view newly created users&lt;/h1>
&lt;p>$date = (get-date).AddHours(-1)
get-aduser -filter {created -gt $date} | select name&lt;/p>
&lt;h1 id="create-a-new-ad-group">create a new Ad group&lt;/h1>
&lt;p>$newAdGroup = @{
Name = &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;
GroupCategory = &amp;lsquo;Security&amp;rsquo;
GroupScope = &amp;lsquo;Global&amp;rsquo;
Path = &amp;lsquo;CN=Users,DC=pomfret,DC=com&amp;rsquo;
}
New-ADGroup @newAdGroup&lt;/p>
&lt;h1 id="add-users-to-group">add users to group&lt;/h1>
&lt;p>$addMemberGroup = @{
Identity = &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;
Members = &amp;lsquo;pomfretj&amp;rsquo;, &amp;lsquo;jonesP&amp;rsquo;
}
Add-ADGroupMember @addMemberGroup&lt;/p>
&lt;p>The second part of the setup was to add an AD group and an AD user to the SQL Server and grant some permissions using dbatools.&lt;/p>
&lt;p># setup - grant permissions to ad users\groups using dbatools&lt;/p>
&lt;h1 id="add-ad-group-and-grant-permissions-db_datareader-to-adventureworks">add ad group and grant permissions (db_datareader to AdventureWorks)&lt;/h1>
&lt;p>New-DbaLogin -SqlInstance dscsvr1 -Login &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo;
New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo;
Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_datareader -User &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo; -Confirm:$false&lt;/p>
&lt;h1 id="add-ad-user-to-sql-server-and-provide-permissions-db_owner-to-adventureworks">add ad user to sql server and provide permissions (db_owner to AdventureWorks)&lt;/h1>
&lt;p>New-DbaLogin -SqlInstance dscsvr1 -Login &amp;lsquo;Pomfret\smithA&amp;rsquo;
New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login &amp;lsquo;Pomfret\smithA&amp;rsquo;
Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_owner -User &amp;lsquo;Pomfret\smithA&amp;rsquo; -Confirm:$false&lt;/p>
&lt;h2 id="viewing-database-access">&lt;strong>Viewing database access&lt;/strong>&lt;/h2>
&lt;p>Now that my lab environment is set up, let’s take a look at database users that have access to the AdventureWorks2017 database.  This is an easy task thanks to dbatools, we can just use &lt;code>Get-DbaDbUser&lt;/code>. Shown below, you can clearly see there is a WindowsUser &amp;lsquo;smithA&amp;rsquo; that has access, as well as a WindowsGroup &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;.&lt;/p>
&lt;p># Find users that have permissions through group membership
Get-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -ExcludeSystemUser | Select-Object SqlInstance, Database, Login, LoginType, HasDbAccess&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/GetDbaDbUser.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaDbUser.png"
loading="lazy"
alt="Get-DbaDbUser results"
>&lt;/a>&lt;/p>
&lt;p>We can also use &lt;code>Get-DbaDbRoleMember&lt;/code> to see exactly which database roles these users have been granted. &lt;/p>
&lt;p>Get-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 | Select-Object SqlInstance, Database, role, Login&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/Get-DbaDbRoleMember.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRoleMember.png"
loading="lazy"
alt="Get-DbaDbRoleMember output"
>&lt;/a>&lt;/p>
&lt;p>The issue is the same for both these examples, we don’t know which users are inheriting the permissions granted to the &amp;lsquo;AdventureWorksReadOnly&amp;rsquo; group. This is where we need to combine these two modules to get the answers we need.&lt;/p>
&lt;p>There are several ways you could combine the output of two functions. For this example I’m going to use a calculated property.&lt;/p>
&lt;p>If I run the exact same code as before to get a list of role members from the dbatools function &lt;code>Get-DbaDbRoleMember&lt;/code>, I can add a calculated property in the Select-Object to lookup the members of that specific group from active directory.  In the example below you can see the &amp;lsquo;AdventureWorksReadOnly&amp;rsquo; group has two members, and we now know that both &amp;lsquo;pomfretJ&amp;rsquo; and &amp;lsquo;jonesP&amp;rsquo; have read access to the AdventureWorks2017 database. &lt;/p>
&lt;p>You can also still see the WindowsUser, &amp;lsquo;smithA&amp;rsquo;, has db_owner permissions.  Since that lookup didn’t return any results (obviously, since it’s a user not a group), the GroupMembers property remains empty.&lt;/p>
&lt;p>Get-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 |
Select-Object SqlInstance, Database, Role, LoginType, Login, @{l=&amp;lsquo;GroupMembers&amp;rsquo;;e={ (Get-AdGroupMember -Identity ($_.Login).Split(&amp;rsquo;\&amp;rsquo;)[1]).Name }}&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/FinalOutput.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/FinalOutput.png"
loading="lazy"
alt="Combining Get-DbaDbRoleMember &amp; Get-AdGroupMember"
>&lt;/a>&lt;/p>
&lt;p>You can also use this same code to determine specific user access, for example, by adding a Where-Object to see just the permissions granted to &amp;lsquo;pomfretJ&amp;rsquo;.&lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>This should give you an easy option for determining specific user access that is hidden behind AD groups, and I think reduces one of the negatives of using AD groups in this situation.  It also shows us that we can combine multiple functions into one to get all the information we need with one easy line of code.&lt;/p>
&lt;p>I would also encourage you to explore the other permission related dbatools functions available, including &lt;code>Get-DbaServerRole&lt;/code> and &lt;code>Get-DbaPermission&lt;/code>. These can also be used in combination with &lt;code>Get-AdGroupMember&lt;/code> to enhance the results.&lt;/p></description></item><item><title>Easily Create A Copy Of Your Database For Testing</title><link>https://jpomfret.github.io/easily-create-a-copy-of-your-database-for-testing/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/easily-create-a-copy-of-your-database-for-testing/</guid><description>&lt;p>Have you ever wanted to quickly backup/restore a database to the same instance to do some side by side testing? Perhaps to make some index changes or code changes, without actually changing the live copy of the database?  Ideally you’d already have another environment for this sort of work, but even then sometimes it’s handy to have a quick option.&lt;/p>
&lt;p>Let’s first take a look at the databases on my SQL Server- we can use a GUI tool for that (SSMS, ADS) or we can use dbatools.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1 | Select-Object SqlInstance, Name, Status, Size&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDatabase.jpg"
loading="lazy"
alt="Get-DbaDatabase"
>&lt;/p>
&lt;p>We’re working hard on the AdventureWorks2017 database, perhaps getting it ready for an upgrade – since it’s now 3+ years out of date.&lt;/p>
&lt;p>dbatools has so many functions, and I know I’ve mentioned it before, but &lt;code>Find-DbaCommand&lt;/code> is a great way of looking for what we need. I want to know what the default backup path is set to, and since I’m just backing up and restoring to the same server, we already know that the instance has the required permissions here. If only there was an easy button for this…&lt;/p>
&lt;p>Find-DbaCommand *default*path*backup*&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/findcommand.jpg"
loading="lazy"
alt="results of Find-DbaCommand"
>&lt;/p>
&lt;p>Even just reading the synopsis, I can see that &lt;code>Get-DbaDefaultPath&lt;/code> will give me exactly what I need.  I recommend the next step is running &lt;code>Get-Help Get-DbaDefaultPath -ShowWindow&lt;/code>, that’ll create a popup that provides all the information you need about the function.&lt;/p>
&lt;p>The only required parameter is a SqlInstance, and you can see the backup property returns gives us the path we need for our copy.&lt;/p>
&lt;p>Get-DbaDefaultPath -SqlInstance mssql1&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDbaDefaultPath.jpg"
loading="lazy"
alt="Get-DbaDefaultPath output"
>&lt;/p>
&lt;p>That’s all the groundwork done- we have our instance, database, and a location to backup/restore from.  We’re going to want to check we have enough disk space available on both the instance and that backup path, then we’re ready to go.&lt;/p>
&lt;h2 id="using-copy-dbadatabase">&lt;strong>Using Copy-DbaDatabase&lt;/strong>&lt;/h2>
&lt;p>I’ve already spoken and blogged a lot about the power of this command (related links at the end of this post), but today’s tip is centred around a less than well-known parameter.  Hidden deep in the comment based help (another great reason to read all of &lt;code>Get-Help Copy-DbaDatabase -ShowWindow&lt;/code>) you’ll find the ‘Prefix’ parameter. This will allow us to easily add a prefix to both the database and the associated files, meaning we won’t have any issues restoring the database to the same server.&lt;/p>
&lt;p>-Prefix &lt;!-- raw HTML omitted --> &lt;br>
All copied database names and physical files will be prefixed with this string&lt;/p>
&lt;p>This option is mutually exclusive of NewName&lt;/p>
&lt;p>Required? false
Position? named
Default value &lt;br>
Accept pipeline input? False
Accept wildcard characters? false&lt;/p>
&lt;p>Here I’ve set a SqlInstance variable so I can reuse the same value multiple times in my code. Then created a hash table ‘$copySplat’ with the necessary parameters so we can utilise splatting (a way to improve code readability) to pass the whole set into &lt;code>Copy-DbaDatabase&lt;/code>. &lt;/p>
&lt;p>Two parameters I want to highlight- I’ve set ‘Prefix’, meaning the database and files for the restored database will start with ‘Test’.  I’ve also set SharedPath and used the code we already wrote to get the default backup path.&lt;/p>
&lt;p>$sqlInstance = &amp;lsquo;mssql1&amp;rsquo;&lt;/p>
&lt;p>$copySplat = @{
Source = $sqlInstance
Destination = $sqlInstance
Database = &amp;lsquo;AdventureWorks2017&amp;rsquo;
BackupRestore = $true
SharedPath = (Get-DbaDefaultPath -SqlInstance $sqlInstance).Backup
Prefix = &amp;lsquo;Test&amp;rsquo;
}
Copy-DbaDatabase @copySplat&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/copyDatabase.jpg"
loading="lazy"
alt="Copy-DbaDatabase"
>&lt;/p>
&lt;p>The output below shows the migration was successful, and there were no warnings or errors (those would appear in the notes column).&lt;/p>
&lt;p>Finally, let’s confirm it worked by rerunning our &lt;code>Get-DbaDatabase&lt;/code> command again:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDatabaseAfter.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Extra proof, it’s now accessible through Azure Data Studio (ADS) and we’re ready to start our testing.  One note, if you are on the same server it’s important to confirm any code you run isn’t referencing the original database name.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ADSView.jpg"
loading="lazy"
>&lt;/p>
&lt;h2 id="additional-content">&lt;strong>Additional content&lt;/strong>&lt;/h2>
&lt;p>As I mentioned I have already spoken and written about the power of &lt;code>Copy-DbaDatabase&lt;/code>, one of my favourite commands.  If you’d like to read more, I’ve written a post on the dbatools blog, &lt;a class="link" href="https://dbatools.io/migrating-application-dbs/" target="_blank" rel="noopener"
>migrating application databases with dbatools&lt;/a>.&lt;/p>
&lt;p>I’ve also recorded a short ‘Life hack’ video, &lt;a class="link" href="https://www.youtube.com/watch?v=Fraig15pwxE&amp;amp;t=1s" target="_blank" rel="noopener"
>easy database migrations with dbatools&lt;/a> that I’ve published on my YouTube channel.&lt;/p></description></item><item><title>Advent of Code 2020</title><link>https://jpomfret.github.io/advent-of-code-2020/</link><pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/advent-of-code-2020/</guid><description>&lt;p>This was the 3rd year I participated in the &lt;a class="link" href="https://adventofcode.com/" target="_blank" rel="noopener"
>Advent of Code&lt;/a> (AoC). If you haven’t heard of AoC, it’s an advent calendar of coding puzzles.  Each day between December 1st and 25th a two part puzzle is released, you can use whatever language you want to solve it, with the goal being just to get the right answer.  Once you solve part 1 for the day, part 2 is unlocked and that builds on top of the story you had for part 1. For each part of each puzzle you complete you get a star, so there are two available per day.&lt;/p>
&lt;p>This year I managed to complete both parts of the first 9 days of the calendar, and then just the first part of days 10 and 15, that’s 20 total stars out of a possible 50.  That doesn’t sound great, less than 50%, so why am I writing a blog about this mediocre performance?&lt;/p>
&lt;p>My goal was to gain more stars than last year, which I succeeded at. I only got 6 total stars last year. Now my goal for next year will be to beat this year&amp;rsquo;s performance.  I did learn several neat things while working on these puzzles and those I thought were worth sharing.&lt;/p>
&lt;h2 id="named-loops-in-powershell">&lt;strong>Named Loops in PowerShell&lt;/strong>&lt;/h2>
&lt;p>A lot of the puzzles involve iterating over an object and manipulating it. I depended on a lot of loops for this. My day 1, part 1 solution is below.  You can see I nested two loops to iterate over the array and calculate the total. Without the named loops this worked – it just didn’t stop when it found the correct answer and I got duplicates.  By naming the outer loop with &lt;code>:expenses&lt;/code> you can then break all the way out of that loop with &lt;code>break expenses&lt;/code>.  Pretty useful!&lt;/p>
&lt;p>$expenses = Get-Content .\Day01\Input.txt&lt;/p>
&lt;h1 id="part-1---514579">Part 1 - 514579&lt;/h1>
&lt;p>:expenses
foreach ($e in $expenses) {
foreach ($f in $expenses) {
if ([int]$e + [int]$f -eq 2020) {
(&amp;ldquo;Part 1 answer: {0}&amp;rdquo; -f ([int]$e * [int]$f))
break expenses
}
}
}&lt;/p>
&lt;h2 id="split-string-into-multiple-variables-at-once">&lt;strong>Split string into multiple variables at once&lt;/strong>&lt;/h2>
&lt;p>A lot of the puzzles input required some string manipulation. Some of this I used regex for, and if it wasn’t that complicated I could use the split method.  Previously I had always split strings into variables by first splitting into an array, and then specifying the index of the item for each variable:&lt;/p>
&lt;p>$split = (&amp;rsquo;test string&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;)
$firstVariable = $split[0]
$secondVariable = $split[1]&lt;/p>
&lt;p>PowerShell has an easier option though- you can accomplish this same behaviour with just one line:&lt;/p>
&lt;p>$firstVariable, $secondVariable = (&amp;rsquo;test string&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;)&lt;/p>
&lt;h2 id="split-a-string-into-a-maximum-number-of-substrings">&lt;strong>Split a string into a maximum number of substrings&lt;/strong>&lt;/h2>
&lt;p>Another string splitting tip is if you only want to split a certain number of times there is an overload for the string method for that. I have used the split method for years, but I have never looked any further into what it can do.  A handy reminder that reading the docs for even simple methods/functions is a worthwhile endeavour (perhaps a 2021 goal?!?).  &lt;/p>
&lt;p>(&amp;lsquo;I only want two substrings&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;,2)&lt;/p>
&lt;p>This results in:&lt;/p>
&lt;p>I
only want two substrings.&lt;/p>
&lt;h2 id="im-not-a-computer-scientist">&lt;strong>I’m not a Computer Scientist&lt;/strong>&lt;/h2>
&lt;p>The final thing I learnt is that I’m not a computer scientist.  The first few days of puzzles were pretty straightforward – I had no problem working out what was needed and writing a solution.  Was it the most effective and beautiful code ever, probably not, but it got the right answer and that was all we needed.  Once we got into the second week the difficulty picked up- I didn’t study maths or computer science and found I was severely lacking when it came to needing more complicated algorithms to solve the puzzles.&lt;/p>
&lt;p>That’s ok though. Although it’s definitely a gap in knowledge when it comes to solving code puzzles, it hasn’t really caused problems or issues in my day to day work. I’m still able to use PowerShell to automate tasks and manage a large database estate.&lt;/p>
&lt;p>Saying that, I am interested in learning more about these topics. I love a puzzle and the Advent of Code is a great way to finish the year with some challenges and learning.&lt;/p>
&lt;p>If you&amp;rsquo;re interested in my efforts, all of my code is on &lt;a class="link" href="https://github.com/jpomfret/AdventOfCode2020" target="_blank" rel="noopener"
>Github&lt;/a>.&lt;/p></description></item><item><title>dbachecks and Azure SQL Databases</title><link>https://jpomfret.github.io/dbachecks-and-azure-sql-databases/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbachecks-and-azure-sql-databases/</guid><description>&lt;p>Last week I gave a presentation at &lt;a class="link" href="https://www.meetup.com/SQL-South-West/" target="_blank" rel="noopener"
>Data South West&lt;/a> on dbachecks and dbatools. One of the questions I got was whether you could run dbachecks against Azure SQL Databases, to which I had no idea. I always try to be prepared for potential questions that might come up, but I had only been thinking about on-premises environments and hadn’t even considered the cloud.  The benefit is this gives me a great topic for a blog post.&lt;/p>
&lt;h2 id="step-1--create-an-azure-sql-database">&lt;strong>Step 1 – Create an Azure SQL Database&lt;/strong>&lt;/h2>
&lt;p>I created a SQL Database through the &lt;a class="link" href="https://portal.azure.com/" target="_blank" rel="noopener"
>Azure Portal&lt;/a>. The wizard is pretty straightforward and the only real decisions needed were around sizing. Since this is just going to be for a test environment I chose a small ‘Basic’ database.&lt;/p>
&lt;h2 id="step-2--connect-to-the-database">&lt;strong>Step 2 – Connect to the Database&lt;/strong>&lt;/h2>
&lt;p>Once the database had been created I navigated to the resource pane in the portal. At the top there is a drop down that helps you get connected using Azure Data Studio.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Connect-1024x203.jpg"
loading="lazy"
alt="Azure SQL Database pane in Azure Portal"
>&lt;/p>
&lt;p>Once Azure Data Studio opened, I was asked to confirm I wanted to connect:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ConnectSure.jpg"
loading="lazy"
alt="Confirmation prompt for connection"
>&lt;/p>
&lt;p>Then a pane opened which enabled me to easily add a firewall rule so client IP could access the Azure SQL Database.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/firewall.jpg"
loading="lazy"
alt="Pane in ADS to configure a firewall rule"
>&lt;/p>
&lt;p>Once that was completed I was connected through Azure Data Studio and able to interact with my server and database.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ADSConnected-1024x382.jpg"
loading="lazy"
alt="ADS connected to Azure SQL Database"
>&lt;/p>
&lt;p>Connecting first through Azure Data Studio is not a requirement, but it does help us to get the firewall rules configured and confirm that connecting from our client machine will not be an issue.&lt;/p>
&lt;p>Another good check to ensure we can connect to our database from PowerShell is to use dbatools’ &lt;code>Connect-DbaInstance&lt;/code>:&lt;/p>
&lt;p>$cred = Get-Credential
Connect-DbaInstance -SqlInstance &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo; -SqlCredential $cred&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/connectDbatools-1024x90.jpg"
loading="lazy"
alt="Connect-DbaInstance results"
>&lt;/p>
&lt;p>The results show we were able to connect successfully.&lt;/p>
&lt;h2 id="step-3--run-some-dbachecks">&lt;strong>Step 3 – Run some dbachecks&lt;/strong>&lt;/h2>
&lt;p>First of all let’s run a single check to ensure our database is online and in the expected state. For this we can use the ‘DatabaseStatus’ check.&lt;/p>
&lt;p>$checkSplat = @{
SqlInstance = &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo;
SqlCredential = $cred
Check = &amp;lsquo;DatabaseStatus&amp;rsquo;
}
Invoke-DbcCheck @checkSplat&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/databaseStatus-1-1024x210.jpg"
loading="lazy"
alt="DatabaseStatus checks succeed"
>&lt;/p>
&lt;p>Here you can easily see, both because of the green result text and plus icon to the left, that our tests were successful. Both the database we created, AzDb01, and the master database are online and have the expected status.&lt;/p>
&lt;p>dbachecks uses tags on the pester tests to enable you to either call specific tests or groups of checks. Each check has a unique tag. In our previous example it was &lt;code>DatabaseStatus&lt;/code> as well as tags that group like checks, for example &lt;code>Database&lt;/code>.&lt;/p>
&lt;p>$checkSplat = @{
SqlInstance = &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo;
SqlCredential = $cred
Check = &amp;lsquo;Database’
}
Invoke-DbcCheck @checkSplat&lt;/p>
&lt;p>Running all the database checks against our Azure SQL Database we get some failures.&lt;/p>
&lt;p>Tests completed in 70.68s
Tests Passed: 37, Failed: 28, Skipped: 8, Pending: 0, Inconclusive: 0&lt;/p>
&lt;p>There are a lot of tests that pass or fail with valid reasons. However, some of the failures are due to errors running the check. These are to be expected since this is a PaaS (Platform as a Service) database offering. One example is the suspect pages check.&lt;/p>
&lt;p>The test failed due to an error in the context block, and it clearly states that the &amp;lsquo;msdb.dbo.suspect_pages&amp;rsquo; table isn’t available in this version of SQL Server.&lt;/p>
&lt;p>SqlException: Reference to database and/or server name in &amp;lsquo;msdb.dbo.suspect_pages&amp;rsquo; is not supported in this version of SQL Server.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FailedSuspectPages-1024x192.jpg"
loading="lazy"
alt="dbachecks output fails for suspect pages"
>&lt;/p>
&lt;p>There are plenty of tests that do work against an Azure SQL Database though, allowing you to keep tabs on many different aspects of your database including:&lt;/p>
&lt;ul>
&lt;li>Database Collation&lt;/li>
&lt;li>Database Owners&lt;/li>
&lt;li>Column Identity Usage&lt;/li>
&lt;li>Duplicate Index&lt;/li>
&lt;li>Disabled Index&lt;/li>
&lt;li>Auto Shrink&lt;/li>
&lt;li>Database Orphaned User&lt;/li>
&lt;li>Compatibility Level&lt;/li>
&lt;li>Database Status&lt;/li>
&lt;li>Database Exists&lt;/li>
&lt;li>And more…&lt;/li>
&lt;/ul>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>So to answer the question: yes, we can run dbachecks against our Azure SQL Databases. As long as we can connect and the version of SQL Supports the features needed to run the test we can ensure our databases in the cloud are configured just how we like them.&lt;/p></description></item><item><title>Ensure Query Store meets best practice across your environment</title><link>https://jpomfret.github.io/ensure-query-store-meets-best-practice-across-your-environment/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/ensure-query-store-meets-best-practice-across-your-environment/</guid><description>&lt;p>It’s no secret that I love dbatools and dbachecks. I am certain that I run a dbatools command at least once a day. It has fundamentally changed how I work as a dba, and it makes my life so much easier.  I mention this when I’m presenting on these topics, but today I want to highlight what I consider the special sauce of open-source software.&lt;/p>
&lt;p>dbatools not only provides us with easy to run functions that get/set/test so many aspects of our environments, but it also encapsulates the knowledge of industry experts, and that right there is part of the magic. For example, I no longer have to remember &lt;a class="link" href="https://www.sqlskills.com/blogs/jonathan/how-much-memory-does-my-sql-server-actually-need/" target="_blank" rel="noopener"
>Jonathan Kehayias’ detailed calculations&lt;/a> for max memory. I can just run &lt;a class="link" href="https://docs.dbatools.io/#Test-DbaMaxMemory" target="_blank" rel="noopener"
>Test-DbaMaxMemory&lt;/a> to check whether I need to adjust my settings.&lt;/p>
&lt;p>I would like to just say I’m in no way suggesting that we can skip the learning here, reading posts from experts and understanding why is vital – just it’s nice to be able to quickly call this knowledge from PowerShell rather from the depths of my brain (and that’s hoping it’s still stored in there).&lt;/p>
&lt;h2 id="adding-query-store-expertise-to-dbatools">&lt;strong>Adding Query Store Expertise to dbatools&lt;/strong>&lt;/h2>
&lt;p>Last week I was working on configuring Query Store, and knowing that Erin Stellato (&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/erinstellato" target="_blank" rel="noopener"
>t&lt;/a>) is the expert on that I headed over to her blog. I found exactly what I needed. Erin has a bunch of great posts on query store, but this one caught my eye: &amp;lsquo;&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/query-store-best-practices" target="_blank" rel="noopener"
>Query Store Best Practices&lt;/a>&amp;rsquo;.&lt;/p>
&lt;p>I read through her suggestions and could easily translate those using dbatools to optimally configure Query Store. First by setting several query store options using &lt;code>Set-DbaDbQueryStoreOption&lt;/code>:&lt;/p>
&lt;p>$queryStoreBP = @{
SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
Database = &amp;lsquo;TestDb&amp;rsquo;
State = &amp;lsquo;ReadWrite&amp;rsquo;
MaxSize = 2048
CaptureMode = &amp;lsquo;Auto&amp;rsquo;
CollectionInterval = 30
}
Set-DbaDbQueryStoreOption @queryStoreBP&lt;/p>
&lt;p>Secondly by configuring two trace flags using &lt;code>Set-DbaStartupParameter&lt;/code> (reboot required):&lt;/p>
&lt;p>Set-DbaStartupParameter -SqlInstance mssql1 -TraceFlag 7745,7752&lt;/p>
&lt;p>Once I was happy with my settings, I realised we were missing a ‘test’ command for dbatools. The suite of ‘test’ functions in dbatools (a lot that end up as checks in dbachecks btw!), give us an easy way to check our environment against best practices, or our desired settings.&lt;/p>
&lt;p>Since dbatools is open-source I was able to write this function (&lt;a class="link" href="https://docs.dbatools.io/#Test-DbaDbQueryStore" target="_blank" rel="noopener"
>Test-DbaDbQueryStore&lt;/a>) and get it added into the module. It’s included as of version 1.0.131, so make sure you’re up to date.  Taking Erin’s suggestions and wrapping them in a little PowerShell, I can make it easier for myself and everyone else to make sure we’re following her guidelines.&lt;/p>
&lt;p>To test a single database you can use the following. It will output each setting, it’s current value, the recommended value, as well as a note from Erin’s blog post on why we should choose that. Again, I recommend you read her post to fully understand the why.&lt;/p>
&lt;p>Test-DbaDbQueryStore -SqlInstance mssql1 -Database testdb&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/querystoreBP.jpg"
loading="lazy"
alt="Sample output from Test-DbaDbQueryStore showing a couple of best practices"
>&lt;/p>
&lt;p>This also means we can find any databases across many instances that aren’t set up to meet best practices:&lt;/p>
&lt;p>$results = Test-DbaDbQueryStore -SqlInstance mssql1, mssql2 |
Where-Object {-not $_.IsBestPractice} |
Select-Object SqlInstance, Database, Name, Value, RecommendedValue
$results | Format-Table&lt;/p>
&lt;h2 id="so-over-to-you">So, over to you&lt;/h2>
&lt;p>Step 1 – Go and read the why - &amp;lsquo;&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/query-store-best-practices" target="_blank" rel="noopener"
>Query Store Best Practices&lt;/a>&amp;rsquo;.&lt;/p>
&lt;p>Step 2 – Easily make sure your environment is up to par.&lt;/p></description></item><item><title>Pester test your Cluster Role Owners</title><link>https://jpomfret.github.io/pester-test-your-cluster-role-owners/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/pester-test-your-cluster-role-owners/</guid><description>&lt;p>In an ideal situation it probably shouldn’t matter which node of a failover cluster your resources and roles are hosted on, but the real world is often far from ideal.  This post will talk through how we can record the current owner nodes and then use Pester to ensure we’re in the ideal configuration. This could be useful post maintenance activities or as a daily check to ensure things are as you expect.&lt;/p>
&lt;h2 id="step-1--store-the-current-resource-owners">&lt;strong>Step 1 – Store the current resource owners&lt;/strong>&lt;/h2>
&lt;p>If we are going to test that we’re in our expected configuration, we need to record what that configuration looks like.  I have a hard coded list of cluster names. However, you could easily pull them from a text file, or a database.  Once we have the list of clusters we can use &lt;code>Get-ClusterGroup&lt;/code> to determine the cluster roles and their current owners.&lt;/p>
&lt;p>To persist this owner information I’m using &lt;code>ConvertTo-Json&lt;/code> and then outputting it to a file. This creates a file that can easily be read back into PowerShell as an object using &lt;code>ConvertFrom-Json&lt;/code>.&lt;/p>
&lt;p>It’s also probably worth mentioning that this ideal configuration can be stored in source control. That’ll keep the file safe and you can easily keep track of any changes that are made to it.&lt;/p>
&lt;p>$clusters = ‘ClusterName1’,’ClusterName2’
$owners = $clusters | % { Get-ClusterGroup -Cluster $PSItem | select Cluster, Name, State, OwnerNode }
$owners | % {
[PSCustomObject] @{
Cluster = $_.Cluster.Name
Name = $_.Name
OwnerNode = $_.OwnerNode.Name
State = $_.State -as [string]
}
} | ConvertTo-Json | Out-File ClusterGroupOwners.json&lt;/p>
&lt;p>You’ll notice I’m creating a &lt;code>PSCustomObject&lt;/code> to pipe to the &lt;code>ConvertTo-Json&lt;/code>. Without that, the object from &lt;code>Get-ClusterGroup&lt;/code> is exploded, with all properties, including nested properties exported into the JSON output. This is more than we need, and I think there is some value in having a clear concise output file. &lt;/p>
&lt;p>I’m also using &lt;code>-as [string]&lt;/code> on the state property. PowerShell automatically translates the real state to a text value when outputted as it’s an enumeration type – but when you pipe that to &lt;code>ConvertTo-Json&lt;/code> you get the raw integer value.&lt;/p>
&lt;h2 id="step-2--test-the-current-configuration">&lt;strong>Step 2 – Test the current configuration&lt;/strong>&lt;/h2>
&lt;p>When it’s time to test our configuration we can read in our ClusterGroupOwners.json and then convert it back to a PowerShell object using &lt;code>ConvertFrom-Json&lt;/code>.  Now we have a PowerShell object of our ideal configuration we can loop through each cluster, checking the current group owners using &lt;code>Get-ClusterGroup&lt;/code> again.  This current state can then be matched against the desired configuration.&lt;/p>
&lt;p>I am using a pretty simple pester test for this work, saving it as Check-ClusterOwners.tests.ps1.&lt;/p>
&lt;p>$desiredConfig = Get-Content ClusterGroupOwners.json | ConvertFrom-Json
$clusters = $desiredConfig | Select -Unique Cluster&lt;/p>
&lt;p>Describe &amp;lsquo;The cluster resources should be owned by the same node as before&amp;rsquo; -Tag ClusterOwner {
Foreach ($cls in $clusters) {
Context (&amp;lsquo;Cluster owners are the same for {0}&amp;rsquo; -f $cls.Cluster) {
$groups = $desiredConfig | Where-Object Cluster -eq $cls.Cluster
$currentOwner = Get-ClusterGroup -Cluster $cls.Cluster
foreach ($grp in $groups) {
It (&amp;rsquo;{0} should be owned by {1}&amp;rsquo; -f $grp.Name, $grp.OwnerNode) {
($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode
}
}
}
}
}&lt;/p>
&lt;p>We’ll call this test using &lt;code>Invoke-Pester .\Check-ClusterOwners.tests.ps1&lt;/code>.&lt;/p>
&lt;p>If everything is as expected we’ll get output similar to this for each cluster – depending on the resources you have set up in your cluster.&lt;/p>
&lt;p>Describing The cluster resources should be owned by the same node as before
Context Cluster owners are the same for clustername
[+] RoleName should be owned by nodename 56ms
[+] RoleName2 should be owned by nodename 92ms&lt;/p>
&lt;p>If you have a resource that is not on the node that is expected, you’ll easily be able to see that in the output:&lt;/p>
&lt;p>Context Cluster owners are the same for clusterName
[-] RoleName should be owned by NodeB 102ms
Expected strings to be the same, but they were different.
String lengths are both 13.
Strings differ at index 12.
Expected: &amp;lsquo;NodeB&amp;rsquo;
But was: &amp;lsquo;NodeA&amp;rsquo;
13: ($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode&lt;/p>
&lt;p>This method of testing can be useful to ensure you’re in the ideal state in many scenarios. For example you could store any databases in your estate that are not ‘online’ and then confirm post reboots/patching that all the databases are in the expected state.&lt;/p></description></item><item><title>T-SQL Tuesday #130 – Automate Your Stress Away</title><link>https://jpomfret.github.io/t-sql-tuesday-#130-automate-your-stress-away/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#130-automate-your-stress-away/</guid><description>&lt;p>&lt;a class="link" href="https://sqlzelda.wordpress.com/2020/09/01/t-sql-tuesday-130-automate-your-stress-away/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Thanks to Elizabeth Nobel (&lt;a class="link" href="https://sqlzelda.wordpress.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SQLZelda" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month’s T-SQL Tuesday party and apologies for being as late as possible to the party! I love the topic of automation so felt sure I’d write something and then time slipped away. Luckily Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/MikeyBronowski" target="_blank" rel="noopener"
>t&lt;/a>) convinced me that it wasn’t too late to write something on my lunch break today (Wednesday in the UK) as it’s still Tuesday on Baker Island. Interesting fact Baker Island uses UTC-12:00 because since it’s uninhabited the islands time zone is unspecified (&lt;a class="link" href="https://en.wikipedia.org/wiki/Baker_Island" target="_blank" rel="noopener"
>Wikipedia&lt;/a>).&lt;/p>
&lt;h2 id="automating-dbachecks-with-scheduled-task">Automating dbachecks with scheduled task&lt;/h2>
&lt;p>I wanted to write about automating your daily checks with dbachecks, there are many ways of expanding on this post, but this should give you a good basis to build from.&lt;/p>
&lt;h3 id="the-environment">The environment&lt;/h3>
&lt;p>I have two docker containers running on my laptop, one running SQL Server 2017 and one running SQL Server 2019. I will use these SQL Server instances to run my sample daily checks against.&lt;/p>
&lt;p>I have also created a database on the 2019 instance (mssql2) called dbachecks to store our daily check results.&lt;/p>
&lt;h3 id="the-checks">The checks&lt;/h3>
&lt;p>There are hundreds of checks available within the dbachecks module, and on top of that you can even &lt;a class="link" href="https://nocolumnname.blog/2018/02/22/adding-your-own-checks-to-dbachecks/" target="_blank" rel="noopener"
>write your own&lt;/a> and include those. For this example I’m going to use the ‘DatabaseStatus’ check to ensure all my databases are online as expected.&lt;/p>
&lt;h3 id="the-automation">The automation&lt;/h3>
&lt;p>To automate the running of our daily checks we’ll first create a PowerShell script and then schedule that using task scheduler.  If you have other enterprise scheduling tools available you could easily use those instead to invoke the PowerShell script.&lt;/p>
&lt;p>The script for my example, shown below, is pretty simple. I have a section to define where the data will be stored (the ability to save dbachecks result information straight into a database was introduced with dbachecks 2.0 and so I would highly recommend updating if you’re on an earlier version).&lt;/p>
&lt;p>The next section (lines 7-9) lists my SQL instances that I want to check, and the checks that should be run.  The list of SQL instances could easily be pulled from a text file, a central management server (CMS) or a database to enhance the script.&lt;/p>
&lt;p>The final three lines (lines 11-13) run the checks, apply a label of ‘MorningChecks’ (this allows for grouping of test results in the reports) and then inserts the results into the database.&lt;/p>
&lt;p>Import-Module dbachecks, dbatools&lt;/p>
&lt;h2 id="dbachecks-database-connection">Dbachecks Database Connection&lt;/h2>
&lt;p>$dbachecksServer = &amp;lsquo;mssql2&amp;rsquo;
$dbachecksDatabase = &amp;lsquo;dbachecks&amp;rsquo;&lt;/p>
&lt;h2 id="define-instances-and-checks-to-run">Define instances and checks to run&lt;/h2>
&lt;p>$SqlInstance = &amp;lsquo;mssql1&amp;rsquo;,&amp;lsquo;mssql2&amp;rsquo;
$checks = &amp;lsquo;DatabaseStatus&amp;rsquo;&lt;/p>
&lt;p>Invoke-DbcCheck -SqlInstance $SqlInstance -Checks $checks -PassThru |
Convert-DbcResult -Label &amp;lsquo;MorningChecks&amp;rsquo; |
Write-DbcTable -SqlInstance $dbachecksServer -Database $dbachecksDatabase&lt;/p>
&lt;p>I saved this script to &lt;code>C:\dbachecks\dbachecks.ps1&lt;/code> and then ran the following PowerShell to schedule the execution of the script daily at 7am.&lt;/p>
&lt;p>$RunAs = Get-Credential
$taskSplat = @{
TaskName = &amp;lsquo;Daily dbachecks&amp;rsquo;
Action = (New-ScheduledTaskAction -Execute &amp;lsquo;powershell&amp;rsquo; -Argument &amp;lsquo;-File dbachecks.ps1&amp;rsquo; -WorkingDirectory C:\dbachecks)
Trigger = (New-ScheduledTaskTrigger -Daily -At &amp;lsquo;07:00&amp;rsquo;)
User = $RunAs.UserName
Password = ($RunAs.GetNetworkCredential().Password)
RunLevel = &amp;lsquo;Highest&amp;rsquo;
}
Register-ScheduledTask @taskSplat&lt;/p>
&lt;p>It’s important to note that the account used to run this scheduled task needs to be an account that has access to all of the SQL instances you want to check, as well as the SQL instance you are writing the final data to.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>Since this is now scheduled daily we can grab our morning coffee, sit down at our desk and immediately review our estate and ensure everything is as expected.&lt;/p>
&lt;p>We wrote the data to a SQL Server so you can go and query the data directly. By default there will be two tables created in the database.&lt;/p>
&lt;ul>
&lt;li>CheckResults – contains the actual results of the checks against your server&lt;/li>
&lt;li>dbachecksChecks – contains the metadata of the checks including tags and descriptions for each check you have invoked.&lt;/li>
&lt;/ul>
&lt;p>The other option is to use the dbachecks PowerBi dashboard, by running the following you can load the dashboard and connect to your dbachecks results database:&lt;/p>
&lt;p>Start-DbcPowerBi -FromDatabase&lt;/p>
&lt;p>When this opens you can see there were some failures on mssql1, right clicking on the orange bar you can drill through to see the details.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/09/dashboard-1.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/dashboard-1-1024x583.jpg"
loading="lazy"
alt="dbachecks main dashboard"
>&lt;/a>&lt;/p>
&lt;p>On the details pane you can see there are two offline databases that I need to look into.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/09/drillthrough.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/drillthrough-1024x279.jpg"
loading="lazy"
alt="details view of dbachecks PowerBi"
>&lt;/a>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Finally, this automation is just the starting piece of automating your daily checks. There are many ways to expand on this, but this is how you can get started with automating daily health checks with dbachecks.&lt;/p>
&lt;p>Thanks again for hosting, and sorry for being so late!&lt;/p></description></item><item><title>dbachecks meets ImportExcel</title><link>https://jpomfret.github.io/dbachecks-meets-importexcel/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbachecks-meets-importexcel/</guid><description>&lt;p>I got a message from a friend on Twitter last night asking ‘Is there an easy way to get dbachecks backup info into an Excel spreadsheet?’.  I sent them a couple of ideas, but figured this is a great use case that many people might be interested in. Pairing infrastructure testing using dbachecks with creating Excel reports with the ImportExcel module is a great addition to your automation tool belt. I also had ImportExcel on my mind this week after watching some great demos from Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog/2020/06/powershell-into-excelimportexcel-module-part-1/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/mikeybronowski" target="_blank" rel="noopener"
>t&lt;/a>) at a user group earlier this week.&lt;/p>
&lt;h2 id="run-the-checks">&lt;strong>Run the Checks&lt;/strong>&lt;/h2>
&lt;p>First step is to run some checks. I’ve previously written about using &lt;a class="link" href="https://jesspomfret.com/checking-backups-with-dbachecks/" target="_blank" rel="noopener"
>dbachecks to check on your SQL Server database backups&lt;/a>, so I’m going to use that as a base here.&lt;/p>
&lt;p>I’m not going to change any of the configuration options, but that is covered in the post I linked to above. I am going to add the &lt;code>DatabaseStatus&lt;/code> check with the default configuration to ensure all my databases are online.&lt;/p>
&lt;p>$sqlinstances = &amp;lsquo;mssql1&amp;rsquo;,&amp;lsquo;mssql2&amp;rsquo;,&amp;lsquo;mssql3&amp;rsquo;,&amp;lsquo;mssql4&amp;rsquo;
$testResults = Invoke-DbcCheck -SqlInstance $sqlinstances -Check LastBackup, DatabaseStatus -PassThru&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/08/tests.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tests.jpg"
loading="lazy"
alt="dbachecks results in PowerShell"
>&lt;/a>&lt;/p>
&lt;p>You can see I have a nice balance of green (passed tests) and red (failed tests). Not really the balance we’re looking for in production, but perfect for a demo environment.&lt;/p>
&lt;p>Using the &lt;code>-PassThru&lt;/code> parameter means that the test results are both displayed on screen and saved to my $testResults variable. We’ll use that to create our report.&lt;/p>
&lt;h2 id="create-the-report--option-1-export-csv">&lt;strong>Create the Report – Option 1: Export-Csv&lt;/strong>&lt;/h2>
&lt;p>The first option we have here is to just get the data into a csv. We can do that natively in PowerShell using the &lt;code>Export-Csv&lt;/code> function.&lt;/p>
&lt;p>$testResults.TestResult |
Select-Object Describe, Context, Name, Result, FailureMessage |
Export-Csv c:\temp\backups.csv -NoTypeInformation&lt;/p>
&lt;p>This will get our data into a csv, which we can then manipulate in Excel.&lt;/p>
&lt;h2 id="create-the-report--option-2-export-excel">&lt;strong>Create the Report – Option 2: Export-Excel&lt;/strong>&lt;/h2>
&lt;p>The second option is to use the ImportExcel module. This is easily in my top 5 all-time favourite PowerShell modules. With this module we can create a great looking Excel report in just a few lines. The following will take our test results and create two worksheets in one Excel file.  The first sheet will contain our raw data, formatted as an Excel table with some conditional formatting to highlight the failed tests. The second tab will contain a pivot table/chart of our results broken down by the test type and result.&lt;/p>
&lt;p>$ConditionalFormat =$(
New-ConditionalText -Text Failed -Range &amp;lsquo;D:D&amp;rsquo;
)&lt;/p>
&lt;p>$excelSplat = @{
Path = &amp;lsquo;C:\Temp\Backups.xlsx&amp;rsquo;
WorkSheetName = &amp;lsquo;TestResults&amp;rsquo;
TableName = &amp;lsquo;Results&amp;rsquo;
Autosize = $true
ConditionalFormat = $ConditionalFormat
IncludePivotTable = $true
PivotRows = &amp;lsquo;Describe&amp;rsquo;
PivotData = @{Describe=&amp;lsquo;Count&amp;rsquo;}
PivotColumns = &amp;lsquo;Result&amp;rsquo;
IncludePivotChart = $true
ChartType = &amp;lsquo;ColumnStacked&amp;rsquo;
}&lt;/p>
&lt;p>$testResults.TestResult |
Select-Object Describe, Context, Name, Result, FailureMessage |
Export-Excel @excelSplat&lt;/p>
&lt;p>The final results are shown below. With so many more checks available in the dbachecks module it would be easy to expand on this example and get a comprehensive report of your environment.&lt;/p>
&lt;p>The full script is available on &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/06_dbachecksToExcel.ps1" target="_blank" rel="noopener"
>my Github&lt;/a> demos repo.&lt;/p>
&lt;p>Results Worksheet:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/excelSheet-1024x369.jpg"
loading="lazy"
alt="excel screenshot showing results worksheet"
>&lt;/p>
&lt;p>Pivot Table/Chart:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/pivot-1024x524.jpg"
loading="lazy"
alt="excel screenshot of pivot chart and table"
>&lt;/p></description></item><item><title>Get a list of databases from named SQL Instances</title><link>https://jpomfret.github.io/get-a-list-of-databases-from-named-sql-instances/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/get-a-list-of-databases-from-named-sql-instances/</guid><description>&lt;p>Have you ever had someone send you the name of a SQL Server and database to do some work, but when you try to connect to the server you can’t? Then,come to find out, there are four named instances on the box and you don’t know which one hosts the database? No? Just me?&lt;/p>
&lt;p>Luckily, dbatools has a couple of commands that can help us out with this. Firstly, we can use &lt;code>Get-DbaService&lt;/code> to get a list of instances that are running on the server:&lt;/p>
&lt;p>$SqlInstances = Get-DbaService -ComputerName mssql1 -Type Engine |
Select @{L=&amp;lsquo;SqlInstance&amp;rsquo;;e={(&amp;rsquo;{0}\{1}&amp;rsquo; -f $_.ComputerName, $_.InstanceName)}}&lt;/p>
&lt;p>I went ahead and piped this to the Select-Object and built the SqlInstance property to be ‘ServerName\InstanceName’.  We can now use this in any of the other dbatools commands. For my use case I wanted database information, so I went with &lt;code>Get-DbaDatabase&lt;/code>:&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance $SqlInstances.SqlInstance |
Format-Table SqlInstance, Name, Status, RecoveryModel -AutoSize&lt;/p>
&lt;p>This made it easy for me to find the database in question without having to connect to each instance manually.&lt;/p>
&lt;p>You could also use this if you had a list of servers by just passing in a comma seperated list to the &lt;code>-ComputerName&lt;/code> parameter on &lt;code>Get-DbaService&lt;/code>.&lt;/p>
&lt;p>Just a short post today, but hopefully useful to somebody.&lt;/p></description></item><item><title>Using Get-WinEvent to look into the past</title><link>https://jpomfret.github.io/using-get-winevent-to-look-into-the-past/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-get-winevent-to-look-into-the-past/</guid><description>&lt;p>Recently I was tasked with troubleshooting an incident on a SQL Server at a certain point in the past, the issue being a high CPU alert.  It’s hard (without monitoring solutions set up) to go back in time and determine what the issue is.  However, one thing we can check is the windows event log to see if there was anything happening on the server at that time.&lt;/p>
&lt;p>Now, you probably know that my favourite tool of choice is PowerShell, so let&amp;rsquo;s take a look at how we can use &lt;code>Get-WinEvent&lt;/code> to see what was happening in the past.&lt;/p>
&lt;p>&lt;code>Get-WinEvent&lt;/code> is the newer revamped version of &lt;code>Get-EventLog&lt;/code>, and there are two improvements I believe are worth mentioning. Firstly, with the introduction of filter parameters we can now find certain events much easier, which we’ll talk about a little later. Secondly, the performance of &lt;code>Get-WinEvent&lt;/code> is much faster than using the legacy command.  I believe this is due to the filtering happening at the event engine instead of within PowerShell.&lt;/p>
&lt;h2 id="finding-some-logs">Finding some logs&lt;/h2>
&lt;p>First things first, let&amp;rsquo;s see what logs we have available on our target machine. I’m targeting a remote machine with the code below, but if we’re investigating an issue on our local machine we can just exclude the &lt;code>-ComputerName&lt;/code> parameter.&lt;/p>
&lt;p>This snippet will output all of the logs on my remote machine that contain records to a gridview. I love to use &lt;code>Out-GridView&lt;/code> for these kinds of tasks because this returned 101 logs and I can now add some text into the search bar to filter for things I might be interested in.&lt;/p>
&lt;p>Get-WinEvent -ListLog * -ComputerName dscsvr2 |
Where-Object RecordCount |
Out-GridView&lt;/p>
&lt;p>You can see if I add dsc into the search bar of &lt;code>Out-Grid View&lt;/code> I have one log with records in that I could investigate further.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/viewlogs.jpg"
loading="lazy"
alt="Out-GridView displaying results from Get-WinEvent -ListLog *"
>&lt;/p>
&lt;h2 id="filtering-events">Filtering events&lt;/h2>
&lt;p>I already mentioned this, but the new &lt;code>Get-WinEvent&lt;/code> gives us three options for filtering events. I’ll show this example using &lt;code>-FilterHashTable&lt;/code>, but just know there are two other options available, &lt;code>-FilterXPath&lt;/code> and &lt;code>-FilterXml&lt;/code>.&lt;/p>
&lt;p>The full list of key/value pairs that we can use to filter on are under the &lt;code>-FilterHashTable&lt;/code> parameter section of the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.diagnostics/get-winevent?view=powershell-7" target="_blank" rel="noopener"
>Microsoft Docs&lt;/a>. For now, we’ll look at filtering based on log name and time.&lt;/p>
&lt;p>So, say I have an alert for high CPU at 21:30 on 2020-07-31 and I want to know what was happening around that time on the server.&lt;/p>
&lt;p>First I’ll set up a few parameters. I’m going to set the &lt;code>$computerName&lt;/code> (I could add multiple here if I wanted to collect logs from more than one server) and the &lt;code>$issueDateTime&lt;/code>. I’ve then also specified the &lt;code>$windowMins&lt;/code>, I’m going to use to create a window of time around my issue to collect events for.&lt;/p>
&lt;p>$computerName = &amp;lsquo;dscsvr2&amp;rsquo;
$issueDateTime = get-date(&amp;lsquo;2020-07-31 21:30&amp;rsquo;)
$windowMins = 30&lt;/p>
&lt;p>Next we’ll build the filter hash table. You can do this inline when you call the command, but I personally like to break it out just for readability.&lt;/p>
&lt;p>$winEventFilterHash = @{
LogName = &amp;lsquo;system&amp;rsquo;,&amp;lsquo;application&amp;rsquo;
StartTime = $issueDateTime.AddMinutes(-($windowMins/2))
EndTime = $issueDateTime.AddMinutes(($windowMins/2))
}&lt;/p>
&lt;p>Finally, we’ll call &lt;code>Get-WinEvent&lt;/code>, then pass in the filter hash table and the computer name.  I’m selecting just a few standard properties, as well as a calculated property to get the username instead of just the sid. The final piece of this pipeline is to use &lt;code>Format-Table&lt;/code>. I would also recommend using Export-Excel to pipe it straight to an excel file for analysis.&lt;/p>
&lt;p>(Note - I use Export-Excel in this post if you&amp;rsquo;re interested in that - &lt;a class="link" href="https://jesspomfret.com/getting-versions/" target="_blank" rel="noopener"
>Getting OS and SQL Version information with dbatools&lt;/a>)&lt;/p>
&lt;p>Get-WinEvent -FilterHashtable $winEventFilterHash -ComputerName $computerName | Select-Object LogName,
ProviderName,
TimeCreated,
Id,
LevelDisplayName,
@{l=&amp;lsquo;UserName&amp;rsquo;;e={(New-Object System.Security.Principal.SecurityIdentifier($_.UserId)).Translate([System.Security.Principal.NTAccount])}},
Message | Format-Table&lt;/p>
&lt;p>As you can see below, the results are returned from both the application and system logs along with the time, level, username and message.  Now, this is just a test box in my lab, but this is a great way of grabbing a window of events from your server to help troubleshoot issues in the past.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/eventoutput.png"
loading="lazy"
alt="Results from Get-WinEvent"
>&lt;/p></description></item><item><title>Truncate all the Tables in a Database with PowerShell</title><link>https://jpomfret.github.io/truncate-all-the-tables-in-a-database-with-powershell/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/truncate-all-the-tables-in-a-database-with-powershell/</guid><description>&lt;p>&lt;strong>TLDR&lt;/strong>; &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/05_TruncateAllTables.ps1" target="_blank" rel="noopener"
>This code&lt;/a> will script out foreign keys and views (including object level permissions), drop the objects, truncate all the tables, and recreate the objects.&lt;/p>
&lt;h2 id="the-details">The Details&lt;/h2>
&lt;p>The most popular post on my blog so far was called ‘&lt;a class="link" href="https://jesspomfret.com/disable-all-triggers/" target="_blank" rel="noopener"
>Disable all Triggers on a Database&lt;/a>’ and this one is a good follow up from that post.&lt;/p>
&lt;p>The scenario here is you need to remove all the data from the tables in your database. This could be as part of a refresh process, or perhaps to clear out test data that has been entered through an application.  Either way, you want to truncate all the tables in your database.&lt;/p>
&lt;p>Using a copy of the AdventureWorks2017 database for my demos, the easiest option to truncate all the tables is to script out truncate statements using the metadata stored in &lt;code>sys.tables&lt;/code>.&lt;/p>
&lt;p>SELECT &amp;lsquo;Truncate table &amp;rsquo; + QUOTENAME(SCHEMA_NAME(schema_id)) + &amp;lsquo;.&amp;rsquo; + QUOTENAME(name)
FROM sys.tables&lt;/p>
&lt;p>You’ll get a results set like shown below which you can copy out into a new query window and execute.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/TruncateFromSysTables.png"
loading="lazy"
alt="select from sys.tables to generate truncate statements"
>&lt;/p>
&lt;p>The problem is if you have foreign keys, even if you order the truncate statements to remove the dependent data first, you can’t issue a truncate statement. The way around this is to script out the foreign keys, drop them, run the truncate statements and then recreate the foreign keys. This is not difficult in T-SQL, but it’s easier with PowerShell and a little bit of &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> magic.&lt;/p>
&lt;h2 id="truncate-tables-with-powershell">Truncate tables with PowerShell&lt;/h2>
&lt;p>The full script is available up on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/05_TruncateAllTables.ps1" target="_blank" rel="noopener"
>Github&lt;/a>, but I’ll walk through the process here. During my post on disabling triggers I stored the previously enabled triggers in a variable to reuse during the script.  I had a really great comment on this post that pointed out a problem: if the session crashed for some reason we would lose the list of triggers we wanted to enable. I will solve that problem in this post by instead of using a variable, saving the information in a temporary file.&lt;/p>
&lt;p>First things first, we need to set up a couple of variables to define our SqlInstance, database and the folder we’ll use as our workspace.&lt;/p>
&lt;p>I’ll then use &lt;code>Connect-DbaInstance&lt;/code> to connect to the instance and save the smo object. This will save having to reconnect to the instance multiple times.&lt;/p>
&lt;p>$sqlInstance = &amp;lsquo;mssql1&amp;rsquo;
$database = &amp;lsquo;AdventureWorks2017&amp;rsquo;
$tempFolder = &amp;lsquo;C:\temp&amp;rsquo;&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance $sqlInstance&lt;/p>
&lt;p>The next step is to collect the foreign keys that we’ll need to drop and recreate. It’s important to note here there are also some views that depend on these tables, so I can also collect that information at the same time. &lt;/p>
&lt;p># Collect up the objects we need to drop and recreate
$objects = @()
$objects += Get-DbaDbForeignKey -SqlInstance $svr -Database $database
$objects += Get-DbaDbView -SqlInstance $svr -Database $database -ExcludeSystemView&lt;/p>
&lt;p>Now that we have collected the objects into a variable we can pipe this to the &lt;code>Export-DbaScript&lt;/code> command to generate T-SQL scripts for both dropping and then recreating the objects. Something to take into consideration when dropping and recreating views is that if there are permissions set at the object level we need to include those in our create scripts.  We can use the &lt;code>New-DbaScriptingOption&lt;/code> command to set the options we care about when we create the scripts.&lt;/p>
&lt;p>Here we are including the permissions, the ‘ScriptBatchTerminator’, which will add ‘Go’ between objects, and finally setting the file type to ANSI.  When we call &lt;code>Export-DbaScript&lt;/code> we can then use these options for the &lt;code>-ScriptingOptionsObject&lt;/code> parameter.&lt;/p>
&lt;p># Script out the create statements for objects
$createOptions = New-DbaScriptingOption
$createOptions.Permissions = $true
$createOptions.ScriptBatchTerminator = $true
$createOptions.AnsiFile = $true&lt;/p>
&lt;p>$objects | Export-DbaScript -FilePath (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder) -ScriptingOptionsObject $createOptions&lt;/p>
&lt;p>We also need to script out the drop statements. To do that we’ll create another options object, this time setting &lt;code>ScriptDrops&lt;/code> to true. Then we’ll again call &lt;code>Export-DbaScript&lt;/code> with the &lt;code>-ScriptingOptionsObject&lt;/code> parameter.&lt;/p>
&lt;p># Script out the drop statements for objects
$options = New-DbaScriptingOption
$options.ScriptDrops = $true
$objects| Export-DbaScript -FilePath (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder) -ScriptingOptionsObject $options&lt;/p>
&lt;p>Once we have the scripts safely in our temporary folder we’ll run three simple statements.&lt;/p>
&lt;p>First, we’ll run the drop statements we scripted out.&lt;/p>
&lt;p>Second, remember we saved the smo connection to our server in the &lt;code>$svr&lt;/code> variable. We’ll use that to access all the tables in our database, pipe that to a &lt;code>Foreach-Object&lt;/code> and call the &lt;code>TruncateData&lt;/code> method.&lt;/p>
&lt;p>Third, we’ll call &lt;code>Invoke-DbaQuery&lt;/code> to recreate the foreign keys and the views we previously dropped.&lt;/p>
&lt;p># Run the drop scripts
Invoke-DbaQuery -SqlInstance $svr -Database $database -File (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;h1 id="truncate-the-tables">Truncate the tables&lt;/h1>
&lt;p>$svr.databases[$database].Tables | ForEach-Object { $_.TruncateData() }&lt;/p>
&lt;h1 id="run-the-create-scripts">Run the create scripts&lt;/h1>
&lt;p>Invoke-DbaQuery -SqlInstance $svr -Database $database -File (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;p>The final step is to clear up the script files we saved to the temporary folder.&lt;/p>
&lt;p># Clear up the script files
Remove-Item (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder), (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;p>This script can be reused for any database that you may need to clear out. As I was writing this post, I realised this could probably be a dbatools command… watch this space. ?&lt;/p></description></item><item><title>Using PSDefaultParameterValues for connecting to SQL Server in containers</title><link>https://jpomfret.github.io/using-psdefaultparametervalues-for-connecting-to-sql-server-in-containers/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-psdefaultparametervalues-for-connecting-to-sql-server-in-containers/</guid><description>&lt;p>I’ve written previously about using containers for demos on my laptop, specifically for my &lt;a class="link" href="https://jesspomfret.com/data-compression-containers/" target="_blank" rel="noopener"
>data compression talk&lt;/a>.  Since I switched those demos over I haven’t looked back- if it’s possible to run my demos off of containers I always choose that option.&lt;/p>
&lt;p>I recently presented a talk called ‘Life Hacks: dbatools edition’ which walks through 6 scenarios where you can immediately implement dbatools to quickly reap the rewards.  The demos can all be run on containers, but I did need to get a little more complex to be able to show off dbatools migration commands. To do this I used a docker compose file.&lt;/p>
&lt;p>The compose file creates one instance straight from the Microsoft SQL Server 2019 image and a second one from a dockerfile that specifies the base SQL Server 2017 image, copies in the files needed to attach the AdventureWorks2017 database, and runs some SQL to get everything setup exactly as desired. Feel free to check out this &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/LifeHacks_dbatools/Docker/docker-compose.yml" target="_blank" rel="noopener"
>setup on my Github&lt;/a>.&lt;/p>
&lt;p>One of the things that bothered me about running my demos on containers was that I couldn’t use windows authentication. Instead I had to pass in a SQL login to connect for every command.&lt;/p>
&lt;h2 id="enter-psdefaultparametervalues">Enter PSDefaultParameterValues&lt;/h2>
&lt;p>I first heard about PSDefaultParameterValues from a &lt;a class="link" href="https://github.com/PSPowerHour/PSPowerHour/tree/master/materials/2018-08-21/potatoqualitee" target="_blank" rel="noopener"
>PSPowerHour session by Chrissy LeMaire&lt;/a> in 2018. After rewatching this recently, I realised she even mentioned this exact scenario. However, it took until I recently rediscovered this handy preference variable that it all clicked together.&lt;/p>
&lt;p>PSDefaultParameterValues does exactly what the name suggests- it lets you specify default values for parameters. PSDefaultParameterValues can be set as a hash table of parameter names and values that will be used in your session for any function that can use it.  A simple example is the verbose parameter. If you wanted to turn on the &lt;code>-Verbose&lt;/code> switch for every function you run you could add &lt;code>-Verbose&lt;/code> to each function call, or you could set PSDefaultParameterValues.&lt;/p>
&lt;h3 id="option-1--add--verbose-to-individual-commands">Option 1 – Add &lt;code>-Verbose&lt;/code> to individual commands&lt;/h3>
&lt;p>Get-DbaDbBackupHistory -SqlInstance mssql1 -Verbose
Repair-DbaDbOrphanUser -SqlInstance mssql1 -Verbose&lt;/p>
&lt;h3 id="option-2--set-psdefaultparametervalues">Option 2 – Set PSDefaultParameterValues&lt;/h3>
&lt;p>$PSDefaultParameterValues = @{ &amp;lsquo;*:Verbose&amp;rsquo; = $True }
Get-DbaDbBackupHistory -SqlInstance mssql1
Repair-DbaDbOrphanUser -SqlInstance mssql1&lt;/p>
&lt;p>One thing to note when specifying PSDefaultParameterValues as I have above: this will overwrite any parameters you already have saved to PSDefaultParameterValues, so be careful. Another way to set &lt;code>-Verbose&lt;/code> to true would be to use the following notation:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;rsquo;*:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;h2 id="getting-more-specific">Getting more specific&lt;/h2>
&lt;p>In the above examples I’m using a wildcard (*) on the left side to specify that this parameter is for all functions. You can also focus in PSDefaultParameterValues by specifying one certain function name that the parameter value will apply to:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;lsquo;Get-DbaDbTable:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;p>You can also specify just the dbatools commands by taking advantage of their naming conventions and using:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;rsquo;*-Dba*:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;h2 id="psdefaultparametervalues-for-connecting-to-containers">PSDefaultParameterValues for connecting to containers&lt;/h2>
&lt;p>As I mentioned, my use case was to avoid having to specify a credential for every function that connected to my SQL Server running in a container. To use this for dbatools I need to specify a few parameter names. Most dbatools functions take the credential for the &lt;code>-SqlCredential&lt;/code> parameter, but for the copy commands there is both &lt;code>-SourceSqlCredential&lt;/code> and &lt;code>-DestinationCredential&lt;/code> that need to be specified.&lt;/p>
&lt;p>First, I create a &lt;code>PSCredential&lt;/code> that contains my username and password (note: this is for a demo environment and is insecure as the password is in plain text. If you are using this for other scenarios you’ll want to protect this credential). &lt;/p>
&lt;p>$securePassword = (&amp;lsquo;Password1234!&amp;rsquo; | ConvertTo-SecureString -asPlainText -Force)
$credential = New-Object System.Management.Automation.PSCredential(&amp;lsquo;sa&amp;rsquo;, $securePassword)&lt;/p>
&lt;p>Once I have the credential I can specify all the parameters that should use that credential by default:&lt;/p>
&lt;p>$PSDefaultParameterValues = @{&amp;quot;*:SqlCredential&amp;quot;=$credential
&amp;ldquo;*:DestinationCredential&amp;rdquo;=$credential
&amp;ldquo;*:DestinationSqlCredential&amp;rdquo;=$credential
&amp;ldquo;*:SourceSqlCredential&amp;rdquo;=$credential}&lt;/p>
&lt;p>Now whenever I call a function within this session, the specified parameters will use my credential. Therefore I can run the following and it’ll automatically use my saved sa login credential.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1&lt;/p>
&lt;h2 id="psdefaultparametervalues-in-your-profile">PSDefaultParameterValues in your profile&lt;/h2>
&lt;p>Setting PSDefaultParameterValues will only persist in the current session, however you can add the code above to your profile so that these default values are always provided.  If I do, whenever I open a PowerShell window I can easily connect to my containers without having to specify the credential.&lt;/p>
&lt;p>One thing to note is that this might be overkill. In my situation this is my demo machine. I always use the same sa password for any containers I run, and the majority of the time I’m running commands with a &lt;code>SqlCredential&lt;/code> parameter I want to connect to those containers.&lt;/p>
&lt;h2 id="override">Override&lt;/h2>
&lt;p>Even if you have set PSDefaultParameterValues in your profile you can still override that default value on any command just by specifying a new value. For example, running the following will pop up the credential request window for you to enter new credentials.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1 -SqlCredential (Get-Credential)&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>To wrap this up, I’ve found a lot of time savings by adding PSDefaultParameterValues to my profile. I can now quickly fire up PowerShell and start running functions against my containers.  It also keeps my demo scripts clean and easier to read. There is no need to specify the same parameters over and over again when it’s always going to be the same value.&lt;/p></description></item><item><title>Changing focus on code execution in VSCode</title><link>https://jpomfret.github.io/changing-focus-on-code-execution-in-vscode/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/changing-focus-on-code-execution-in-vscode/</guid><description>&lt;p>I wrote previously about how I loved little life hacks and shortcuts for my &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-123-summary" target="_blank" rel="noopener"
>February T-SQL Tuesday prompt&lt;/a>. If you read that you’ll know I use VSCode a lot and really love all the shortcuts available in that program. This is just a quick tip that I’m so glad I found, read – took the time to work out.&lt;/p>
&lt;p>I specifically write a lot of PowerShell in VSCode and so often find myself using F8 to run the selected line in the integrated console. One thing that always drove me a little crazy was that my cursor stayed in the integrated console after execution, rather than returning to the script I was writing.  I hadn’t managed to find the shortcut to return to the editor window I was working in until recently when I decided to figure it out.&lt;/p>
&lt;p>Since I started writing this post a couple of weeks ago, I discovered an even better solution thanks to the following tweet from &lt;a class="link" href="http://twitter.com/simon_sabin" target="_blank" rel="noopener"
>Simon Sabin&lt;/a>. The tweet also links to a GitHub issue where there is a discussion on why this is the default behaviour.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/simon" target="_blank" rel="noopener"
>https://twitter.com/simon&lt;/a>_sabin/status/1252253795603681281&lt;/p>
&lt;p>So by adding the following to your &lt;code>settings.json&lt;/code> file you can override that behaviour, and keep the focus in your script pane.&lt;/p>
&lt;p>&amp;ldquo;powershell.integratedConsole.focusConsoleOnExecute&amp;rdquo;: false&lt;/p>
&lt;h2 id="original-solution">&lt;strong>Original Solution&lt;/strong>&lt;/h2>
&lt;p>The original solution to this dilemma is to use the keyboard shortcut to return focus to the script pane. Using Ctrl+1 will return you to the first editor group (unless you are using ZoomIt!).&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/onepanev2-1.gif"
loading="lazy"
alt="Use Ctrl&amp;#43;1 to get to editor from console"
>&lt;/p>
&lt;p>If you have more than one editor group open you can use CTRL+2 to get to the second group, or CTRL+3 to get to the third group.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/twoPanev2-1.gif"
loading="lazy"
alt="ctrl&amp;#43;2 to get back to pane 2"
>&lt;/p>
&lt;p>Also, as with most shortcuts in VSCode, you can also customise the key bindings, by opening the command palette (F1) and choosing ‘Open Keyboard Shortcuts’ either in the GUI or JSON format.&lt;/p>
&lt;p>The commands to customise are below with the defaults:&lt;br>
- workbench.action.focusFirstEditorGroup: Ctrl+1&lt;br>
- workbench.action.focusSecondEditorGroup: Ctrl+2&lt;br>
- workbench.action.focusThirdEditorGroup: Ctrl+3 (and so on up to the eighth editor group)&lt;/p>
&lt;p>A bonus tip for you: If you’re in the GUI keyboard shortcut editor and you right click and copy, or press Ctrl+C, you’ll actually copy the JSON you’d need to customise your key bindings in the keyboard shortcuts (JSON) file.&lt;/p>
&lt;p>{
&amp;ldquo;key&amp;rdquo;: &amp;ldquo;ctrl+5&amp;rdquo;,
&amp;ldquo;command&amp;rdquo;: &amp;ldquo;workbench.action.focusFifthEditorGroup&amp;rdquo;
}&lt;/p>
&lt;p>I’m really excited to have discovered a couple of optimisations so now when I’m writing a script and I execute a line of PowerShell in the integrated console I will easily be able to navigate back to my editor pane using Ctrl+1 instead of having to reach for the mouse.&lt;/p></description></item><item><title>Anyone know what day it is?</title><link>https://jpomfret.github.io/anyone-know-what-day-it-is/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/anyone-know-what-day-it-is/</guid><description>&lt;p>It’s an interesting time we’re living in right now. I’m based in the UK and we just received the announcement last week that we have at least three more weeks of lockdown. I’ve been working at home for over a month now and I feel really lucky that I can work from home. There are a lot of folks who can’t and are instead working hard on the front lines to keep us safe and well. Firstly, a shout out to those folks! Thanks for all your doing.&lt;/p>
&lt;p>Now, this is not a post on how to set up your home office, or how to be the most productive work from home employee. This is a post to solve one simple problem.  I’ve no idea what day it is.&lt;/p>
&lt;p>Monday through Friday I wake up, drink some coffee and then head to my desk to work my day job. I usually workout in the garden with my wife at lunch and then we walk after work. Every day is pretty much the same, so I’ve written an earth-shattering PowerShell function for you.&lt;/p>
&lt;p>function Find-WhatDayAreWeDoing {
param (
[switch]$Raw
)&lt;/p>
&lt;pre>&lt;code>if($raw) {
(Get-Date).DayOfWeek
}
else {
(&amp;quot;We're doing {0} today!&amp;quot; -f (Get-Date).DayOfWeek)
}
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;p>You can add this into your profile and then simply just call the function to remind you what day we’re on.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FindWhatDay1.jpg"
loading="lazy"
alt="Find-WhatDayAreWeDoing"
>&lt;/p>
&lt;p>I’ve also added the &lt;code>-Raw&lt;/code> switch so you can use that to just return the day, this is useful if you wanted to add it into your prompt.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FindWhatDay2.jpg"
loading="lazy"
>&lt;/p>
&lt;p>To change the prompt in PowerShell you just need to create a function called ‘Prompt’. For example, if I run this in my PowerShell session or put it in my profile, I’ll be able to see what day it is constantly.&lt;/p>
&lt;p>function Prompt {
Write-Host (&amp;quot;[{0}] PS&amp;gt; &amp;quot; -f (Find-WhatDayAreWeDoing -raw))
}&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/prompt1.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The prompt is a really useful snippet of code and there are many  great ideas out there. I use the amazing &lt;a class="link" href="https://dbatools.io/prompt/" target="_blank" rel="noopener"
>dbatools prompt&lt;/a>. Their prompt lists the current time and the execution time for the last statement run. You could change it to add in the day also:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/prompt2.jpg"
loading="lazy"
>&lt;/p>
&lt;p>I’ll be honest, development on &lt;code>Set-WhatDayWeAreDoing&lt;/code> is not going as well. Stay safe folks and keep keeping on.&lt;/p></description></item><item><title>Interactive debugging in VSCode</title><link>https://jpomfret.github.io/interactive-debugging-in-vscode/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/interactive-debugging-in-vscode/</guid><description>&lt;p>I was browsing twitter the other day when a tweet about dbatools caught my eye (I use &lt;a class="link" href="https://tweetdeck.twitter.com/" target="_blank" rel="noopener"
>TweetDeck&lt;/a> and so have a column for tweets that contain &lt;a class="link" href="http://twitter.com/psdbatools" target="_blank" rel="noopener"
>@PSdbatools&lt;/a>).&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/way0utwest/status/1242891971473137666" target="_blank" rel="noopener"
>https://twitter.com/way0utwest/status/1242891971473137666&lt;/a>&lt;/p>
&lt;p>A dbatools bug!! Oh no!&lt;/p>
&lt;p>One of the reasons this caught my eye was that I’ve seen this error in my environment with that same command. I had discounted that it was a bug and figured it was instead something in my environment. I presumed it was something related to the fact I was using containers and Azure Data Studio connections.&lt;/p>
&lt;p>Step one for dbatools bug fixing is to check for an issue on the &lt;a class="link" href="http://dbatools.io/bugs" target="_blank" rel="noopener"
>GitHub repo&lt;/a> and create one if there isn’t one already. It turned out that there was &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools/issues/6292" target="_blank" rel="noopener"
>one already created&lt;/a> so we’re covered there.&lt;/p>
&lt;p>So I figured I’d take a look and see what was happening and how we could fix it. Now I’m going to be honest with you, my usual method of debugging involves adding &lt;code>Write-Host 'Hi&lt;/code>&amp;rsquo;, or piping objects to &lt;code>Out-GridView&lt;/code>. I did start down this route, but the &lt;code>Get-DbaRegServer&lt;/code> function calls an internal function, and things quickly got complicated.&lt;/p>
&lt;p>Luckily, the &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell" target="_blank" rel="noopener"
>PowerShell extension for VSCode&lt;/a> includes a debugger so we can level up our game and use that to track down our issues. Since I haven’t already used this for my dbatools folder when I click the ‘Run’ icon on the left navigation bar I see the following:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/startDebug.jpg"
loading="lazy"
alt="run and debug window in VSCode"
>&lt;/p>
&lt;p>Pressing the ‘Run and Debug’ button will run your active file and, if you have breakpoints set up, then it’ll break at those points for you to troubleshoot. This is really useful if you have written a script and it’s not working correctly. Since I’m troubleshooting the call of a function I could write a simple script with the code to call the function, save it and then press ‘Run and Debug’. However there is another option, and that is to launch an interactive debugger. &lt;/p>
&lt;p>Pressing the ‘create a launch.json file’ link opens the command palette with the option to choose your PowerShell debug configuration. Choosing the ‘Interactive Session’ configuration means we can use the integrated console within VSCode to call functions and launch the debugger.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/debugConfig.jpg"
loading="lazy"
alt="Select a PowerShell debug configuration"
>&lt;/p>
&lt;p>This will open a launch.json file that you can edit to add more functionality and customization, but we’ll just save it as is right now.&lt;/p>
&lt;p>{
// Use IntelliSense to learn about possible attributes.
// Hover to view descriptions of existing attributes.
// For more information, visit: &lt;a class="link" href="https://go.microsoft.com/fwlink/?linkid=830387" target="_blank" rel="noopener"
>https://go.microsoft.com/fwlink/?linkid=830387&lt;/a>
&amp;ldquo;version&amp;rdquo;: &amp;ldquo;0.2.0&amp;rdquo;,
&amp;ldquo;configurations&amp;rdquo;: [
{
&amp;ldquo;name&amp;rdquo;: &amp;ldquo;PowerShell: Interactive Session&amp;rdquo;,
&amp;ldquo;type&amp;rdquo;: &amp;ldquo;PowerShell&amp;rdquo;,
&amp;ldquo;request&amp;rdquo;: &amp;ldquo;launch&amp;rdquo;,
&amp;ldquo;cwd&amp;rdquo;: &amp;quot;&amp;quot;
}
]
}&lt;/p>
&lt;p>As soon as you save it the left ‘Run and Debug’ pane will change to look like this. Now we’re ready to run the interactive debugger by pressing the green play button or F5.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DebugInteractive.jpg"
loading="lazy"
alt="Debug and Run window for PowerShell extension"
>&lt;/p>
&lt;p>So now that we’re set up, let’s start troubleshooting.&lt;/p>
&lt;p>Step one is to reproduce this issue. This particular bug was easy to reproduce. The only requirements are that you have Azure Data Studio installed and at least one connection set up, then just running &lt;code>Get-DbaRegServer&lt;/code> caused the error.&lt;/p>
&lt;p>Next we need to add some breakpoints. These need to be the positions in the code where you want to stop execution and take a look at how things are set in the moment. It’s also a great way to see if you entered certain sections of the code that may be guarded by conditional logic.&lt;/p>
&lt;p>Running &lt;code>Get-DbaRegServer&lt;/code> in the integrated console you can see the error, even down to the line from the function where the error is being thrown. In the screenshot below you can see hovering over that line in VSCode allows you to follow the link to open the function and navigate to the exact line.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDbaRegServerError.jpg"
loading="lazy"
alt="Get-DbaRegServer throws an error"
>&lt;/p>
&lt;p>Line 180 of the Get-DbaRegServer is the following:&lt;/p>
&lt;p>$tempserver.ConnectionString = $adsconn.ConnectionString&lt;/p>
&lt;p>We’ll insert a breakpoint here by clicking in the gutter at line 180.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/addBreakpoint.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now pressing F5 the interactive debugger will start, and we can rerun &lt;code>Get-DbaRegServer&lt;/code> in the interactive console. When we do that as soon as the execution gets to line 180 the code will stop, waiting for us to respond.&lt;/p>
&lt;p>You can see below that we are able to find the &lt;code>$adsconn&lt;/code> variable in the variables pane on the left and see that it’s actually an object with three values – which is the issue here – we’re expecting to only have one returned.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/04/breakpoint.jpg" target="_blank" rel="noopener"
>&lt;img src="https://i2.wp.com/jesspomfret.com/wp-content/uploads/2020/04/breakpoint.jpg?fit=650%2C246&amp;amp;ssl=1"
loading="lazy"
alt="VSCode code stopped at breakpoint, displaying variables"
>&lt;/a>&lt;/p>
&lt;p>I read back through the &lt;code>Get-DbaRegServer&lt;/code> function to find where the &lt;code>$adsconn&lt;/code> variable was set and found it was from calling the internal function &lt;code>Get-ADSConnection&lt;/code>. I added in another breakpoint within that function to dig in deeper.&lt;/p>
&lt;p>Adding the breakpoint within the second function means that when we call &lt;code>Get-RegServer&lt;/code> and then that calls &lt;code>Get-ADSConnection&lt;/code> the code will wait within the second function and allow you to inspect variables within that function.&lt;/p>
&lt;p>This meant that I was able to determine that there were several connection strings being returned for each server and that we needed to filter down to one.&lt;/p>
&lt;p>Changing line 174 in the &lt;code>Get-DbaRegServer&lt;/code> function to include an additional filter, shown below, meant that only one connection string was returned and solved the problem.&lt;/p>
&lt;p>$adsconn = $adsconnection | Where-Object { $_.server -eq $server.Options[&amp;lsquo;server&amp;rsquo;] -and -not $_.database }&lt;/p>
&lt;p>Hopefully this walkthrough shows a useful way of using the interactive debugger to hunt down bugs.&lt;/p></description></item><item><title>Keeping the demo gods at bay with Pester</title><link>https://jpomfret.github.io/keeping-the-demo-gods-at-bay-with-pester/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-the-demo-gods-at-bay-with-pester/</guid><description>&lt;p>A short while ago (it’s getting further and further away, but let’s stick with short for now) I was a football/soccer player. As with many athletes, I was pretty superstitious as far as my pregame routine. I always felt better going out onto the pitch if everything had gone smoothly as I got ready.  I put my boots, shin-pads and socks on in a certain order and even taped my socks up in a certain way. The good news is I’ve managed to find a slightly more reliable way to get ready for my presentations – and I’m going to share the secret.&lt;/p>
&lt;p>First you put your right sock on, then your left sock on. Follow that by putting on your right shoe, and then your left shoe… just joking. You use Pester tests!&lt;/p>
&lt;p>If you don’t know what &lt;a class="link" href="https://pester.dev/" target="_blank" rel="noopener"
>Pester&lt;/a> is, it’s a test framework for PowerShell.  In the simplest explanation, using their Domain-Specific Language (DSL) you describe how things should look. If all looks good it returns output in green and if it doesn’t you get red output.  There are a lot of great use cases for Pester, like using it to ensure your code does what it’s supposed to, using it to validate your SQL Server environment (&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a>), or in this example using it to make sure your demos are setup and ready to go.&lt;/p>
&lt;p>When I’m preparing for a presentation I go through the demos over and over again, so it’s easy to accidentally leave things in a state that will cause issues when I go to do my demos in the presentation. If you’re creating a table, for example, during the demo and you already created it practicing and then forgot to drop it, the demo gods will strike and it’ll fail when it matters most! A simple Pester test to check whether the table exists will solve this issue.&lt;/p>
&lt;p>So what do I test?&lt;/p>
&lt;p>Last Wednesday I presented my ‘Life hacks: dbatools Edition’ session for the &lt;a class="link" href="https://www.meetup.com/Southampton-Data-Platform-and-Cloud-Group/" target="_blank" rel="noopener"
>Southampton Data Platform and Cloud meetup&lt;/a> so I’ll talk you through the tests I ran to make sure I was ready to present that session, and it’s a demo heavy one!&lt;/p>
&lt;p>First things first, I test that I can import the dbatools module. I make sure I’m getting the version and the number of commands I expect. dbatools puts out new versions all the time, so I usually update this in the weeks leading up to my presentation as I’m practicing.&lt;/p>
&lt;p>Describe &amp;ldquo;Module is good to go&amp;rdquo; {
Context &amp;ldquo;dbatools imports&amp;rdquo; {
$null = Import-Module dbatools
$module = Get-Module dbatools
It &amp;ldquo;Module was imported&amp;rdquo; {
$module | Should Not BeNullOrEmpty
}
It &amp;ldquo;Module version is 1.0.99&amp;rdquo; {
$module.Version | Should Be &amp;ldquo;1.0.99&amp;rdquo;
}
It &amp;ldquo;Module should import 587 commands&amp;rdquo; {
(get-command -module dbatools | Measure).Count | Should Be 587
}
}
}&lt;/p>
&lt;p>My demo setup involves two containers running on my laptop. Because of that, I’m using the sa credential to connect and I’m setting some PSDefaultParameterValues so I don’t have to include the &lt;code>$credential&lt;/code> in every function call. I can test all that is setup correctly like so.&lt;/p>
&lt;p>Describe &amp;ldquo;Credentials exist&amp;rdquo; {
Context &amp;ldquo;Credential exists&amp;rdquo; {
It &amp;ldquo;Credential is not null&amp;rdquo; {
$credential | Should Not BeNullOrEmpty
}
}
Context &amp;ldquo;username is sa&amp;rdquo; {
It &amp;ldquo;Username is sa&amp;rdquo; {
$credential.UserName | Should Be &amp;ldquo;sa&amp;rdquo;
}
}
Context &amp;ldquo;PSDefaultParameterValues are set&amp;rdquo; {
$params = $PSDefaultParameterValues
It &amp;ldquo;PSDefaultParameterValues contains expected values&amp;rdquo; {
$params.Keys -contains &amp;lsquo;*:SqlCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:SourceSqlCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:DestinationCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:DestinationSqlCredential&amp;rsquo; | Should Be True
}
}
}&lt;/p>
&lt;p>I then have a couple of simple checks to make sure I can connect to both my instances.&lt;/p>
&lt;p>Describe &amp;ldquo;Two instances are available&amp;rdquo; {
Context &amp;ldquo;Two instances are up&amp;rdquo; {
$mssql1 = Connect-DbaInstance -SqlInstance mssql1
$mssql2 = Connect-DbaInstance -SqlInstance mssql2
It &amp;ldquo;mssql1 is available&amp;rdquo; {
$mssql1.Name | Should Not BeNullOrEmpty
$mssql1.Name | Should Be &amp;lsquo;mssql1&amp;rsquo;
}
It &amp;ldquo;mssql2 is available&amp;rdquo; {
$mssql2.Name | Should Not BeNullOrEmpty
$mssql2.Name | Should Be &amp;lsquo;mssql2&amp;rsquo;
}
}
}&lt;/p>
&lt;p>I then make sure that my databases are set up as expected. I am using two databases on my mssql1 SQL Server instance, AdventureWorks2017 and DatabaseAdmin. I make sure each of those exist, are online, and that the compatibility level is set correctly. I also check that the indexes on the Employee table are set up as I expect since I use those in my demos.&lt;/p>
&lt;p>Describe &amp;ldquo;mssql1 databases are good&amp;rdquo; {
Context &amp;ldquo;AdventureWorks2017 is good&amp;rdquo; {
$db = Get-DbaDatabase -SqlInstance mssql1
$adventureWorks = $db | where name -eq &amp;lsquo;AdventureWorks2017&amp;rsquo;
It &amp;ldquo;AdventureWorks2017 is available&amp;rdquo; {
$adventureWorks | Should Not BeNullOrEmpty
}
It &amp;ldquo;AdventureWorks status is normal&amp;rdquo; {
$adventureWorks.Status | Should Be Normal
}
It &amp;ldquo;AdventureWorks Compat is 140&amp;rdquo; {
$adventureWorks.Compatibility | Should Be 140
}
}
Context &amp;ldquo;Indexes are fixed on HumanResources.Employee (bug)&amp;rdquo; {
$empIndexes = (Get-DbaDbTable -SqlInstance mssql1 -Database AdventureWorks2017 -Table Employee).indexes | select name, IsUnique
It &amp;ldquo;There are now just two indexes&amp;rdquo; {
$empIndexes.Count | Should Be 2
}
It &amp;ldquo;There should be no unique indexes&amp;rdquo; {
$empIndexes.IsUnique | Should BeFalse
}
}
Context &amp;ldquo;DatabaseAdmin is good&amp;rdquo; {
$db = Get-DbaDatabase -SqlInstance mssql1
$DatabaseAdmin = $db | where name -eq &amp;lsquo;DatabaseAdmin&amp;rsquo;
It &amp;ldquo;DatabaseAdmin is available&amp;rdquo; {
$DatabaseAdmin | Should Not BeNullOrEmpty
}
It &amp;ldquo;DatabaseAdmin status is normal&amp;rdquo; {
$DatabaseAdmin.Status | Should Be Normal
}
It &amp;ldquo;DatabaseAdmin Compat is 140&amp;rdquo; {
$DatabaseAdmin.Compatibility | Should Be 140
}
}
}&lt;/p>
&lt;p>One of my demos shows the backup history for AdventureWorks, so I test that with Pester before I start to make sure there is history to show. Nothing worse than getting up to show a wonderful set of dbatools functions and nothing being returned because I haven’t actually taken any backups!&lt;/p>
&lt;p>Describe &amp;ldquo;Backups worked&amp;rdquo; {
Context &amp;ldquo;AdventureWorks was backed up&amp;rdquo; {
$instanceSplat = @{
SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
}
It &amp;ldquo;AdventureWorks has backup history&amp;rdquo; {
Get-DbaDbBackupHistory @instanceSplat | Should Not BeNullOrEmpty
}
}
}&lt;/p>
&lt;p>While I was writing my demos I came across issues where my PowerShell environment was set to x86 so I added a test for that to make sure it doesn’t happen again.&lt;/p>
&lt;p>Describe &amp;ldquo;Proc architecture is x64&amp;rdquo; {
Context &amp;ldquo;Proc arch is good&amp;rdquo; {
It &amp;ldquo;env:processor_architecture should be AMD64&amp;rdquo; {
$env:PROCESSOR_ARCHITECTURE | Should Be &amp;ldquo;AMD64&amp;rdquo;
}
}
}&lt;/p>
&lt;p>Finally, I check to see what’s running on my computer. Zoomit, everyone’s favourite screen zoom tool should be running, and we should make sure that Slack and Teams are not.&lt;/p>
&lt;p>Describe &amp;ldquo;Check what&amp;rsquo;s running&amp;rdquo; {
$processes = Get-Process zoomit*, teams, slack -ErrorAction SilentlyContinue
Context &amp;ldquo;ZoomIt is running&amp;rdquo; {
It &amp;ldquo;ZoomIt64 is running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Zoomit64&amp;rsquo;) | Should Not BeNullOrEmpty
}
It &amp;ldquo;Slack is not running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Slack&amp;rsquo;) | Should BeNullOrEmpty
}
It &amp;ldquo;Teams is not running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Teams&amp;rsquo;) | Should BeNullOrEmpty
}
}
}&lt;/p>
&lt;p>Now there are obviously ways that the demo gods can still strike, but using Pester to test your demos is a great way to try and tilt the odds in your favour.&lt;/p>
&lt;p>You can view all the code, including the tests, for this presentation on my &lt;a class="link" href="https://github.com/jpomfret/demos/tree/master/LifeHacks_dbatools" target="_blank" rel="noopener"
>Github&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s what the output looks like:&lt;/p>
&lt;p>Executing all tests in &amp;lsquo;.\Tests\demo.tests.ps1&amp;rsquo;&lt;/p>
&lt;p>Executing script .\Tests\demo.tests.ps1&lt;/p>
&lt;p>Describing Module is good to go&lt;/p>
&lt;pre>&lt;code>Context dbatools imports
\[+\] Module was imported 1.59s
\[+\] Module version is 1.0.99 287ms
\[+\] Module should import 587 commands 162ms
&lt;/code>&lt;/pre>
&lt;p>Describing Credentials exist&lt;/p>
&lt;pre>&lt;code>Context Credential exists
\[+\] Credential is not null 263ms
Context username is sa
\[+\] Username is sa 83ms
Context PSDefaultParameterValues are set
\[+\] PSDefaultParameterValues contains expected values 80ms
&lt;/code>&lt;/pre>
&lt;p>Describing Two instances are available&lt;/p>
&lt;pre>&lt;code>Context Two instances are up
\[+\] mssql1 is available 592ms
\[+\] mssql2 is available 26ms
&lt;/code>&lt;/pre>
&lt;p>Describing mssql1 databases are good&lt;/p>
&lt;pre>&lt;code>Context AdventureWorks2017 is good
\[+\] AdventureWorks2017 is available 863ms
\[+\] AdventureWorks status is normal 32ms
\[+\] AdventureWorks Compat is 140 46ms
Context Indexes are fixed on HumanResources.Employee (bug)
\[+\] There are now just two indexes 1.53s
\[+\] There should be no unique indexes 49ms
Context DatabaseAdmin is good
\[+\] DatabaseAdmin is available 256ms
\[+\] DatabaseAdmin status is normal 15ms
\[+\] DatabaseAdmin Compat is 140 17ms
&lt;/code>&lt;/pre>
&lt;p>Describing Backups worked&lt;/p>
&lt;pre>&lt;code>Context AdventureWorks was backed up
\[+\] AdventureWorks has backup history 627ms
&lt;/code>&lt;/pre>
&lt;p>Describing Proc architecture is x64&lt;/p>
&lt;pre>&lt;code>Context Proc arch is good
\[+\] env:processor\_architecture should be AMD64 125ms
&lt;/code>&lt;/pre>
&lt;p>Describing Check what&amp;rsquo;s running&lt;/p>
&lt;pre>&lt;code>Context ZoomIt is running
\[+\] ZoomIt64 is running 150ms
\[+\] Slack is not running 43ms
\[+\] Teams is not running 17ms
&lt;/code>&lt;/pre>
&lt;p>Tests completed in 6.86s
Tests Passed: 21, Failed: 0, Skipped: 0, Pending: 0, Inconclusive: 0&lt;/p>
&lt;p>Good news, all passed and I&amp;rsquo;m ready to give my demos!&lt;/p></description></item><item><title>Using AutomatedLab to setup a SQL Server Lab</title><link>https://jpomfret.github.io/using-automatedlab-to-setup-a-sql-server-lab/</link><pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-automatedlab-to-setup-a-sql-server-lab/</guid><description>&lt;p>I’ve recently found myself in a few situations where I’ve needed a certain lab setup to test something.  Most scenarios these days I find myself firing up some docker containers to run demos or tests against. However, there are a few circumstances where you may need a little more than that, perhaps to test something Windows OS related (my container setup is using Linux) or just to replicate a production environment more closely than you can with containers.  In these situations I turn to PowerShell.&lt;/p>
&lt;p>There is a fantastic PowerShell module called &lt;a class="link" href="https://github.com/AutomatedLab/AutomatedLab" target="_blank" rel="noopener"
>AutomatedLab&lt;/a> that can enable you to easily build out a lab for the  specific scenario you need to test. Even better is the module comes with 70 sample scripts that you can start with and adapt to meet your needs.&lt;/p>
&lt;p>The module gives you the option to work with Hyper-V or VMWare. I will say most of the examples are using Hyper-V, and that is what I’ll be using also.&lt;/p>
&lt;p>For my lab I want a SQL Server 2019 instance joined to a domain, and a separate client machine that I can manage the SQL Server from. On the client I would need to be able to connect to the internet as I want to be able to download PowerShell modules from the gallery easily.&lt;/p>
&lt;h2 id="step-1-get-the-module">&lt;strong>Step 1: Get the module&lt;/strong>&lt;/h2>
&lt;p>There is some setup involved to make sure the pieces needed to build your lab are in the right places.  AutomatedLab gives you two options to ensure you’re set up correctly. &lt;a class="link" href="https://github.com/AutomatedLab/AutomatedLab/wiki/1.-Installation" target="_blank" rel="noopener"
>The documentation on GitHub&lt;/a> already does a great job of outlining these so I’ll just point you in that direction for reference.&lt;/p>
&lt;p>In my lab I used the second method to download from the PowerShell Gallery and then run the configuration command. I ran the following to get my laptop setup:&lt;/p>
&lt;p>Install-Module -Name AutomatedLab -AllowClobber
New-LabSourcesFolder&lt;/p>
&lt;h2 id="step-2-stage-the-isos">&lt;strong>Step 2: Stage the ISOs&lt;/strong>&lt;/h2>
&lt;p>In order to build our lab we will need to store the ISOs needed for installation where AutomatedLab can access them. We used the defaults during our install so that location is &lt;code>C:\LabSources\ISOs&lt;/code>.  You can see below I’ve placed the ISOs for Windows Server 2019, SQL Server 2016 and SQL Server 2019 in this folder.&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/jesspomfret.com/wp-content/uploads/2020/02/isos.jpg?fit=650%2C112&amp;amp;ssl=1"
loading="lazy"
alt="iso folder"
>&lt;/p>
&lt;p>You can test whether AutomatedLab can see the operating system ISOs by using the &lt;code>Get-AvailableOperatingSystems&lt;/code> command. You can see below it has found my ISO folder and therefore lists the available operating systems that I can use. You’ll notice that the Windows Server 2019 ISO allows me to install two editions, Standard or Datacenter, as well as choosing between installing the core version or the more traditional desktop experience. &lt;/p>
&lt;p>It is important to ensure you have the appropriate licensing in place for the OS you install. You can use evaluation editions or enter product keys at a later time.&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/jesspomfret.com/wp-content/uploads/2020/02/AvailableOS.jpg?fit=650%2C79&amp;amp;ssl=1"
loading="lazy"
alt="available isos"
>&lt;/p>
&lt;h2 id="step-3-craft-the-lab-setup-script">&lt;strong>Step 3: Craft the lab setup script&lt;/strong>&lt;/h2>
&lt;p>Now this sounds like a scary step, but fear not! I mentioned before the AutomatedLab team have included over 70 sample scripts, which means it’s likely there is something similar already written. I found that there were two samples that covered all the parts needed for my lab, so I was able to combine them to create the exact scenario needed.&lt;/p>
&lt;p>I’ve added comments throughout the script below so you can easily see the pieces needed:&lt;/p>
&lt;p>&lt;a class="link" href="https://gist.github.com/jpomfret/15dd515bfd421ae13c047f95d65eb333" target="_blank" rel="noopener"
>https://gist.github.com/jpomfret/15dd515bfd421ae13c047f95d65eb333&lt;/a>&lt;/p>
&lt;p>By default, if you don’t specify a the &lt;code>-VmPath&lt;/code> parameter on the &lt;code>New-LabDefinition&lt;/code> function AutomatedLab will test the speed of the available drives and choose the fastest.&lt;/p>
&lt;h2 id="step-4-install-the-lab">&lt;strong>Step 4: Install the lab&lt;/strong>&lt;/h2>
&lt;p>With the script written, open the PowerShell console using ‘Run as Administrator’ and execute the script. &lt;/p>
&lt;p>.\AutomatedLab_SQLServer.ps1&lt;/p>
&lt;p>There will also be some security settings to consider and accept for the first install such as enabling WinRM and adding ‘*’ to your TrustedHosts. Once those are confirmed and the prework is complete, a toast will pop up showing that the lab has started building:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/StartToast.jpg"
loading="lazy"
alt="lab started toast"
>&lt;/p>
&lt;p>This is the perfect time to grab a cup of coffee as there are some steps that can take a while, especially if this is the first install.&lt;/p>
&lt;p>AutomatedLab uses a base disk for installations of the same OS. Creating this base disk can take a while but once it is created it significantly reduces the amount of disk space you need for your lab. If you deploy 4 VMs with the same OS, as I have here, the base disk will be created and then differential disks can be used for each VM.&lt;/p>
&lt;p>If you build a second lab in the same VM folder with the same OS it can again reuse this base disk.&lt;/p>
&lt;h2 id="step-5-use-the-lab">&lt;strong>Step 5: Use the lab&lt;/strong>&lt;/h2>
&lt;p>Once the install is complete, another toast will appear and it’s time to log in and start using your lab.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/completetoast.jpg"
loading="lazy"
alt="lab finished toast"
>&lt;/p>
&lt;p>The summary we displayed using &lt;code>Show-LabDeploymentSummary&lt;/code> will show the machine names, administrator password and some other connection information. We have again used the defaults for these settings but they can be overwritten.&lt;/p>
&lt;h2 id="remove-the-lab">&lt;strong>Remove the lab&lt;/strong>&lt;/h2>
&lt;p>Once you are done with your lab you can quickly and easily run the following to destroy all the pieces created by your lab (except the base OS disk).&lt;/p>
&lt;p>Remove-Lab SQLLab&lt;/p></description></item><item><title>Backups with dbatools &amp; BurntToast</title><link>https://jpomfret.github.io/backups-with-dbatools-burnttoast/</link><pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/backups-with-dbatools-burnttoast/</guid><description>&lt;p>I have just a quick tip for you today using the &lt;a class="link" href="https://github.com/Windos/BurntToast" target="_blank" rel="noopener"
>BurntToast&lt;/a> module to notify us when a backup is complete. As DBAs there is always plenty to do, so we don’t want to have to sit and watch a long running script to catch the moment when it finishes.  Usually what happens to me is I kick off the script, move on to something else and then totally forget about it, perhaps until someone asks if it’s done yet. Oops. Well this tip will help avoid that.&lt;/p>
&lt;p>The &lt;a class="link" href="https://github.com/Windos/BurntToast" target="_blank" rel="noopener"
>BurntToast&lt;/a> module, created by Josh King (&lt;a class="link" href="https://toastit.dev/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/windosnz" target="_blank" rel="noopener"
>t&lt;/a>), allows you to easily add Windows toast notifications to your PowerShell scripts. I’m going to show you how to use BurntToast to keep track of a database backup.&lt;/p>
&lt;p>By this time you should know about my love for &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a>, so today we’re going to take a look at how to take a &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/copy-only-backups-sql-server?view=sql-server-ver15" target="_blank" rel="noopener"
>copy-only&lt;/a> backup.  This is a backup that doesn’t upset the LSN chain of your regular database backups and can be used to just save a specific point in time for perhaps pre-upgrade, or to restore the database somewhere else.&lt;/p>
&lt;p>The following script will take a copy-only backup of the AdventureWorks2019 database on the mssql2 instance. Since I’m not specifying a path it will go to the default backup directory defined for that instance. I’m also saving the results of the command to the &lt;code>$backup&lt;/code> variable.&lt;/p>
&lt;p>The second section below that will run directly after the backup completes will use the results in $backup to notify us using BurntToast:&lt;/p>
&lt;p>## Take a copy only backup, using splatting for readability
$backupSplat = @{
SqlInstance = &amp;ldquo;mssql2&amp;rdquo;
Database = &amp;ldquo;AdventureWorks2019&amp;rdquo;
CopyOnly = $true
}
$backup = Backup-DbaDatabase @backupSplat&lt;/p>
&lt;h2 id="notify-the-backup-is-complete-using-splatting-for-readability">Notify the backup is complete, using splatting for readability&lt;/h2>
&lt;p>toastSplat = @{
Text = (&amp;ldquo;Backup of {0} completed in {1}&amp;rdquo; -f $backup.Database, $backup.Duration.ToString())
AppLogo = &amp;ldquo;C:\temp\dbatools.png&amp;rdquo;
}
New-BurntToastNotification @toastSplat&lt;/p>
&lt;p>That’s it, 2 commands to take a backup and notify us on completion.  I’ve also used the &lt;code>-AppLogo&lt;/code> parameter to add the &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools/blob/development/bin/dbatools.png" target="_blank" rel="noopener"
>dbatools logo&lt;/a>. You can see that this backup only took 1 second to complete (hopefully I didn’t get sidetracked in that time) but if the backup takes a few minutes or longer this is a useful tip to let you know when it’s finished.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/dbatoolsToast.jpg"
loading="lazy"
>&lt;/p>
&lt;p>You could also add some error handling to this script to make it a little more robust by perhaps using try-catch to change the message if the backup was unsuccessful.&lt;/p>
&lt;p>This is just one use case for using BurntToast with dbatools You could use this in any script that you’re writing to keep you notified when it’s done. This will allow you to get on with whatever else you have on your plate and not have to worry about remembering that backup you kicked off a while ago.&lt;/p>
&lt;p>Since writing this post I saw that Josh has created the &lt;a class="link" href="https://github.com/Windos/PoshNotify" target="_blank" rel="noopener"
>PoshNotify&lt;/a> module which allows you to generate popups cross-platform. So if you are not using Windows you can adapt the above script to use this module instead.&lt;/p></description></item><item><title>Desired State Configuration: A few warnings when using PSDscRunAsCredentials</title><link>https://jpomfret.github.io/desired-state-configuration-a-few-warnings-when-using-psdscrunascredentials/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-a-few-warnings-when-using-psdscrunascredentials/</guid><description>&lt;p>When you enact a configuration against a target node by default the process runs as the local system account.  For most of your DSC resources this is fine. However, if you need to access something like a file share, active directory or user registry settings, you might start to run into permission issues.  &lt;/p>
&lt;p>DSC Resources have a built in property, &lt;code>PSDscRunAsCredential&lt;/code>, that when configured changes the account that the resource will be executed under.&lt;/p>
&lt;p>To use this you will use a &lt;code>PSCredential&lt;/code> object in your resources as shown below. Note that this example doesn’t require the use of other credentials and would have had the required permissions as the local system account.&lt;/p>
&lt;p>$Cred = Get-Credential
Configuration DtcServiceRunning {&lt;/p>
&lt;pre>&lt;code>Import-DscResource -ModuleName PSDesiredStateConfiguration
Node 'dscsvr2' {
Service MsDtcRunning {
Name = 'MSDTC'
Ensure = 'Present'
State = 'Running'
PsDscRunAsCredential = ($creds)
}
}
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;h1 id="generates-mof-file">Generates MOF file&lt;/h1>
&lt;p>DtcServiceRunning -Output .\Output\&lt;/p>
&lt;p>When you use credentials in your configurations there are a couple of gotchas- we’ll talk through these next.&lt;/p>
&lt;h3 id="warning-1-plain-text-passwords">Warning 1: Plain Text Passwords&lt;/h3>
&lt;p>Although it feels like you are handling the credential securely, when you run the code above you will get an error explaining that storing passwords in plain text is a bad idea.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/01_PlainTextPasswords.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/01_PlainTextPasswords.jpg"
loading="lazy"
alt="Converting and storing encrypted passwords as plain text is not recommended."
>&lt;/a>&lt;/p>
&lt;p>When you run your configuration, &lt;code>DtcServiceRunning&lt;/code> in my example, a MOF file is generated and with our current setup the passwords will be stored in plain text (you can read more about generating &lt;a class="link" href="https://jesspomfret.com/dsc-mof-files/" target="_blank" rel="noopener"
>MOF files here&lt;/a>). &lt;/p>
&lt;p>The correct way to handle this issue is to generate a certificate and use that to encrypt the MOF file, a topic I will blog about one day soon. To skirt around this issue in test we can add a configuration property that basically forces us to accept we know this is a bad idea, but we’re going for it anyway.&lt;/p>
&lt;p>$configData = @{
AllNodes = @(
@{
NodeName = &amp;ldquo;dscsvr2&amp;rdquo;
PsDscAllowPlainTextPassword = $true
PsDscAllowDomainUser = $true
}
)
}
DtcServiceRunning -Output .\Output\ -ConfigurationData $configData&lt;/p>
&lt;p>Once we run this we’ll see our MOF file has been successfully generated.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/02_moffile.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="warning-2-changing-the-password-used-in-a-mof-file">Warning 2: Changing the password used in a MOF file&lt;/h3>
&lt;p>Once you push your MOF file out to the target node and it is enacted, the MOF remains on the target node in a designated folder and is named &lt;code>current.mof&lt;/code>. The credentials, including the password (hopefully encrypted by a certificate!), are within the file.&lt;/p>
&lt;p>You can run a couple of commands to check your current configuration that compare the desired state, defined in the &lt;code>current.mof&lt;/code> file, to the nodes current state.&lt;/p>
&lt;p>If the password you used for the &lt;code>PsDscRunAsCredentials&lt;/code> property has changed since the MOF file was enacted you’ll get the following errors when you try and run &lt;code>Get-DscConfiguration&lt;/code> or &lt;code>Test-DscConfiguration&lt;/code>.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/04_testFailed.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/04_testFailed.jpg"
loading="lazy"
alt="The user name or password is incorrect."
>&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/05_getfailed.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/05_getfailed.jpg"
loading="lazy"
alt="The user name or password is incorrect."
>&lt;/a>&lt;/p>
&lt;p>The user name or password is incorrect.&lt;/p>
&lt;p>To fix this you must rerun your configuration, which will in turn generate a new MOF file that contains the new password. Once this MOF file has been generated you’ll push it out to your target node.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/05_startDscConfiguration.jpg"
loading="lazy"
alt="Start-DscConfiguration"
>&lt;/p>
&lt;p>Now that the MOF file contains the new password you are able to check in on your current configuration again.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/06_test.jpg"
loading="lazy"
alt="Test-DscConfiguration"
>&lt;/p>
&lt;p>This wasn’t something I thought about before I used &lt;code>PsDscRunAsCredential&lt;/code> on all the resources in my configuration.  A while later I went to check on my target nodes and realized I couldn’t without pushing out a new MOF file. After some discussion in the DSC Slack channel around this, the recommended approach is to only use &lt;code>PsDscRunAsCredential&lt;/code> if necessary, and to be aware of the requirement to generate a new MOF file once you change the password.&lt;/p></description></item><item><title>Disable all Triggers on a Database</title><link>https://jpomfret.github.io/disable-all-triggers-on-a-database/</link><pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/disable-all-triggers-on-a-database/</guid><description>&lt;p>Sometimes it’s best not to ask why. However, if for some reason you have a number of triggers on tables within a database that you would like to temporarily disable, read on.&lt;/p>
&lt;p>I came across a situation recently while automating a process to refresh test environments where this exact scenario came up.  As part of the process several scripts were run to obfuscate production data. While these ran all the UPDATE triggers were firing. Not only were the triggers adding a significant amount of time to the process, they were also updating dates and other values that we’d prefer kept their original values.&lt;/p>
&lt;p>Now, as I mentioned this is not a discussion on whether this is a good database design or not, this is just how to solve this issue.&lt;/p>
&lt;p>In the snippet below I use &lt;code>Connect-DbaInstance&lt;/code> from &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> to create a &lt;code>$svr&lt;/code> object. If you don’t have dbatools installed you could either &lt;a class="link" href="http://dbatools.io/install" target="_blank" rel="noopener"
>install dbatools&lt;/a>, or use &lt;code>New-Object Microsoft.SqlServer.Management.Smo.Server&lt;/code>. The dbatools function is essentially a wrapper around this command that adds a lot of additional checks and options.&lt;/p>
&lt;p>I have also defined an array &lt;code>$triggers&lt;/code> to keep track of the triggers I disable. It’s likely that you’ll want to put the environment back to how it started, so this will make sure you don’t enable any triggers that started off disabled.&lt;/p>
&lt;p>Then we get to the actual work. Using the &lt;code>$svr&lt;/code> object we can loop through all the tables, and then all the triggers on those tables. If a certain trigger is enabled, it is added to the &lt;code>$triggers&lt;/code> array and then disabled using &lt;code>$tr.isenabled&lt;/code>.  As with most (all?) changes made through SMO you then need to call the alter method ,&lt;code>$tr.alter()&lt;/code>, to actually make the change on the server.&lt;/p>
&lt;p>$database = ‘AdventureWorks2017’
$svr = Connect-DbaInstance -SqlInstance server1
$foreach ($tbl in $svr.databases[$database].Tables)
{
foreach ($tr in $($tbl.Triggers | Where-Object Isenabled)) {
$triggers += $tr | Select-Object @{l=&amp;lsquo;SchemaName&amp;rsquo;;e={$tbl.Schema}}, @{l=&amp;lsquo;TableName&amp;rsquo;;e={$tbl.name}}, @{l=&amp;lsquo;TriggerName&amp;rsquo;;e={$_.name}}
$tr.isenabled = $FALSE
$tr.alter()
}
}&lt;/p>
&lt;p>When you are ready to enable the triggers again you can use the following code. This loops through the triggers that we had previously disabled and added to our array and enables them.&lt;/p>
&lt;p>foreach($tr in $triggers) {
$trigger = $svr.Databases[$database].Tables[$tr.TableName,$tr.SchemaName].Triggers[$tr.TriggerName]
$trigger.IsEnabled = $true
$trigger.alter()
}&lt;/p></description></item><item><title>Aggregating Data with PowerShell</title><link>https://jpomfret.github.io/aggregating-data-with-powershell/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/aggregating-data-with-powershell/</guid><description>&lt;p>As a SQL Server DBA, aggregating data is first nature.  I can easily throw together some T-SQL if I need to get the average sales per product group, or perhaps the number of employees hired per month. Yesterday I was writing a little PowerShell when I came across a problem. I needed to get the size of my database data and log files for several databases on a server, when I realized I didn’t know how to group and sum this data in PowerShell.&lt;/p>
&lt;p>&lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> has the &lt;code>Get-DbaDbFile&lt;/code> command, which easily allowed me to collect information about the databases I needed.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Once I had this data, I wanted to group by the type of file and get the total size.  To start I piped the output to &lt;code>Select-Object&lt;/code> to trim down the fields in the result set, then piped this output to the &lt;code>Group-Object&lt;/code>, specifying the field I wanted to group by.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Database, TypeDescription, Size |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Group-Object TypeDescription
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Group.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now that the data is grouped you can aggregate the size property for the files, which is within the &lt;code>Group&lt;/code>.  We’ll add an expression to the final &lt;code>Select-Object&lt;/code> which uses the &lt;code>Measure-Object&lt;/code> to sum the sizes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Database, TypeDescription, Size |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Group-Object TypeDescription |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Name, @{l=&amp;#39;Size&amp;#39;;e={($_.Group.Size.MegaByte | Measure-Object -Sum).Sum}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Final.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Another interesting note here is that since dbatools returns the Size as a special type, &lt;code>Sqlcollaborative.Dbatools.Utility.Size&lt;/code>, you can specify that you want it to be returned in MBs.  If you have very small databases you might want to change that so you don’t lose accuracy with any rounding.&lt;/p>
&lt;p>The code for the examples in this post was run against a Docker Container I use for demos. You can read more information on getting that set up in my post &lt;a class="link" href="https://jesspomfret.com/data-compression-containers/" target="_blank" rel="noopener"
>Data Compression Demos in Containers&lt;/a>.&lt;/p></description></item><item><title>Desired State Configuration: Troubleshooting in Push Refresh Mode</title><link>https://jpomfret.github.io/desired-state-configuration-troubleshooting-in-push-refresh-mode/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-troubleshooting-in-push-refresh-mode/</guid><description>&lt;p>One of the biggest obstacles people face when using DSC is the troubleshooting and reporting pieces. There are options here to integrate with third party tools to create a more polished enterprise solution, but if you’re going with just straight DSC you might feel it is lacking some in this area.&lt;/p>
&lt;p>We do however have several tools available to troubleshoot issues with configurations or to monitor our nodes to determine whether they are still in the desired state. I’m specifically going to look at the options available if you’re using DSC in the Push refresh mode. If you are using DSC in pull mode with a web server or if you’re using Azure Automation you have some other options available. You can configure the Local Configuration Manager (LCM) to send reports to the pull server. These reports are stored in a database on the server and can be accessed by calling the web service. Perhaps the topic of another blog post.&lt;/p>
&lt;p>The options we’ll look at today are the functions available within the &lt;code>PSDesiredStateConfiguration&lt;/code> module, and the DSC Windows event logs.&lt;/p>
&lt;h3 id="psdesiredstateconfiguration-functions">&lt;strong>PSDesiredStateConfiguration Functions&lt;/strong>&lt;/h3>
&lt;p>We should be fairly familiar with this module as it comes built in with WMF 4.0+ and contains several base resources as well as functions to manage and use DSC.&lt;/p>
&lt;p>In order for us to explore these commands we need to have a target node with an active configuration. I pushed out a simple configuration that will ensure the directory &lt;code>c:\temp&lt;/code> exists on the target node. The configuration was successful and the folder was created.&lt;/p>
&lt;p>First up we have &lt;code>Get-DscConfiguration&lt;/code>. The command based help for this function says it will get “the&lt;br>
current configuration of the nodes.” When I run this against my target node it returns details about the file resource I used to create the &lt;code>C:\temp&lt;/code> directory and notes that &lt;code>Ensure&lt;/code> is &lt;code>Present&lt;/code>. This is as I would expect.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/get_present.jpg"
loading="lazy"
>&lt;/p>
&lt;p>If I manually delete the &lt;code>C:\Temp&lt;/code> folder, the node is no longer in the desired state. When I rerun &lt;code>Get-DscConfiguration&lt;/code> it shows the folder is absent, which is not the behavior I expected from the help. I was expecting to get the current configuration that had been pushed out to the node. It seems that this function returns the resources included in the configuration and their current state. It does not however indicate if this is the desired state.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Get_Absent.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The next function available is &lt;code>Get-DscConfigurationStatus&lt;/code> The description of this function states it will retrieve “detailed information about completed configuration runs on the system.”   If we run that with just the &lt;code>-CimSession&lt;/code> parameter to connect to our node we get some useful information about the last run. &lt;/p>
&lt;p>However, there is a lot of information available from this function that is not within the default columns returned. If we look into the output further we can in fact see whether resources are in the desired state or not.  &lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DscConfigurationStatus_additionalCOls.jpg"
loading="lazy"
>&lt;/p>
&lt;p>During my testing using &lt;code>Get-DscConfigurationStatus&lt;/code> it did not always accurately report when there were resources not in the desired state. Therefore I wouldn’t rely on it for reporting, I would instead look at our third option below.&lt;/p>
&lt;p>The third and final function I’ll highlight from this module is &lt;code>Test-DscConfiguration&lt;/code>. The comment based help for this one states it “tests whether the actual configuration on the nodes matches the desired configuration.”&lt;/p>
&lt;p>Just running this with the target node as the &lt;code>-ComputerName&lt;/code> parameter does not provide much information. It returns `False` telling us it is not in the desired state, but doesn’t explain why.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_basic.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Running with the &lt;code>-Verbose&lt;/code> switch goes into details. It returns the same verbose output you would get from running the configuration in the first place. However, reading through the output of a large configuration file with many resources could get time consuming and it would be easy to miss resources not in their desired state. &lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_verbose.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Finally, we can use the &lt;code>-Detailed&lt;/code> parameter which will return a PowerShell object with exactly the information we’re looking for.  This object gives us more options on how to use this information. For example, we can return whether it is in the desired state and a list of resources not in the desired state is available.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_detailed.jpg"
loading="lazy"
>&lt;/p>
&lt;p>This gives us some good information if our configuration was successful, but what happens if we’re troubleshooting a failed configuration? Running both &lt;code>Get-DscConfiguration&lt;/code> and &lt;code>Test-DscConfiguration&lt;/code> state they will run against a pending configuration and both return the error that the network path wasn’t found.  No information is returned on which resource threw this error. If you have a large, complicated configuration it would be nice to receive a little more guidance on where to look next. Hint, it’s the event logs.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/get_test_failed.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="windows-event-logs">Windows Event Logs&lt;/h3>
&lt;p>The next step in the troubleshooting handbook is to head to the windows event logs.  DSC has four event logs, but only two are enabled by default, and it doesn’t seem like much gets written to the Admin log.&lt;/p>
&lt;p>Running the following will show you the enabled logs and the number of records:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WinEvent -ComputerName dscsvr2 -ListLog *dsc*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/wineventlogs.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Looking in the Operational log you can find the error and the related resource:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WinWvent -ComputerName dscsvr2 `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -LogName Microsoft-Windows-DSC/Operational -MaxEvents 10 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object TimeCreated, Message |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Format-List
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/07/eventlogerror.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/eventlogerror.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>The event log also points to a json file that contains more detailed logging, this matches what would be returned if you ran the &lt;code>Start-DscConfiguration&lt;/code> with the &lt;code>-Verbose&lt;/code> and &lt;code>-Wait&lt;/code> switches.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Message : Job {93E0C95E-8F21-11E9-85A2-00155D016620} : Details logging completed for C:\Windows\system32\configuration\ConfigurationStatus\{93E0C95E-8F21-11E9-85A2-00155D016620}-0.details.json.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>If more detail is needed you can enable the debug and analytic logs for DSC and rerun the configuration. The analytic logs will contain a lot more messages that will help you get to the bottom of why your configuration failed.&lt;/p></description></item><item><title>Getting OS and SQL Version information with dbatools</title><link>https://jpomfret.github.io/getting-os-and-sql-version-information-with-dbatools/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/getting-os-and-sql-version-information-with-dbatools/</guid><description>&lt;p>There have been a lot of blog posts and talk around upgrading your servers in the past. However, the chatter always seems to intensify when we start getting close to that dreaded ‘end of support’ date for your older Windows and SQL Server versions.  I hope this isn’t the first place you are discovering this, but July 9th 2019 marks the end of support for both SQL Server 2008 and 2008R2, closely followed on January 14th 2020 with the end of support for Windows Server 2008 and 2008R2.&lt;/p>
&lt;p>With these dates on the horizon it’s a good time to look at our estate and make sure we have a good understanding of the versions we currently support. I’m going to show you how to do that easily with a couple of &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> functions. Then, bonus content, I’ll show you how to present it for your managers with one of my other favourite PowerShell modules &lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a>.&lt;/p>
&lt;p>First things first, I need an object that contains our servers.  At my work we use a central management server to keep track of servers, but you could just as easily pull server names in from a text file, or a database.&lt;/p>
&lt;p>$servers = Get-DbaCmsRegServer -SqlInstance CmsServerName&lt;/p>
&lt;p>Let’s first look at what operating systems we are running. The dbatools function we need for this is &lt;code>Get-DbaOperatingSystem&lt;/code>. I’ll use the name property of my &lt;code>$servers&lt;/code> object to get the OS information for all my servers and save it to a variable.&lt;/p>
&lt;p>$os = Get-DbaOperatingSystem -ComputerName $servers.name&lt;/p>
&lt;p>I chose to save the results to a variable for this since I’m going to examine the results using PowerShell and then also output them to Excel, saving me from having to gather the information from each server multiple times. If I only planned on looking at the results on screen I could instead have just piped the &lt;code>Get-DbaOperationSystem&lt;/code> results straight into &lt;code>Group-Object&lt;/code>.&lt;/p>
&lt;p>Using &lt;code>Group-Object&lt;/code> I can quickly see how many servers I have for each versions of windows, and how many I have going out of support in the near future.&lt;/p>
&lt;p>$os | Group-Object OSVersion |
Sort-Object Name |
Select-Object Name, Count, @{l=&amp;lsquo;Servers&amp;rsquo;;e={$_.Group.ComputerName -Replace &amp;lsquo;.domain.name,&amp;rsquo;&amp;rsquo; -Join &amp;lsquo;,&amp;rsquo;}}&lt;/p>
&lt;p>I have used the -Replace option in my &lt;code>Select-Object&lt;/code> to remove the domain name from the output and instead only return the server name.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/05/os-2.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/os-2.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>We can do the same with &lt;code>Get-DbaProductKey&lt;/code> to get the SQL version information.&lt;/p>
&lt;p>$sql = Get-DbaProductKey -ComputerName $servers.name
$sql | Group-Object Version |
Sort-Object Name |
Select Name, Count, @{l=&amp;lsquo;Servers&amp;rsquo;;e={$_.Group.SqlInstance -join &amp;lsquo;,&amp;rsquo;}}&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/05/sql.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/sql.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>With just 5 lines of code we can review our entire estate and make sure we know what we have nearing the end of support. This is pretty useful information, and also a good thing to export into a pretty spreadsheet and share with your team or management. Enter &lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a>.&lt;/p>
&lt;p>If you haven’t used this module before, prepare to have your mind blown. Doug Finke has crafted some PowerShell magic to enable you to both import from and export to Excel, using PowerShell, without needing Excel installed even.&lt;/p>
&lt;p>The full code is below. We’ve already done the work of gathering our data so if you are following along skip the first 3 lines below.&lt;/p>
&lt;p>I’ve separated out the properties I want to select and will therefore end up in my spreadsheet. I’ve also used &lt;a class="link" href="https://dbatools.io/splat/" target="_blank" rel="noopener"
>splatting&lt;/a> to make the call to &lt;code>Export-Excel&lt;/code> easier to read.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The important part of this script is the parameters used for the &lt;code>Export-Excel&lt;/code> call so I’ll go through them here:&lt;/p>
&lt;p>[table id=7 /]&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ExcelOutput-1024x669.jpg"
loading="lazy"
>&lt;/p>
&lt;p>There you have it, a simple script to get the current OS and SQL versions you are running with a good looking Excel sheet as the output. Hope you don’t find too many instances out there nearing the end of support.&lt;/p></description></item><item><title>PowerShell Comment Based Help: Examples with Multiple Lines of Code</title><link>https://jpomfret.github.io/powershell-comment-based-help-examples-with-multiple-lines-of-code/</link><pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/powershell-comment-based-help-examples-with-multiple-lines-of-code/</guid><description>&lt;p>One of the reasons I love PowerShell is the comment based help. This allows you to easily get documentation for functions directly within your PowerShell session. By using &lt;code>Get-Help&lt;/code> for a function you can retrieve a description, information on the parameters, and examples of how to use the function.&lt;/p>
&lt;p>Recently I was adding help to a function  and wanted to add two lines of code to my example. Usually the syntax for an example looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Get-Cat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">This will get me a default of one cat.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The first line under &lt;code>.EXAMPLE&lt;/code> will be formatted with a PowerShell prompt in front to show it is code. The second line is a description of the example.&lt;/p>
&lt;p>If I want to add two lines of code, and I used the following, it would only display the first line with a prompt as shown in the screenshot below.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$cats = 4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Get-Cat -NumberOfCats $cats
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Store the number of cats in a variable and then get that number of cats.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/example_first.jpg"
loading="lazy"
>&lt;/p>
&lt;p>You can see above the first example looks good, however in the second example the first two lines should both have a prompt to show they are code. I spent a little while Googling this without much avail. I then figured, somewhere within &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> there must be an example with two lines of code. Sure enough I found my answer, and it’s pretty straightforward. You just add the prompt to the code yourself and then when the example is displayed it is formatted properly.&lt;/p>
&lt;p>I changed my examples to the following and you can see now they display as expected.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;Get-Cat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">This will get me a default of one cat.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;$cats = 4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;Get-Cat -NumberOfCats $cats
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Store the number of cats in a variable and then get that number of cats.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/example_second.jpg"
loading="lazy"
>&lt;/p>
&lt;p>I know you’re all dying to see the result of my &lt;code>Get-Cat&lt;/code> function so here you go. If you need to add this to your PowerShell profile etc. so you can quickly brighten any day, the code is on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/02_CommentBasedHelp_Examples.ps1" target="_blank" rel="noopener"
>github&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/cats.jpg"
loading="lazy"
>&lt;/p></description></item><item><title>Desired State Configuration: Resources</title><link>https://jpomfret.github.io/desired-state-configuration-resources/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-resources/</guid><description>&lt;p>A critical part of our DSC configuration is made up of resources. These are the building blocks we need to to define our desired state.  There are two kinds of resources that we can use: class based and MOF based (most common). We are going to focus our efforts today on looking at MOF based resources.&lt;/p>
&lt;p>Resources come packaged up as modules and our servers, which use at least WMF 4.0, come with several built-in. We have two main options for additional resources; we can find DSC resource modules in the PowerShell Gallery or we can write our own.&lt;/p>
&lt;h3 id="finding-dsc-resources">Finding DSC Resources&lt;/h3>
&lt;p>To find existing resources we have a few options. We could navigate to the &lt;a class="link" href="https://www.powershellgallery.com/packages?q=Tags%3A%22DSC%22" target="_blank" rel="noopener"
>PowerShell Gallery&lt;/a> website and browse through the modules with the ‘DSC’ tag. We could also use PowerShell by using the `Find-Module` command and the `-Tag` parameter to match our results from the PowerShell Gallery.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-Module -Tag DSC
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-Modules.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Our second option using PowerShell is to use &lt;code>Find-DscResource&lt;/code>. This cmdlet finds specific resources that are contained in modules. Running a count against that right now (4/1/2019) would return 1,413 resources that are available to configure our environment. Using the &lt;code>-Filter&lt;/code> parameter you can search for a keyword throughout the names, descriptions and tags of all these resources.&lt;/p>
&lt;h3 id="installing-resources">Installing Resources&lt;/h3>
&lt;p>If you find a DSC Resource you want to use in your configurations, for example to install SQL Server we will want to use SqlSetup from the SqlServerDsc module, you can install the module as you would a regular PowerShell module. For example, using &lt;code>Install-Module&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Install-Module.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Once the module is installed you can use these resources in your configurations. One thing to remember is the resources need to be available on both your authoring station and your target node.&lt;/p>
&lt;h3 id="five-useful-resources">Five Useful Resources&lt;/h3>
&lt;p>To wrap up I’m going to look at five resources I’ve used in my configurations to give you an idea of what is available.&lt;/p>
&lt;h4 id="file">File&lt;/h4>
&lt;p>The &lt;code>File&lt;/code> resource is one of the built-in resources and is useful for creating files and folders. For my SQL Servers I use it to create separate folders for the data, log, and tempdb files. Another use case would be to create a file and, by using the &lt;code>contents&lt;/code> property, we could even add data to it.&lt;/p>
&lt;h4 id="firewall">Firewall&lt;/h4>
&lt;p>Part of the &lt;code>NetworkingDsc&lt;/code> module, the &lt;code>Firewall&lt;/code> resource can be used to configure firewall rules on your target node. This is useful when installing SQL Server so you can open up access remotely. There is also a &lt;code>SqlWindowsFirewall&lt;/code> resource that will accomplish this task, but has less properties to configure. However, depending on your preferred setup, this may suffice.&lt;/p>
&lt;h4 id="sqlsetup">SqlSetup&lt;/h4>
&lt;p>&lt;code>SqlSetup&lt;/code> is the resource that installs SQL Server. Included in the &lt;code>SqlServerDsc&lt;/code> module there are many properties available to configure the installation just right.&lt;/p>
&lt;h4 id="script">Script&lt;/h4>
&lt;p>One of the most flexible resources is the &lt;code>Script&lt;/code>. This is also a built in resource and gives you the ability to code any PowerShell script into a simple resource. You basically write three mini functions: one that gets the current state, one that tests if it’s in the desired state, and finally one to ‘make it so’.&lt;/p>
&lt;h4 id="sqlagentoperatorhttpsgithubcompowershellsqlserverdscsqlagentoperator">&lt;a class="link" href="https://github.com/PowerShell/SqlServerDsc#sqlagentoperator" target="_blank" rel="noopener"
>SqlAgentOperator&lt;/a>&lt;/h4>
&lt;p>The final resource I’ve picked is part of the &lt;code>SqlServerDsc&lt;/code> module and allows you to create a SQL Agent operator. This is useful if you like to send alerts or notifications from your SQL Servers. The reason I picked this resource is that I wrote it. One of the best parts of DSC is that most of the modules are open source and available on Github. If there is a resource missing that you’d find useful, you are encouraged to write it and submit it to the Microsoft repos. That’s pretty cool if you ask me.&lt;/p></description></item><item><title>Desired State Configuration: Local Configuration Manager</title><link>https://jpomfret.github.io/desired-state-configuration-local-configuration-manager/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-local-configuration-manager/</guid><description>&lt;p>Once we have crafted the perfect configuration and shipped it out to our target nodes, it’s time for the magic to happen. The &lt;a class="link" href="https://jesspomfret.com/dsc-mof-files/" target="_blank" rel="noopener"
>MOF file that we created&lt;/a> by executing our configuration is translated and enacted by the Local Configuration Manager (LCM) on each target node. The LCM is the engine of DSC and plays a vital role in managing our target nodes.&lt;/p>
&lt;p>The LCM on each target node has many settings that can be configured using a meta configuration. This document is written very similarly to our regular DSC configurations and then pushed out to the target node.  I’m going to cover a few of the important LCM settings for use in &lt;code>push&lt;/code> mode. This is where the LCM passively waits for a MOF file to arrive. The other option is &lt;code>pull&lt;/code> mode- this is a little more complicated to set up and in this scenario the LCM is set to actively check in with a pull server for new configurations.&lt;/p>
&lt;h3 id="important-lcm-settings">&lt;strong>Important LCM Settings&lt;/strong>&lt;/h3>
&lt;p>As mentioned we are going to look at a subset of LCM settings. A full list is available at books online “&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/managing-nodes/metaconfig" target="_blank" rel="noopener"
>Configuring the Local Configuration Manager&lt;/a>”.&lt;/p>
&lt;p>[table id=6 /]&lt;/p>
&lt;h3 id="configure-the-lcm">Configure the LCM&lt;/h3>
&lt;p>We are going to change a couple of the LCM settings by writing a meta configuration document, compiling it as a MOF and pushing it to our target node. The LCM on that target node will receive this MOF file and enact it to put the LCM into the desired state. To start with we can check out the available settings by using the &lt;code>Get-DscLocalConfigurationManager&lt;/code> cmdlet.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/get_before.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We are going to change two settings in this example. First, I’m going to change the ConfigurationModeFrequencyMins to 20 minutes, instead of the default of 15.  Secondly, I will change the RebootNodeIfNeeded to true. This means if I push out a configuration that requires a reboot my node will automatically reboot.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[DSCLocalConfigurationManager()]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">configuration LCMConfig
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node dscsvr2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Settings
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ConfigurationModeFrequencyMins = 20
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RebootNodeIfNeeded = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I’ll then execute the &lt;code>LCMConfig&lt;/code> configuration to generate a meta MOF file. You can see this is named with the target node name and then the extension is &lt;code>.meta.mof&lt;/code>. For a regular configuration the file would just be named with the target node name and the extension of just &lt;code>.mof&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">LCMConfig -Output .\output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/CreateMetaMof.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We will then enact this configuration using &lt;code>Set-DscLocalConfigurationManager&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Set-DscLocalConfigurationManager -Path .\output\ -ComputerName dscsvr2 -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/set-DscLcm-1024x239.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Once this is complete we can check our settings using the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object ConfigurationModeFrequencyMins, RebootNodeIfNeeded
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/get_after-1024x107.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now our LCM is in our defined desired state and we are ready to push out a configuration to set the desired state of our server.&lt;/p>
&lt;h3 id="applyandautocorrect">ApplyAndAutoCorrect&lt;/h3>
&lt;p>The LCM can also play an important role in keeping our servers in the desired state. If we changed the &lt;code>ConfigurationMode&lt;/code> to &lt;code>ApplyAndAutoCorrect&lt;/code> the LCM would check every 15 minutes (default value for &lt;code>ConfigurationModeFrequencyMins&lt;/code>) to ensure the server was still in the desired state. If it found it was not, the LCM would reenact the current MOF to put the server back to desired state. This is a pretty powerful feature but one that definitely requires some thought. I can imagine a 3rd party vendor wouldn’t be too happy if they set something on installation and my DSC configuration reverted that automatically.&lt;/p></description></item><item><title>Desired State Configuration: MOF Files</title><link>https://jpomfret.github.io/desired-state-configuration-mof-files/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-mof-files/</guid><description>&lt;p>In my &lt;a class="link" href="https://jesspomfret.com/introduction-to-dsc/" target="_blank" rel="noopener"
>last Desired State Configuration (DSC) post&lt;/a> a couple of weeks ago I covered some of the concepts involved with DSC, and I also have a &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-110/" target="_blank" rel="noopener"
>T-SQL Tuesday post&lt;/a> to get you started writing your first configuration. So today we are going to look at the next step in the process: what happens after we’ve written a configuration?&lt;/p>
&lt;p>Here’s a quick recap on how to write a simple configuration. I’ve named this configuration &lt;code>CreateSqlFolder&lt;/code>. I’m targeting the node &lt;code>dscsvr2&lt;/code> and within that node block I’m using the &lt;code>File&lt;/code> resource to define my desired state of having a directory &lt;code>C:\SQL2017\SQLData&lt;/code> exist. With the final line of the script I’m calling the configuration and specifying where the output should land.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration CreateSqlFolder {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDesiredStateConfiguration
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node dscsvr2 {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> File CreateDataDir {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> DestinationPath = &amp;#39;C:\SQL2017\SQLData\&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Type = &amp;#39;Directory&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">CreateSqlFolder -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="generate-a-mof-file">Generate a MOF file&lt;/h3>
&lt;p>When I run this script I see the output in the screenshot below, a MOF file has been created in my output folder. Managed Object Format (MOF) files are used to describe Common Information Model (CIM) classes, these are industry standards which gives us flexibility in working with DSC. In DSC this is important as the MOF file is the artefact that will actually be used to configure our nodes. This MOF will be delivered to our target node and enacted by the Local Configuration Manager (LCM).&lt;/p>
&lt;p>The LCM will be covered in more detail in a later post, but for now know that it can be configured to be in either ‘Push’ mode or ‘Pull’ mode.  Pull mode is more complicated to set up but perhaps more appropriate for managing a large number of servers.  For now, we will look at the ‘Push’ mode where we will deliver the MOF manually to the target node for the LCM to enact.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/CreateSqlFoldersMOF.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Executing configuration to create a MOF file for target node.&lt;/p>
&lt;h3 id="publish-a-mof-file">Publish a MOF File&lt;/h3>
&lt;p>To get the MOF from my authoring station out to the target node I have a couple of options.  First, I can run &lt;code>Start-DscConfiguration&lt;/code>. This will push out the MOF and immediately enact the configuration.  Using the &lt;code>-wait&lt;/code> and &lt;code>-verbose&lt;/code> switches we can see the output returned to our PowerShell console as the configuration is applied.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Start-DscConfiguration -Path .\output\ -ComputerName dscsvr2 -Wait -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/startDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration-1024x242.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>If we want to push out the configuration but not immediately enact it we can use &lt;code>Publish-DscConfiguration&lt;/code>. I again used the &lt;code>-Verbose&lt;/code> switch to return output:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Publish-DscConfiguration -Path .\output\ -ComputerName dscsvr2 -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/publishDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/publishDscConfiguration-1024x122.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see this in this screenshot it says ‘Configuration document successfully saved to pending state’, letting us know this is now ready for the LCM to enact. We can confirm our &lt;code>PendingConfiguration&lt;/code> by running the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2 | Select LCMState
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/getDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/getDscConfiguration-1024x123.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>To enact the pending configuration we would again use &lt;code>Start-DscConfiguration&lt;/code>, only this time instead of specifying a path we’d add the &lt;code>-UseExisting&lt;/code> switch.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/startDscConfiguration_useexisting.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration_useexisting-1024x248.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It is important to note that if the LCM settings are currently set to the defaults this configuration will be automatically applied when the next consistency check runs within 15 mins.&lt;/p>
&lt;p>Look for a post coming soon where we’ll look at the LCM in more detail and examine some of the settings we have to manage how it works within DSC.&lt;/p></description></item><item><title>Introduction to Desired State Configuration</title><link>https://jpomfret.github.io/introduction-to-desired-state-configuration/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/introduction-to-desired-state-configuration/</guid><description>&lt;p>I’m currently working on a pretty interesting project to explore using PowerShell’s Desired State Configuration (DSC) to manage SQL Servers. DSC uses declarative language to define the desired state of your infrastructure.&lt;/p>
&lt;p>Ensuring that the directory &lt;code>C:\Test&lt;/code> exists is a simple example. A more complicated example would be the complete configuration of a SQL Server. This is my end goal.&lt;/p>
&lt;p>This post is aiming to just introduce DSC and a few of the concepts that come along with it, and give us a good building block for future posts that dive deeper into this topic.&lt;/p>
&lt;p>The infrastructure that surrounds DSC warrants several posts on its own, so for this first scratch of the surface just know that we will write DSC Configuration documents and these documents will be managed and executed on our target nodes.&lt;/p>
&lt;h3 id="declarative-vs-imperative">Declarative Vs Imperative&lt;/h3>
&lt;p>If you are already familiar with PowerShell scripts you write imperative code, or the actual instructions on how to accomplish something. For example if I want to create a folder I’d write:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">New-Item -Path C:\test -ItemType Directory
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>However, when writing DSC configurations you use declarative language, where you describe the desired state without having to instruct exactly how to get there. Using the same example you would add the following resource block to your configuration document to ensure the &lt;code>C:\test&lt;/code> folder exists.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">File CreateTestDir {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> DestinationPath = &amp;#39;C:\test&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Type = &amp;#39;Directory&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="resources">Resources&lt;/h3>
&lt;p>Resources are one of the central building blocks in DSC. Each resource contains the code that takes the declarative syntax you write and makes it happen. In our example above our file resource will translate our desired state into regular PowerShell code, most likely using the same &lt;code>New-Item&lt;/code> cmdlet that we had in our example. This resource is built into Windows so we can’t examine it to prove that.&lt;/p>
&lt;p>There are currently 22 resources available within the built in PSDesiredStateConfiguration module. The table below contains the descriptions of a few, for a full list you can review the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/reference/resources/windows/builtinresource" target="_blank" rel="noopener"
>Microsoft docs&lt;/a>.&lt;/p>
&lt;p>[table id=2 /]&lt;/p>
&lt;p>On top of these built in resources are hundreds more that have been developed by Microsoft, or by the community. They come packaged just like modules and most can be installed directly from the &lt;a class="link" href="https://www.powershellgallery.com/packages?q=DSC" target="_blank" rel="noopener"
>PowerShell Gallery&lt;/a>), some examples are:&lt;/p>
&lt;p>[table id=5 /]&lt;/p>
&lt;p>As you can see DSC can be used to configure a wide variety of components. We can collect resources from several modules and then combine them into one configuration document to describe our desired state.&lt;/p>
&lt;h3 id="idempotent">Idempotent&lt;/h3>
&lt;p>Another interesting aspect of DSC is that the resources are written to be idempotent. This means that in our file example above if the folder already exists it won’t try and create it again.&lt;/p>
&lt;p>There are two main types of resources, class based and MOF based. We’ll be focusing on MOF based in this post.  Within each resource are three functions: &lt;code>Get-TargetResource&lt;/code>, &lt;code>Set-TargetResource&lt;/code> and &lt;code>Test-TargetResource&lt;/code>.  When you run a configuration that contains our file resource example, the &lt;code>Test-TargetResource&lt;/code> will fire first to see whether we’re already in the desired state. That function returns true or false. If the directory doesn’t exist, the &lt;code>Set-TargetResource&lt;/code> will fire to create the folder.&lt;/p>
&lt;p>On the other hand, if we ran the &lt;code>New-Item&lt;/code> snippet and the directory already existed it would throw errors. To avoid this, we would have to wrap it with extra logic ourselves to test whether the folder exits as expected and if not, go ahead and create it.&lt;/p>
&lt;h3 id="so-why-use-desired-state-configuration">So Why Use Desired State Configuration?&lt;/h3>
&lt;p>DSC is a framework that provides the ability to manage our infrastructure with Configuration as Code.  There are several benefits to managing our infrastructure this way. The two biggest reasons I think DSC will work well in my particular scenario is automation and that my infrastructure will be in source control.&lt;/p>
&lt;p>DSC enables automation for building SQL Servers by creating a configuration document that defines exactly how the server should be built. For example, the document tells you things like where the database data and log files should be stored, how tempdb is configured, and whether database mail is enabled.&lt;/p>
&lt;p>The configuration document can then be combined with configuration data, which contains values specific to this build. For example the instance name and perhaps the edition and version of SQL Server to install would be found in the configuration data.  We can reuse the same configuration document for every server, all we would need to do is provide the appropriate configuration data.&lt;/p>
&lt;p>Using configuration as code for building SQL Servers gives us another great benefit because these documents can be checked into source control.  We now know exactly what the servers should look like, and when we make a change that will be tracked in source control. This creates documentation on your entire build. If you needed to rebuild a server during disaster recovery, for example, you could just push that configuration out to a new server and wait for it to end up in your desired state.&lt;/p>
&lt;h3 id="resources-1">Resources&lt;/h3>
&lt;p>If you want to know more about DSC I have listed a few links below. I also plan on expanding this post into a series covering general DSC concepts as well as the specifics for managing SQL Servers with DSC.&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.amazon.com/PowerShell-Desired-State-Configuration-Depth-ebook/dp/B07CNQD3M9/ref=sr_1_1" target="_blank" rel="noopener"
>Pro PowerShell Desired State Configuration: An In-Depth Guide to Windows PowerShell DSC&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/overview/overview" target="_blank" rel="noopener"
>Windows PowerShell Desired State Configuration&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/configurations/configdata" target="_blank" rel="noopener"
>Using configuration data in DS&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/azure/devops/learn/what-is-infrastructure-as-code" target="_blank" rel="noopener"
>Infrastructure as Code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>dbatools with Bert</title><link>https://jpomfret.github.io/dbatools-with-bert/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbatools-with-bert/</guid><description>&lt;p>This weekend, while I was having a great time at SQL Saturday Cleveland, I ran into my friend Bert (&lt;a class="link" href="https://bertwagner.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/bertwagner" target="_blank" rel="noopener"
>t&lt;/a>). He had some dbatools questions, which I was happy to help him with.  Now that dbatools has over 500 commands, it is both awesome and terrifying.  Bert wanted to know how to automate his database backups and then check he was using the correct recovery model.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="backup-your-databases">Backup your databases&lt;/h3>
&lt;p>Bert’s first question was how to automate his database backups. I showed him the &lt;code>Backup-DbaDatabase&lt;/code> command and explained some of the parameters available.&lt;/p>
&lt;p>First, we backed up all the databases on his instance:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase -SqlInstance localhost\sql2017
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase-2.gif"
loading="lazy"
>&lt;/p>
&lt;p>We then looked at specifying a specific database to backup:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">﻿Backup-DbaDatabase -SqlInstance localhost\sql2017 -Database ApplicationDatabase
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_Database-1.gif"
loading="lazy"
>&lt;/p>
&lt;p>This command will backup to the default backup location for your instance. If you want to override that you can use the &lt;code>BackupDirectory&lt;/code> parameter:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance localhost `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Database ApplicationDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-BackupDirectory C:\backups\﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_BackupDir.gif"
loading="lazy"
>&lt;/p>
&lt;p>The final options we looked at were two switches: &lt;code>CompressBackup,&lt;/code>which will make use of backup compression, and &lt;code>CopyOnly,&lt;/code> which will leave your LSN chain intact by taking a copy only backup.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance localhost `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Database ApplicationDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-CompressBackup `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-CopyOnly
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_Switches.gif"
loading="lazy"
>&lt;/p>
&lt;p>Once we had Bert’s databases all backed up and safe he realized he also needed to make sure the database recovery model was set correctly.&lt;/p>
&lt;h3 id="check-database-recovery-model">Check Database Recovery Model&lt;/h3>
&lt;p>Bert wanted to make sure he was using the Full recovery model for his databases. We went about finding any that were in Simple with the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRecoveryModel.gif"
loading="lazy"
>&lt;/p>
&lt;p>We also talked about running this command against multiple instances, either by using a central management server or from a text file:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance $(Get-Content C:\servers.txt) `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-RecoveryModel Simple﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We found some databases in the simple recovery model that we wanted to change. This can easily be accomplished by piping the output of our get command into the set command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Set-DbaDbRecoveryModel -RecoveryMode Full
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRecoveryModel_Set.gif"
loading="lazy"
>&lt;/p>
&lt;h3 id="search-dbatools-commands">Search dbatools Commands&lt;/h3>
&lt;p>The final tip I had for Bert was how to use &lt;code>Find-DbaCommand&lt;/code> to help him find the commands he needed to complete his tasks.&lt;/p>
&lt;p>A lot of the commands have tags, which is a good way to find anything relating to compression. For example:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-DbaCommand -Tag Compression﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-DbaCommand_Compression.gif"
loading="lazy"
>&lt;/p>
&lt;p>You can also just specify keywords and the command will search for any reference of these within the inline command based help for all the commands.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-DbaCommand triggers
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-DbaCommand_Trigger-1.gif"
loading="lazy"
>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>There are many more resources to get help with dbatools. Firstly, their website, &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>https://dbatools.io/&lt;/a>, has a lot of great information on how to get started.&lt;/p>
&lt;p>Secondly, the dbatools slack channel is always full of people who can lend a hand. You can get an invite here: &lt;a class="link" href="https://dbatools.io/slack/" target="_blank" rel="noopener"
>https://dbatools.io/slack/&lt;/a>.&lt;/p>
&lt;p>Finally, feel free to get in contact with me if you have any questions or need some help finding the commands you need to get going with dbatools.&lt;/p></description></item><item><title>T-SQL Tuesday #110 - "Automate All the Things"</title><link>https://jpomfret.github.io/t-sql-tuesday-#110-automate-all-the-things/</link><pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#110-automate-all-the-things/</guid><description>&lt;p>&lt;a class="link" href="https://garrybargsley.com/t-sql-tuesday-110-automate-all-the-things/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Automation is something that interests me greatly, and I think if you have read even one of my previous posts you’ll know that my favorite tool for this kind of work is PowerShell.  This is the perfect topic to kick off T-SQL Tuesday for 2019 so thanks goes to Garry Bargsley for hosting this month.&lt;/p>
&lt;p>One of my goals for 2019 is to improve our server build process, it’s currently reasonably well scripted but there are some definite gaps. I’ve started looking at using PowerShell Desired State Configuration (DSC) to install and configure SQL Server to both meet our needs and increase our speed and efficiency. Although full automation is a stretch goal for this project the DSC technology can certainly scale to accomplish that.&lt;/p>
&lt;h3 id="powershell-desired-state-configuration">PowerShell Desired State Configuration&lt;/h3>
&lt;p>PowerShell DSC is a platform to support the concept of Infrastructure as Code (IaC).  It uses declarative syntax instead of the usual imperative syntax of PowerShell.  This means that you describe your desired state rather than the specific steps needed to get there.  There are two modes for DSC, push and pull, although pull mode offers more features and scalability, we’ll look at writing our configuration and using push mode for this blog post to keep it simple.&lt;/p>
&lt;p>I hope that this blog post will be the first of a series this year as I work to finalize the full process of installing and configuring SQL Server with DSC. For now I will share step 1, ensuring the freshly built Windows OS meets the necessary prerequisites for the SQL Server installation.  Today we’ll look at installing two Windows Features ‘.NET 3.5 Features’ and ‘Active Directory module for Windows PowerShell’ on our target node.&lt;/p>
&lt;p>The first thing we’ll want to do is to update the in-box DSC resources, PSDesiredStateConfiguration. This version comes with Windows PowerShell 4.0 and 5.0 however there have been notable improvements that we will want to take advantage of.  Since the updated module is available in the PowerShell Gallery we can install it to our workstation using the following (note the name change):&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Install-Module PSDSCResources
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Since we are using the push mode we need to make sure any modules we use to write our configurations are available on our target nodes.  I have manually copied the &lt;code>PSDSCResource&lt;/code> module to a path within the &lt;code>$env:PSModulePath&lt;/code> on the target node so it’ll be available when the configuration is enacted there. There are other ways to handle this including setting up a resource module file share.&lt;/p>
&lt;h3 id="writing-our-first-configuration">Writing our First Configuration&lt;/h3>
&lt;p>We’re now ready to write our first configuration, although we are still writing PowerShell the syntax is a little different.  One of my favorite things about PowerShell is using the command based help to discover how to execute new cmdlets and functions, DSC is no different here. We can use &lt;code>Get-DscResource&lt;/code> to list all the resources we have available.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDscResource.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Since we are going to use the WindowsFeature resource we can find out how to use that by passing in the &lt;code>-Syntax&lt;/code> parameter to &lt;code>Get-DscResource&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/WindowsFeatureSyntax.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The only required parameter for the WindowsFeature resource is Name so we’ll include that.  I also like to include the ‘Ensure’ parameter, it defaults to ‘Present’ but I feel it makes it clearer to specifically define that.  I also want to set ‘IncludeAllSubFeature’ to true so those get installed also. &lt;/p>
&lt;p>The below code is all we need to get started with our first configuration. Apart from the resource blocks that we have already mentioned there are a couple of other important parts to note. First is the keyword &lt;code>Configuration&lt;/code> which shows we are writing a DSC Configuration document. In this case I’ve named our configuration ‘SQLServerPreReq’. Secondly, the &lt;code>Node&lt;/code> keyword is important, this defines the target node for our configuration.  It’s important to remember that this is a simple example, it is possible to pass in multiple node names or to parameterize that node name to make our configuration more useful.&lt;/p>
&lt;p>The last line of code calls our SQLServerPreReq configuration and specifies the output path.  When you call a configuration a MOF file is created which is the document that will be sent to the target node and used to both enact the configuration and monitor for configuration drift (a feature of the pull mode that we’ll save for a future post).&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration SQLServerPreReq {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDSCResources
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node &amp;#39;DscSvr2&amp;#39; {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeature dotNet
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = &amp;#39;NET-Framework-Features&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeature ADPowershell
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = &amp;#39;RSAT-AD-PowerShell&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SQLServerPreReq -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Once we have our MOF file the final step is to enact our configuration against our target node.  I’m using the &lt;code>-wait&lt;/code> and &lt;code>-verbose&lt;/code> parameters so that the configuration doesn’t run in the background and we can view the verbose messages on screen as it executes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Start-DscConfiguration -Path .\Output\ -Wait -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/01/startDscConfiguration1-1.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration1-1.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Once this runs successfully you can confirm the features are installed using &lt;code>Get-WindowsFeature&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WindowsFeature -ComputerName dscsvr2 `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Name @(&amp;#39;RSAT-AD-PowerShell&amp;#39;,&amp;#39;NET-Framework-Features&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/GetWindowsFeature.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="using-composite-resources">Using Composite Resources&lt;/h3>
&lt;p>This simple configuration shows how you can install two windows features using PowerShell DSC, however it is rather redundant to have to specify separate resource blocks for each feature.  Since we updated our in-box DSC Resources to the newer PSDSCResources module we are able to use the new WindowsFeatureSet resource which is an example of a composite resource. We can review the syntax again using Get-DscResource:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscResource -Name WindowsFeatureSet -Syntax
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The main difference is that WindowsFeatureSet takes an array of features to install and then translates this to use multiple WindowsFeature resources when the MOF file is created.  This allows us to keep our configuration document as tidy and concise as possible.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration SQLServerPreReq_v2 {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDSCResources
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node &amp;#39;DscSvr2&amp;#39; {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeatureSet PreReqFeatures
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = @(&amp;#39;NET-Framework-Features&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;RSAT-AD-PowerShell&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SQLServerPreReq_v2 -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>That’s step one towards automating my SQL Server builds, I’m looking forward to adding more to this series. Thanks again to Garry for picking the perfect T-SQL Tuesday topic to kick off the year with.&lt;/p></description></item><item><title>PSParameterSets: Mandatory sometimes</title><link>https://jpomfret.github.io/psparametersets-mandatory-sometimes/</link><pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/psparametersets-mandatory-sometimes/</guid><description>&lt;p>I came across a situation this week where I wanted to add the option of running an existing script,  for a specific server/database combination.  The script currently has no parameters and runs against all servers in the environment from a scheduled task.  I wanted to make sure that behavior didn’t change. The other requirement was that if I specified Server, Database should be a mandatory parameter and vice versa.&lt;/p>
&lt;p>The final solution was to add the two parameters to a parameter set and make them both mandatory.  I also had to add a different DefaultParameterSet (thanks to &lt;a class="link" href="http://twitter.com/awickham" target="_blank" rel="noopener"
>Andrew&lt;/a> for this idea), otherwise it defaulted to the defined parameter set, meaning the script always required both Server and Database parameters.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[CmdletBinding(DefaultParametersetname=&amp;#34;Normal&amp;#34;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">param (
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [Parameter(Mandatory = $True, ParameterSetName=&amp;#39;Specific&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [string]$ServerName,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [Parameter(Mandatory = $True, ParameterSetName=&amp;#39;Specific&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [string]$DatabaseName
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">if($PSCmdlet.ParameterSetName -eq &amp;#39;Normal&amp;#39;) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host &amp;#39;Running without params&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">} else {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host (&amp;#34;Server: {0}&amp;#34; -f $ServerName)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host (&amp;#34;Database: {0}&amp;#34; -f $DatabaseName)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I saved the above code as params.ps1 to ensure my test cases worked. As you can see in my testing below, I can still run params.ps1 without any parameters, this replicates the current behavior of my nightly job. &lt;/p>
&lt;p>I can also now pass in Server and Database parameters, if I specify one the script will prompt for the other since they are both required.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ParamsTest.jpg"
loading="lazy"
>&lt;/p></description></item><item><title>T-SQL Tuesday #108 - Non SQL Server Technologies</title><link>https://jpomfret.github.io/t-sql-tuesday-#108-non-sql-server-technologies/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#108-non-sql-server-technologies/</guid><description>&lt;p>&lt;a class="link" href="https://curiousaboutdata.com/2018/10/29/t-sql-tuesday-108-invitation-non-sql-server-technologies/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>It’s T-SQL Tuesday time again, I have struggled in the last month or two to get anything up on my blog. Turns out weddings are pretty time consuming ?! Now that I’m happily married and home from an amazing &lt;a class="link" href="https://www.instagram.com/jpomfret/" target="_blank" rel="noopener"
>honeymoon in Hawaii&lt;/a> it’s back to work on my blog and professional development.  Which makes this T-SQL Tuesday topic a perfect one to get back to, so thanks to Malathi Mahadeven (&lt;a class="link" href="https://curiousaboutdata.com" target="_blank" rel="noopener"
>B&lt;/a>|&lt;a class="link" href="https://twitter.com/sqlmal" target="_blank" rel="noopener"
>T&lt;/a>) for hosting this month.&lt;/p>
&lt;p>I feel like with last week’s PASS Summit (I didn’t attend this year so just watching from afar) it makes it even harder than usual to pick just one thing to learn.  There are so many things right now that I want to read about or fiddle with.&lt;/p>
&lt;p>I’ve decided to pick a main subject, with an auxiliary bonus area attached - kind of cheating, I know.  I’ve been working on a project at work to automate our SQL Server builds with Powershell Desired State Configuration (DSC) so this will be my main goal. I already have a basic understanding of how DSC works and how to install SQL Server with it, I want to improve this knowledge to the point where I can present a session on it.&lt;/p>
&lt;p>The side goal is docker/containers/kubernetes (maybe), I’m wondering if I could use these to test my DSC configurations, maybe not to install SQL Server (I have no idea though) but I imagine I could configure SQL Server running in a container.&lt;/p>
&lt;p>I saw the tweet below last week from the beard, &lt;a class="link" href="https://twitter.com/sqldbawithbeard" target="_blank" rel="noopener"
>Rob Sewell&lt;/a>, that quoted &lt;a class="link" href="https://twitter.com/bobwardms" target="_blank" rel="noopener"
>Bob Ward’s&lt;/a> thoughts on learning directions.  Feels like this is probably solid advice to justify my side goal.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/sqldbawithbeard/status/1061032613979267072" target="_blank" rel="noopener"
>https://twitter.com/sqldbawithbeard/status/1061032613979267072&lt;/a>&lt;/p>
&lt;h2 id="learning-plan">Learning Plan&lt;/h2>
&lt;h4 id="learn-dsc-basics--completed">Learn DSC Basics – completed&lt;/h4>
&lt;p>I’ve already started learning DSC, I was lucky enough to take a PowerShell DSC class a couple of months ago and that combined with reading online documentation and blogs has given me a good base.&lt;/p>
&lt;p>Resources:&lt;/p>
&lt;ul>
&lt;li>Microsoft Docs - &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/overview" target="_blank" rel="noopener"
>https://docs.microsoft.com/en-us/powershell/dsc/overview&lt;/a>&lt;/li>
&lt;li>SQLServerDSC Github - &lt;a class="link" href="https://github.com/PowerShell/SqlServerDsc" target="_blank" rel="noopener"
>https://github.com/PowerShell/SqlServerDsc&lt;/a>&lt;/li>
&lt;li>DSC Install of SQL Server &lt;a class="link" href="https://chrislumnah.com/2017/03/07/dsc-install-of-sql-server/" target="_blank" rel="noopener"
>https://chrislumnah.com/2017/03/07/dsc-install-of-sql-server/&lt;/a>&lt;/li>
&lt;li>Making modules available in Push mode &lt;a class="link" href="http://nanalakshmanan.com/blog/Push-Config-Pull-Module/" target="_blank" rel="noopener"
>http://nanalakshmanan.com/blog/Push-Config-Pull-Module/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="learn-more-about-how-dsc-resources-are-written-and-developed">Learn more about how DSC resources are written and developed&lt;/h4>
&lt;p>This is where I currently am, I have the basics up and running to install SQL Server (blog post coming one day) but there are things I’d like to configure that aren’t currently built into the SQLServerDSC module.  Since this is open sourced on github I have the opportunity to learn while doing, I’ve already started working on adding an SQL Agent Operator resource so I can configure an operator during my install.&lt;/p>
&lt;h4 id="dsc-sql-server-and-containers">DSC, SQL Server and Containers?&lt;/h4>
&lt;p>Can I even use DSC to configure SQL Server running in a container? I have no idea, but I plan on finding out.  If this is possible it feels like this would be a really easy way to spin up ‘unconfigured’ SQL Server and test my configurations.  If not – hey maybe I learned a bit about containers along the way, and it feels like those are only getting more mainstream.&lt;/p>
&lt;h4 id="final-goal---present-a-dsc-session">Final goal - Present a DSC session&lt;/h4>
&lt;p>My final goal is to create a &amp;lsquo;Automate your SQL Server Install with DSC&amp;rsquo; session. Presenting on something forces you to learn it in depth, this will be great for myself and hopefully the community. Hopefully it&amp;rsquo;ll make its way to a SQL Saturday next year.  The session would be a crash course on DSC specifically to install and configure SQL Server with the end goal of attendees being able to use this process to automate their own builds. Watch this space, currently in the idea phase.&lt;/p></description></item><item><title>Testing Availability Group Read-Only Routing with dbatools</title><link>https://jpomfret.github.io/testing-availability-group-read-only-routing-with-dbatools/</link><pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/testing-availability-group-read-only-routing-with-dbatools/</guid><description>&lt;p>I recently set up an Availability Group with the intent of using the secondary as a read only replica for reporting.  We have a few AG&amp;rsquo;s in our environment already but currently none are using this feature.&lt;/p>
&lt;p>I&amp;rsquo;m not going to step through setting up the AG or configuring the readable secondary as there are plenty of good posts out there as well as the official &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-only-access-on-an-availability-replica-sql-server?view=sql-server-2017" target="_blank" rel="noopener"
>books online documentation&lt;/a>.&lt;/p>
&lt;p>Once my AG was created I set the &amp;lsquo;Connections in Primary Role&amp;rsquo; to &amp;lsquo;Allow read/write connections&amp;rsquo; and the &amp;lsquo;Readable Secondary&amp;rsquo; to &amp;lsquo;Read-intent only&amp;rsquo; as shown below. On a side note it&amp;rsquo;s important to set these for both instances, if you&amp;rsquo;re running with 01B as the Primary after a failover by setting both you&amp;rsquo;ll get the same behavior, with read only connections being routed to the now secondary, 01A server.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/AGReplicas.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The other part I needed to set up was &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-only-routing-for-an-availability-group-sql-server?view=sql-server-2017" target="_blank" rel="noopener"
>read-only routing&lt;/a>, this enables SQL Server to reroute those read only connections to the appropriate replica.  You can also list the read only replicas by priority if you have multiple available or you can group them to enable load-balancing.&lt;/p>
&lt;p>Although this seems to be setup correctly so that connections that specify their application intent of read only will be routed to the secondary node I wanted to prove it. I used the &lt;a class="link" href="https://dbatools.io/functions/connect-dbainstance/" target="_blank" rel="noopener"
>Connect-DbaInstance&lt;/a> function from dbatools to connect to the listener name with the -ApplicationIntent property set to &amp;lsquo;ReadOnly&amp;rsquo;.&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance AGListenerName `
-Database DatabaseInAG `
-ApplicationIntent ReadOnly&lt;/p>
&lt;p>$svr.Query(&amp;lsquo;Select @@ServerName as ServerName&amp;rsquo;)&lt;/p>
&lt;h2 id="servername">ServerName&lt;/h2>
&lt;p>*******01B&lt;/p>
&lt;p>You can see it routed correctly to 01B which is currently the secondary node.  If I don&amp;rsquo;t specify the ApplicationIntent property on the connection it&amp;rsquo;ll be routed to the primary.&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance AGListenerName `
-Database DatabaseInAG&lt;/p>
&lt;p>$svr.Query(&amp;lsquo;Select @@ServerName as ServerName&amp;rsquo;)&lt;/p>
&lt;h2 id="servername-1">ServerName&lt;/h2>
&lt;p>*******01A&lt;/p>
&lt;p>This was a quick and easy way to ensure my read only routing was working as expected, and another great use of dbatools.&lt;/p></description></item><item><title>Checking backups with dbachecks</title><link>https://jpomfret.github.io/checking-backups-with-dbachecks/</link><pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/checking-backups-with-dbachecks/</guid><description>&lt;p>Folks, there is something fantastic coming from the creators of dbatools!&lt;/p>
&lt;p>Chrissy LeMaire (&lt;a class="link" href="https://blog.netnerds.net/" target="_blank" rel="noopener"
>blog&lt;/a>|&lt;a class="link" href="https://twitter.com/cl" target="_blank" rel="noopener"
>twitter&lt;/a>) and Rob Sewell (&lt;a class="link" href="https://sqldbawithabeard.com/" target="_blank" rel="noopener"
>blog&lt;/a>|&lt;a class="link" href="https://twitter.com/sqldbawithbeard" target="_blank" rel="noopener"
>twitter&lt;/a>) just announced something big at SQLBits 2018, a new PowerShell module that combines &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> with &lt;a class="link" href="https://github.com/pester/Pester" target="_blank" rel="noopener"
>Pester&lt;/a> to ensure your environment is &amp;ldquo;as expected&amp;rdquo;. I&amp;rsquo;ve been lucky enough to get to know both Chrissy and Rob by contributing to dbatools and when they introduced me to this new module I instantly saw a lot of potential.&lt;/p>
&lt;p>I&amp;rsquo;m going to start off with just a small way to gain some value from dbachecks, ensuring your backups are completing with the frequency you expect.&lt;/p>
&lt;p>The module is hosted on &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>GitHub&lt;/a> which means you can fork and contribute to it just as you would with any other open source project. There is also a lot of useful information out there including the &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks#dbachecks" target="_blank" rel="noopener"
>readme&lt;/a>.  Reviewing this readme is an important first step as there are a couple of prerequisites and some potential caveats when you go to update the module.&lt;/p>
&lt;p>You can download a copy of the module from the PowerShell Gallery (if this doesn&amp;rsquo;t work for you due to corporate firewalls, PowerShell version etc. head back to the readme for more ways to get the module):&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Install-Module&lt;/span> &lt;span class="n">dbachecks&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>First off let&amp;rsquo;s take a look at &lt;code>Get-DbcCheck&lt;/code> to look for checks we may want to implement: &lt;img src="https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbCCheck.jpg"
width="650"
height="114"
srcset="https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbCCheck_hub0361c16f2227e05f77352518f09a3a0_66738_480x0_resize_q75_box.jpg 480w, https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbCCheck_hub0361c16f2227e05f77352518f09a3a0_66738_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="570"
data-flex-basis="1368px"
>&lt;/p>
&lt;p>Each check has one unique tag which basically names the check and then a number of other tags that can also be used to call a collection of checks.&lt;/p>
&lt;p>For this example we are going to use several checks to ensure that we meet the following requirements:&lt;/p>
&lt;ul>
&lt;li>Full backup once a week - using &lt;code>LastFullBackup&lt;/code>&lt;/li>
&lt;li>Differential backup once a day - using &lt;code>LastDiffBackup&lt;/code>&lt;/li>
&lt;li>Log backup every hour - using &lt;code>LastLogBackup&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Since each of the three checks we want to run also have the &lt;code>LastBackup&lt;/code> tag we can use that to call the collection of checks at once.&lt;/p>
&lt;p>There are many ways to point dbachecks at your instances, for this simple example we&amp;rsquo;ll just pass in one server name to check.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">$server&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="s2">&amp;#34;ServerName&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Invoke-DbcCheck&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="nv">$server&lt;/span> &lt;span class="n">-Check&lt;/span> &lt;span class="n">LastBackup&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksFailing.jpg"
width="649"
height="470"
srcset="https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksFailing_hu15024b79f8af7d5ceaf8a211b077fb9f_205773_480x0_resize_q75_box.jpg 480w, https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksFailing_hu15024b79f8af7d5ceaf8a211b077fb9f_205773_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>As you can clearly see from the test results there is a lot of red, meaning I&amp;rsquo;m not meeting backup requirements. However looking closer at the context we can see that the check is not configured for my specific needs &amp;ldquo;StackOverflow full backups on Server should be less than 1 days&amp;rdquo;, but I only require a full backup within 7 days.&lt;/p>
&lt;p>The checks are set up in a way that make them extremely flexible. You can configure them to meet your needs exactly. We can use &lt;code>Get-DbcConfig&lt;/code> to review the backup configurations.  Here you can see we&amp;rsquo;re looking for full backups every 1 day (policy.backup.fullmaxdays), differentials every 25 hours (policy.backup.diffmaxhours) and log backups every 15 minutes (policy.backup.logmaxminutes).&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbcConfig.jpg"
width="650"
height="209"
srcset="https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbcConfig_hu03ba9c2abcee862b937048a682e1f9d5_124831_480x0_resize_q75_box.jpg 480w, https://jpomfret.github.io/checking-backups-with-dbachecks/Get-DbcConfig_hu03ba9c2abcee862b937048a682e1f9d5_124831_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="746px"
>&lt;/p>
&lt;p>Let&amp;rsquo;s change these configuration properties to match our requirements of a full backup within the last 7 days and a log backup in the last 60 minutes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Set-DbcConfig&lt;/span> &lt;span class="n">-Name&lt;/span> &lt;span class="n">policy&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">backup&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">fullmaxdays&lt;/span> &lt;span class="n">-Value&lt;/span> &lt;span class="n">7&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Set-DbcConfig&lt;/span> &lt;span class="n">-Name&lt;/span> &lt;span class="n">policy&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">backup&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">logmaxminutes&lt;/span> &lt;span class="n">-Value&lt;/span> &lt;span class="n">60&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now that the configuration is setup correctly we can rerun and confirm our environment backups are in the green.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksSuccess.jpg"
width="650"
height="530"
srcset="https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksSuccess_hu7741d7f664fdc4e4c34c92acd55bbce5_186017_480x0_resize_q75_box.jpg 480w, https://jpomfret.github.io/checking-backups-with-dbachecks/ChecksSuccess_hu7741d7f664fdc4e4c34c92acd55bbce5_186017_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;p>Validating your backups are running is just one small example of how you can utilize dbachecks to keep your environment in line.  In fact this is just the tip of the iceberg, there are 80 checks as of writing this post as well as multiple ways to display the results (including a pretty impressive PowerBi dashboard - &lt;a class="link" href="http://claudioessilva.eu/2018/02/22/dbachecks-using-power-bi-dashboards-to-analyse-results/" target="_blank" rel="noopener"
>Cláudio Silva has a great post on that&lt;/a>).&lt;/p>
&lt;p>I hope this has peaked your interest in dbachecks, I suggest heading over to the &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks#dbachecks" target="_blank" rel="noopener"
>readme&lt;/a> to learn more or download a copy and get checking right away!&lt;/p>
&lt;p>Useful links:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks on github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://dbachecks.io/blog" target="_blank" rel="noopener"
>dbachecks blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://dbachecks.io/introducing" target="_blank" rel="noopener"
>Introducing dbachecks&lt;/a> - including links to a plethora of blog posts by other contributors&lt;/li>
&lt;/ul></description></item></channel></rss>