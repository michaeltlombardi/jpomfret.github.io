<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tsql on Jess Pomfret</title><link>https://jpomfret.github.io/tags/tsql/</link><description>Recent content in tsql on Jess Pomfret</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Tue, 07 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://jpomfret.github.io/tags/tsql/index.xml" rel="self" type="application/rss+xml"/><item><title>Using Extended Events to determine the best batch size for deletes</title><link>https://jpomfret.github.io/p/using-extended-events-to-determine-the-best-batch-size-for-deletes/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/p/using-extended-events-to-determine-the-best-batch-size-for-deletes/</guid><description>&lt;p>I found myself needing to clear out a large amount of data from a table this week as part of a clean up job.  In order to avoid the transaction log catching fire from a long running, massive delete, I wrote the following T-SQL to chunk through the rows that needed to be deleted in batches. The question is though, what’s the optimal batch size?&lt;/p>
&lt;p>It’s worth noting that an index on the date column was required for this to be as efficient as possible. The table I was deleting from had 4 billion rows and I didn’t want to scan that each time!&lt;/p>
&lt;p>DECLARE @Before DATE = DATEADD(DAY,-30,GETDATE()),
@BatchSize INT = 50000&lt;/p>
&lt;p>WHILE (1=1)
BEGIN&lt;/p>
&lt;pre>&lt;code>DELETE TOP (@BatchSize) t
FROM dbo.bigTable t
WHERE Date &amp;lt; @Before
-- if we deleted less than a full batch we're done
IF @@rowcount &amp;lt; @BatchSize
BREAK;
-- add a delay between batches
WAITFOR DELAY '00:00:01'
&lt;/code>&lt;/pre>
&lt;p>END&lt;/p>
&lt;p>Time for a science experiment. The easiest way to determine the optimal batch size is to run some tests. I decided to test deleting in batches of 10k, 25k, 50k and 100k and measure the delete durations with extended events.&lt;/p>
&lt;p>My goal was two part - to delete as many rows as possible in a two-hour maintenance window, but also to reduce the amount of time locks were held on the target table.&lt;/p>
&lt;p>The following T-SQL creates an extended events session that captures one event, &lt;code>sqlserver.sql_statement_completed&lt;/code>, and filters on both my username and the statement like ‘delete top%’. I didn’t choose a target as I just chose to watch the live data, but you could easily add an event_file if you want to persist the data.&lt;/p>
&lt;p>CREATE EVENT SESSION [DeleteExperiment] ON SERVER
ADD EVENT sqlserver.sql_statement_completed(SET collect_statement=(1)
ACTION(sqlserver.nt_username)
WHERE ([sqlserver].[equal_i_sql_unicode_string]([sqlserver].[session_nt_user],N&amp;rsquo;JessUserName&amp;rsquo;)
AND [sqlserver].[like_i_sql_unicode_string]([statement],N&amp;rsquo;delete top%&amp;rsquo;)
)
)
GO&lt;/p>
&lt;p>Once the extended events session had been successfully created and was running, I opened the ‘Watch Live Data’ pane and started running my deletes in another window.  I left each experiment running for a while to make sure I got a decent sample size for each batch size.&lt;/p>
&lt;p>Once I’d cycled through the different batch sizes, I used the grouping and aggregation features in the Extended events wizard, shown on the toolbar below:&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/1tCeCwFt3DECWGZha_oV0qjLs0tYq3d3JSxccZc7Fy931ZkgvrMeZrn0665AOrR4GtFe0kBEPgrwBSNcGbx7-axO5QflWksX6BkB58AF6gvydBSV-7S0lrMfwSwIH2qBp1Jm6K7N"
loading="lazy"
>&lt;/p>
&lt;p>I grouped by ‘last_row_count’ which is my batch size, and then calculated the average duration for each group. You can see in the screenshot below the values for each.&lt;/p>
&lt;p>&lt;img src="https://lh5.googleusercontent.com/uku5O7ozzE8RSSh1ehpNdNj5EEWdizFtbC0ndXZfPMh5G5zCSEsIP8Vk3R3sdt7c3DsGf9If2__M7LdmjQ3WHSce5LDjMyhtRbmxLLPmOtMF4XVWlGovW3ZdDaTUIn3l-pwMOUXI"
loading="lazy"
>&lt;/p>
&lt;p>The duration unit is microseconds for the sp_statement_completed event so after some squinting and calculations the results are as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Batch Size&lt;/th>
&lt;th>Average Duration (Seconds)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10,000&lt;/td>
&lt;td>0.46&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25,000&lt;/td>
&lt;td>1.74&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50,000&lt;/td>
&lt;td>3.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100,000&lt;/td>
&lt;td>7.57&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For now, I’ve decided to go with batches of 50,000 for the deletes. Depending on how things go I might change this to 25,000 as in my mind both those batch sizes met my criteria.&lt;/p>
&lt;p>Hope this blog post has given you some ideas for testing out different scenarios with Extended Events.&lt;/p></description></item><item><title>T-SQL Tuesday #136: Blog About Your Favorite Data Type (Or Least Favorite)</title><link>https://jpomfret.github.io/p/t-sql-tuesday-#136-blog-about-your-favorite-data-type-or-least-favorite/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/p/t-sql-tuesday-#136-blog-about-your-favorite-data-type-or-least-favorite/</guid><description>&lt;p>&lt;a class="link" href="https://www.brentozar.com/archive/2021/03/tsql2sday-136-invitation-blog-about-your-favorite-data-type-or-least-favorite/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Shout out to Brent Ozar (&lt;a class="link" href="http://brentozar.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/BrentO" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month&amp;rsquo;s TSQL2sday. It’s time again for this monthly blog party and he wants to know all about our favourite or least favourite data types. To start with I was having a hard time thinking of a favourite data type. I know I have favourite words (merge and plethora, in case you’re wondering), but it seems a bit wrong to pick favourites here – I mean lots of them are great in their own right. Then it came to me- my favourite data type is the right one for the job at hand. Feels like I’m skirting the question a little here, but bear with me.&lt;/p>
&lt;p>Let’s talk about accuracy and precision, and how much of it you actually need.  The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).&lt;/p>
&lt;p>First things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:&lt;/p>
&lt;p>Let’s talk about accuracy and precision, and how much of it you actually need.  The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).&lt;/p>
&lt;p>First things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Datatype&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;th>Storage Size&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>One day&lt;/td>
&lt;td>3 bytes&lt;/td>
&lt;td>Doesn&amp;rsquo;t include time&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SmallDateTime&lt;/td>
&lt;td>One minute&lt;/td>
&lt;td>4 bytes&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTime&lt;/td>
&lt;td>Rounded to increments of .000, .003, or .007 seconds&lt;/td>
&lt;td>8 bytes&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTime2&lt;/td>
&lt;td>100 nanoseconds&lt;/td>
&lt;td>6 bytes for precision less than 3.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7 bytes for precision 3 or 4.&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All other precision require 8 bytes.&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Time&lt;/td>
&lt;td>100 nanoseconds (1 millisecond in Informatica)&lt;/td>
&lt;td>5 bytes&lt;/td>
&lt;td>Doesn&amp;rsquo;t include date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTimeOffset&lt;/td>
&lt;td>100 nanoseconds&lt;/td>
&lt;td>10 bytes&lt;/td>
&lt;td>Includes time zone awareness&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This table was created from information on Microsoft Docs, and there is plenty more information if you’re interested: &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener"
>Data types (Transact-SQL) - SQL Server | Microsoft Docs&lt;/a>.&lt;/p>
&lt;h2 id="the-setup">The Setup&lt;/h2>
&lt;p>For this example, let’s say I have a table that gets loaded by a daily batch job. The column we will discuss needs to store just the date, no time and no need for time zone awareness. This leaves us with 4 options to investigate.&lt;/p>
&lt;p>In order to demonstrate the importance of choosing the right datatype I’m going to create 4 really simple tables. They will each have one column, with a default value of today&amp;rsquo;s date, and no time information.  These tables are totally unrealistic, but it isolates the storage required for a single column and will allow us to focus in on the differences our decision here can cause in a small test case.&lt;/p>
&lt;p>create table dataTypeDate (
batchDate date default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeSmallDateTime (
batchSmallDateTime smalldatetime default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeDateTime (
batchDateTime datetime default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeDateTime2 (
batchDateTime2 datetime2 default (convert(date,getdate()))
)&lt;/p>
&lt;p>Once we have four tables we can run the following to load our batch for the day. To simulate this we’re going to insert the default values, 500,000 times.  You can see here I’m using the &lt;code>GO 500000&lt;/code> syntax to run each insert half a million times.&lt;/p>
&lt;p>insert into dataTypeDate
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeSmallDateTime
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeDateTime
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeDateTime2
default values;
GO 500000&lt;/p>
&lt;h2 id="the-comparison">The Comparison&lt;/h2>
&lt;p>First, let’s take a look at the first row in each table so we can see the different accuracies of our columns:&lt;/p>
&lt;p>SELECT TOP 1 batchDate FROM dataTypeDate
SELECT TOP 1 batchSmallDateTime FROM dataTypeSmallDateTime
SELECT TOP 1 batchDateTime FROM dataTypeDateTime
SELECT TOP 1 batchDateTime2 FROM dataTypeDateTime2&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/topOneRow.jpg"
loading="lazy"
alt="query results from SSMS for select top 1 from each table"
>&lt;/p>
&lt;p>We have the exact same data in these columns, just a date, no time. However, because of the different datatypes we used when we defined our tables, you can clearly see the different accuracies that were available to us.  The requirement for the datatype to be able to meet stricter accuracies is where the additional storage comes in. As you saw in the table above, storing just a date, which is all we need in this situation, will cost us 3 bytes. Any other datatype we choose will add unnecessary storage. While we’re talking bytes, and it doesn’t seem like a big deal for one row with one column, it adds up quickly when we’re talking millions of rows with multiple columns that are inappropriate typed.&lt;/p>
&lt;p>The following query will display some information about our four tables, including the number of 8k pages that each table is using.&lt;/p>
&lt;p>SELECT
schema_name(obj.SCHEMA_ID) as SchemaName,
obj.name as TableName,
ind.type_desc as IndexType,
pas.row_count as NumberOfRows,
pas.used_page_count as UsedPageCount,
(pas.used_page_count * 8)/1024 as SizeUsedMB,
par.data_compression_desc as DataCompression,
(pas.reserved_page_count * 8)/1024 as SizeReservedMB
FROM sys.objects obj
INNER JOIN sys.indexes ind
ON obj.object_id = ind.object_id
INNER JOIN sys.partitions par
ON par.index_id = ind.index_id
AND par.object_id = obj.object_id
INNER JOIN sys.dm_db_partition_stats pas
ON pas.partition_id = par.partition_id
WHERE obj.schema_id &amp;lt;&amp;gt; 4
ORDER BY UsedPageCount desc&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/tables-1024x135.jpg"
loading="lazy"
alt="results of query to show number of pages used by each table"
>&lt;/p>
&lt;p>We can see that there are 500,000 rows in each of our tables and even with only one column there is a sizable difference in the number of pages needed if we chose DateTime or DateTime2 over just the Date type. It’s about a 25% savings- multiply that out by multiple columns across multiple tables and we’re going to start seeing a pretty sizable difference in our storage needs.&lt;/p>
&lt;p>This is still a pretty small dataset, but it does clearly show that there is a significant difference in the amount of storage needed if we choose an unnecessarily accurate datatype for our date values.&lt;/p>
&lt;h2 id="storage-is-cheap--why-do-i-care">&lt;strong>Storage is cheap – why do I care?&lt;/strong>&lt;/h2>
&lt;p>Storage might not be our biggest concern, although enterprise grade storage is not as cheap as the USB drives at the supermarket checkout, but there are several other reasons why this wasted space is a big deal. Here’s a few:&lt;/p>
&lt;p>&lt;strong>Buffer Cache&lt;/strong> – When SQL Server needs to interact with our data it first reads it into memory. Wasted space on disk then becomes wasted space in memory. That means we can store less data in the buffer cache and will have to flush out pages more quickly than if they were optimised.&lt;/p>
&lt;p>&lt;strong>Backup\restore&lt;/strong> – The bigger your database the longer it’s going to take to perform backup and restore activities.&lt;/p>
&lt;p>&lt;strong>Transaction log activity&lt;/strong> – The bigger the record in SQL Server the more space it’ll need when any operations are written to the transaction log. This means you’ll need more disk space for your transaction log and your t-log backups will be larger. More wasted space.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>So in a truly diplomatic fashion, I rate all the datatypes equally. We’ve only looked at types specific to datetimes in this post, but each and every datatype is suitable for storing certain data. The most important point is that we make solid decisions on both the type of data we want to store and the accuracy/precision needed to store that data.&lt;/p>
&lt;p>Thanks for reading, and thanks again to Brent for hosting.&lt;/p></description></item><item><title>Using T-SQL to Aggregate Strings</title><link>https://jpomfret.github.io/p/using-t-sql-to-aggregate-strings/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/p/using-t-sql-to-aggregate-strings/</guid><description>&lt;p>I’m a SQL Server Database Engineer by day, but I must say my blog has a lot more PowerShell and automation posts than T-SQL.  However, last week I found a really great T-SQL aggregate function that I had no idea existed, so I thought I’d share it with you.&lt;/p>
&lt;p>I have been working on a project to document our SQL Server environment and create GitHub issues for things that need fixed. Issues are written in markdown so you can easily generate some pretty good looking issues with plenty of data using PowerShell. This is worth a blog post of it’s own, so keep an eye out for that soon.&lt;/p>
&lt;p>Long story short I wanted a way to be able to list all the SQL Server instances on the server I was logging the issue for. I have a database with two tables, one that contains server information and one that contains instance information. Running the following gets me one row per server/instance combination.&lt;/p>
&lt;p>SELECT s.ServerListId, s.ServerName, i.InstanceListId, i.InstanceName
FROM ServerList s
INNER JOIN InstanceList i
ON s.ServerListId = i.ServerListId&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ServerListId&lt;/th>
&lt;th>ServerName&lt;/th>
&lt;th>InstanceListId&lt;/th>
&lt;th>InstanceName&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>MSSQL1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>MSSQL3&lt;/td>
&lt;td>3&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>NAMEDINST1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>5&lt;/td>
&lt;td>NAMEDINST2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>I started thinking about how to group this data by server name and then concatenate the instance names together. Luckily a quick google found &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener"
>STRING_AGG()&lt;/a>. This T-SQL aggregate function has only been available since SQL Server 2017, and does exactly what I needed. It takes two parameters, the first being the column name that should be aggregated and the second a separator to use.&lt;/p>
&lt;p>For this example I’ll group by ServerName, and aggregate the InstanceName column using a comma to separate the values.&lt;/p>
&lt;p>SELECT ServerName, STRING_AGG(InstanceName,&amp;rsquo;, &amp;lsquo;) as InstanceName
FROM ServerList s
INNER JOIN InstanceList i
ON s.ServerListId = i.ServerListId
GROUP BY ServerName&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ServerName&lt;/th>
&lt;th>InstanceName&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MSSQL1&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MSSQL2&lt;/td>
&lt;td>MSSQLSERVER, NAMEDINST1, NAMEDINST2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MSSQL3&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Hope some of you find this quick T-SQL post useful. It definitely fit my need well for this scenario.&lt;/p></description></item></channel></rss>