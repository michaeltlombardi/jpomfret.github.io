<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Jess Pomfret</title><link>https://jpomfret.github.io/post/</link><description>Recent content in Posts on Jess Pomfret</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Thu, 07 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jpomfret.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Run ActiveDirectory PowerShell commands against another domain</title><link>https://jpomfret.github.io/ad-powershell-domain/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/ad-powershell-domain/</guid><description>&lt;img src="https://jpomfret.github.io/ad-powershell-domain/cover.jpg" alt="Featured image of post Run ActiveDirectory PowerShell commands against another domain" />&lt;p>Active Directory groups are used all over our IT estates. They can be used to simplify managing SQL Server access (&lt;a class="link" href="https://jesspomfret.com/sql-server-permissions-via-ad/" target="_blank" rel="noopener"
>Discover SQL Server Permissions hidden via AD Group Membership&lt;/a>) as well as for other applications. One of my favourite commands from the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/activedirectory/?view=windowsserver2022-ps" target="_blank" rel="noopener"
>ActiveDirectory PowerShell module&lt;/a> is &lt;code>Get-AdUser&lt;/code>, specifically when used in the following snippet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-ADUser&lt;/span> &lt;span class="n">UserName&lt;/span> &lt;span class="n">-Properties&lt;/span> &lt;span class="n">MemberOf&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="nb">Select-Object&lt;/span> &lt;span class="n">-ExpandProperty&lt;/span> &lt;span class="n">MemberOf&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This snippet will list all the groups the user is in. Super useful for troubleshooting permissions issues or if you’re onboarding a new employee and want to see what groups their peers are in. But what happens if your environment consists of multiple domains, and you have a query about a user in another domain?&lt;/p>
&lt;p>Well good news - here’s the answer!&lt;/p>
&lt;p>First we need to know a little about the other domain, specifically the name of a domain controller in that domain. We can find that out by running the following in a console:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="n">PS&lt;/span>&lt;span class="p">&amp;gt;&lt;/span> &lt;span class="n">nltest&lt;/span> &lt;span class="p">/&lt;/span>&lt;span class="n">dclist&lt;/span>&lt;span class="err">:&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Get&lt;/span> &lt;span class="n">list&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">DCs&lt;/span> &lt;span class="k">in&lt;/span> &lt;span class="n">domain&lt;/span> &lt;span class="s1">&amp;#39;otherdomain.com&amp;#39;&lt;/span> &lt;span class="n">from&lt;/span> &lt;span class="s1">&amp;#39;\\\\DC1.otherdomain.com&amp;#39;&lt;/span>&lt;span class="p">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">PDC&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DC3&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="p">\[&lt;/span>&lt;span class="n">DS&lt;/span>&lt;span class="p">\]&lt;/span> &lt;span class="n">Site&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="n">London&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">The&lt;/span> &lt;span class="n">command&lt;/span> &lt;span class="n">completed&lt;/span> &lt;span class="n">successfully&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The results are shown above, and in this example you can see there are three domain controllers. We can pick any for the next step. Once we have the domain controller we just need to add the &lt;code>-Server&lt;/code> parameter to our original snippet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-ADUser&lt;/span> &lt;span class="n">AnotherUserName&lt;/span> &lt;span class="n">-Server&lt;/span> &lt;span class="n">DC1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">otherdomain&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">com&lt;/span> &lt;span class="n">-Properties&lt;/span> &lt;span class="n">MemberOf&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Select-Object&lt;/span> &lt;span class="n">-ExpandProperty&lt;/span> &lt;span class="n">MemberOf&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This will now return all the groups that &lt;code>AnotherUserName@otherdomain.com&lt;/code> is in.&lt;/p>
&lt;p>For full transparency, I used this tip a lot at a previous job but today when I needed it I couldn’t remember how to get the name of the AD controller. So this blog is for future Jess, because let’s be honest, I’ll be back here soon.&lt;/p></description></item><item><title>Log Shipping – Pre-stage database backups with dbatools</title><link>https://jpomfret.github.io/log-ship-staged/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/log-ship-staged/</guid><description>&lt;img src="https://jpomfret.github.io/log-ship-staged/cover.jpg" alt="Featured image of post Log Shipping – Pre-stage database backups with dbatools" />&lt;p>Log shipping is a SQL Server feature used for disaster-recovery where the transaction log backups are ‘shipped’ from your production instance to a secondary instance. This enables you to cutover to this secondary server in the event of a disaster where you lose your primary instance. Log shipping is not a new feature but is still quite popular.&lt;/p>
&lt;p>Recently I was tasked with setting up Log Shipping for a reasonably large production database. I didn’t want to initialize the secondary database with a new full backup as I was already taking full and log backups of the database. In this case we have the option of initialising the database by restoring the full &amp;amp; log backups up to the latest point in time and then configuring log shipping.&lt;/p>
&lt;h2 id="which-backups-to-restore">Which backups to restore?&lt;/h2>
&lt;p>In order for us to stage the database on the secondary at the point in time where we can configure log shipping we need to get the last full backup and any log backups taken since then.  If we were using differential backups as part of our strategy we would need the last full, the latest differential, and any log backups since then.&lt;/p>
&lt;p>This could be a lot of backup files to find, put in the right order, and then restore (with no recovery) onto the secondary server. dbatools makes this so easy! We can use &lt;code>Get-DbaDbBackupHistory&lt;/code> with the &lt;code>-Last&lt;/code> switch to get the latest backup chain. Then by piping that to &lt;code>Restore-DbaDatabase&lt;/code> we can automatically restore each piece of the puzzle. First, the full backups and then any differentials or log backups we need to get us to the point in time we are now.&lt;/p>
&lt;p>Get-DbaDbBackupHistory -SqlInstance mssql1 -Database productionDb -Last |
Restore-DbaDatabase -SqlInstance mssql2 -NoRecovery -UseDestinationDefaultDirectories&lt;/p>
&lt;p>Depending on how long the restores take you might have new log backups to apply to the secondary database, like those that have been taken on the primary since we ran the last command.  Again, we can use dbatools to help us with this. I will execute the same call to &lt;code>Get-DbaDbBackupHistory&lt;/code> to get the last backup chain, but instead of piping it straight to &lt;code>Restore-DbaDatabase&lt;/code> I will use &lt;code>Out-GridView&lt;/code> with the &lt;code>-PassThru&lt;/code> switch to effectively create a GUI window where I can select the backups I want to restore (any since the last log backup we applied), and then pass them on down the pipeline to be restored by &lt;code>Restore-DbaDatabase&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-DbaDbBackupHistory&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql1&lt;/span> &lt;span class="n">-Database&lt;/span> &lt;span class="n">productionDb&lt;/span> &lt;span class="n">-Last&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Out-GridView&lt;/span> &lt;span class="n">-PassThru&lt;/span> &lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Restore-DbaDatabase&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql2&lt;/span> &lt;span class="n">-NoRecovery&lt;/span> &lt;span class="n">-UseDestinationDefaultDirectories&lt;/span> &lt;span class="n">-Continue&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="check-on-what-we-restored">Check on what we restored&lt;/h2>
&lt;p>Once the restores are complete we can view what was restored using &lt;code>Get-DbaDbRestoreHistory&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-PowerShell" data-lang="PowerShell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">Get-DbaDbRestoreHistory&lt;/span> &lt;span class="n">-SqlInstance&lt;/span> &lt;span class="n">mssql2&lt;/span> &lt;span class="n">-Database&lt;/span> &lt;span class="n">productionDb&lt;/span> &lt;span class="n">-Last&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>At this point our secondary database has been initalised and we’re ready to set up log shipping. You can use the GUI in SSMS for this, or I’d recommend taking a look at dbatools offering &lt;code>Invoke-DbaDbLogShipping&lt;/code>.&lt;/p></description></item><item><title>Using Extended Events to determine the best batch size for deletes</title><link>https://jpomfret.github.io/using-extended-events-to-determine-the-best-batch-size-for-deletes/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-extended-events-to-determine-the-best-batch-size-for-deletes/</guid><description>&lt;p>I found myself needing to clear out a large amount of data from a table this week as part of a clean up job.  In order to avoid the transaction log catching fire from a long running, massive delete, I wrote the following T-SQL to chunk through the rows that needed to be deleted in batches. The question is though, what’s the optimal batch size?&lt;/p>
&lt;p>It’s worth noting that an index on the date column was required for this to be as efficient as possible. The table I was deleting from had 4 billion rows and I didn’t want to scan that each time!&lt;/p>
&lt;p>DECLARE @Before DATE = DATEADD(DAY,-30,GETDATE()),
@BatchSize INT = 50000&lt;/p>
&lt;p>WHILE (1=1)
BEGIN&lt;/p>
&lt;pre>&lt;code>DELETE TOP (@BatchSize) t
FROM dbo.bigTable t
WHERE Date &amp;lt; @Before
-- if we deleted less than a full batch we're done
IF @@rowcount &amp;lt; @BatchSize
BREAK;
-- add a delay between batches
WAITFOR DELAY '00:00:01'
&lt;/code>&lt;/pre>
&lt;p>END&lt;/p>
&lt;p>Time for a science experiment. The easiest way to determine the optimal batch size is to run some tests. I decided to test deleting in batches of 10k, 25k, 50k and 100k and measure the delete durations with extended events.&lt;/p>
&lt;p>My goal was two part - to delete as many rows as possible in a two-hour maintenance window, but also to reduce the amount of time locks were held on the target table.&lt;/p>
&lt;p>The following T-SQL creates an extended events session that captures one event, &lt;code>sqlserver.sql_statement_completed&lt;/code>, and filters on both my username and the statement like ‘delete top%’. I didn’t choose a target as I just chose to watch the live data, but you could easily add an event_file if you want to persist the data.&lt;/p>
&lt;p>CREATE EVENT SESSION [DeleteExperiment] ON SERVER
ADD EVENT sqlserver.sql_statement_completed(SET collect_statement=(1)
ACTION(sqlserver.nt_username)
WHERE ([sqlserver].[equal_i_sql_unicode_string]([sqlserver].[session_nt_user],N&amp;rsquo;JessUserName&amp;rsquo;)
AND [sqlserver].[like_i_sql_unicode_string]([statement],N&amp;rsquo;delete top%&amp;rsquo;)
)
)
GO&lt;/p>
&lt;p>Once the extended events session had been successfully created and was running, I opened the ‘Watch Live Data’ pane and started running my deletes in another window.  I left each experiment running for a while to make sure I got a decent sample size for each batch size.&lt;/p>
&lt;p>Once I’d cycled through the different batch sizes, I used the grouping and aggregation features in the Extended events wizard, shown on the toolbar below:&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/1tCeCwFt3DECWGZha_oV0qjLs0tYq3d3JSxccZc7Fy931ZkgvrMeZrn0665AOrR4GtFe0kBEPgrwBSNcGbx7-axO5QflWksX6BkB58AF6gvydBSV-7S0lrMfwSwIH2qBp1Jm6K7N"
loading="lazy"
>&lt;/p>
&lt;p>I grouped by ‘last_row_count’ which is my batch size, and then calculated the average duration for each group. You can see in the screenshot below the values for each.&lt;/p>
&lt;p>&lt;img src="https://lh5.googleusercontent.com/uku5O7ozzE8RSSh1ehpNdNj5EEWdizFtbC0ndXZfPMh5G5zCSEsIP8Vk3R3sdt7c3DsGf9If2__M7LdmjQ3WHSce5LDjMyhtRbmxLLPmOtMF4XVWlGovW3ZdDaTUIn3l-pwMOUXI"
loading="lazy"
>&lt;/p>
&lt;p>The duration unit is microseconds for the sp_statement_completed event so after some squinting and calculations the results are as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Batch Size&lt;/th>
&lt;th>Average Duration (Seconds)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10,000&lt;/td>
&lt;td>0.46&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25,000&lt;/td>
&lt;td>1.74&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50,000&lt;/td>
&lt;td>3.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100,000&lt;/td>
&lt;td>7.57&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For now, I’ve decided to go with batches of 50,000 for the deletes. Depending on how things go I might change this to 25,000 as in my mind both those batch sizes met my criteria.&lt;/p>
&lt;p>Hope this blog post has given you some ideas for testing out different scenarios with Extended Events.&lt;/p></description></item><item><title>Collating index usage stats across Availability Group replicas</title><link>https://jpomfret.github.io/collating-index-usage-stats-across-availability-group-replicas/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/collating-index-usage-stats-across-availability-group-replicas/</guid><description>&lt;p>One of the benefits available to us when using SQL Server Availability Groups is that we can offload read activity to a secondary replica. This can be useful if we need to run reports against our OLTP databases. Instead of this taking up valuable resources on the primary instance we can make use of the otherwise idle secondary replica.&lt;/p>
&lt;p>Note: This could affect your licensing standpoint, so ensure you’re in compliance on that front.&lt;/p>
&lt;p>Last week, I was working on a project to analyse indexes on a database that was part of an availability group. The main goal was to find unused indexes that could be removed, but I was also interested in gaining an overall understanding of how the system was indexed.&lt;/p>
&lt;p>Unused indexes not only take up disk space, but they also add overhead to write operations and require maintenance which can add additional load on your system.  We can also use this analysis to look for a high number of lookups which could indicate we need to adjust indexes slightly.&lt;/p>
&lt;p>&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: dbatools does have a command called &lt;code>Find-DbaDbUnusedIndex&lt;/code> to just look for unused indexes – however since I wanted to collect overall usage as well it wasn’t appropriate in this situation.&lt;/p>
&lt;p>dbatools has a command &lt;code>Get-DbaHelpIndex&lt;/code> which returns detailed information on our indexes which we can then use to complete the necessary analysis. To run this against a single database we could use the following code:&lt;/p>
&lt;p>Get-DbaHelpIndex -SqlInstsance mssql1 -Database AdventureWorks | Out-GridView&lt;/p>
&lt;p>In the above example I’ve used &lt;code>Out-GridView&lt;/code> to popup the results in a nice, easy to view GUI. I love using this output option to get a feel for the results. You can also filter and sort to help do some initial analysis to help get an understanding of your data.&lt;/p>
&lt;p>This is perfect – except I mentioned this database was in an AG. Oh, and it is set up to take advantage of using that read-only replica to run reporting against. That means the whole picture of the index usage is spread across two instances. We might find a totally unused index on our primary replica, a great candidate to be dropped, unless it’s heavily used by reports on the secondary.&lt;/p>
&lt;p>Remember, the secondary replica is just a read-only copy – so the indexes needed on the secondary must be created on the primary.&lt;/p>
&lt;p>In this situation we need to combine the index stats for both replicas into one easy to use result set – for this we can make use of PowerShell’s PSCustomObject to join these two result sets. In the code below I’ve set up a few variables at the top, and then run &lt;code>Get-DbaHelpIndex&lt;/code> against both instances. We then set up a variable to catch the results in &lt;code>$export&lt;/code> and use foreach-object to loop through the results for the primary instance. As we loop through, we’re looking for the matching index on the secondary replica before adding properties from both sides to the PSCustomObject.&lt;/p>
&lt;p>Finally, we lean on the ImportExcel module to export the results to an excel spreadsheet – if you haven’t checked this module out yet I highly recommend it.&lt;/p>
&lt;p>&lt;a class="link" href="https://gist.github.com/jpomfret/a9afa22c4d1129fecc4ea3e6cde1b51c" target="_blank" rel="noopener"
>https://gist.github.com/jpomfret/a9afa22c4d1129fecc4ea3e6cde1b51c&lt;/a>&lt;/p>
&lt;p>Looking at our results spreadsheet we can now easily review the index usage across both replicas and make sure that any indexes we identify as unused, truly are unused.&lt;/p></description></item><item><title>T-SQL Tuesday #143: Short code examples</title><link>https://jpomfret.github.io/t-sql-tuesday-#143-short-code-examples/</link><pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#143-short-code-examples/</guid><description>&lt;p>&lt;a class="link" href="https://johnmccormack.it/2021/10/t-sql-tuesday-143-short-code-examples/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Well folks, it’s Wednesday here in the UK, which means I’m a day late to get my blog post in for T-SQL Tuesday. However, if I was in Hawaii it would be still Tuesday so let&amp;rsquo;s go for it&amp;hellip;&lt;/p>
&lt;p>I used a handy short script this morning and I figured it was worth a quick, late entry! Hopefully John Mccormack (&lt;a class="link" href="https://johnmccormack.it/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/actualjohn" target="_blank" rel="noopener"
>t&lt;/a>), will forgive me for stretching the deadline!&lt;/p>
&lt;p>First of all, shout out to John for hosting the monthly blog party, he has got a great prompt and I’m really excited to see the wrap-up post as I’m sure it’ll be full of great little code snippets.&lt;/p>
&lt;blockquote>
&lt;p>T-SQL Tuesday this month is going back to basics and its all about code. I’d like to know &lt;strong>“What are your go to handy short scripts”?&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>This morning I was working on pulling together some information which included whether certain accounts were in the local administrator’s group on some remote servers. I had the perfect snippet saved in my code repo so I was quickly able to answer that question – and then I realised I should share that with you all.&lt;/p>
&lt;p>The following PowerShell snippet uses the &lt;code>net localgroup&lt;/code> command line tool to retrieve the results and parse them so we just get the account names.  The final line includes the &lt;code>-ComputerName&lt;/code> parameter so you can easily run it against remote machines.&lt;/p>
&lt;p>Invoke-Command -ScriptBlock { net localgroup administrators |
Where-Object { $_ -AND $_ -notmatch &amp;ldquo;command completed successfully&amp;rdquo; } |
Select -skip 4
} -ComputerName mssql1&lt;/p>
&lt;p>Hope this comes in handy, and sorry again John for sneaking in late.&lt;/p></description></item><item><title>T-SQL Tuesday #141: Work/Life balance</title><link>https://jpomfret.github.io/t-sql-tuesday-#141-work/life-balance/</link><pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#141-work/life-balance/</guid><description>&lt;p>&lt;a class="link" href="https://tjaybelt.blogspot.com/2021/08/t-sql-tuesday-141-worklife-balance.html" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>It’s T-SQL Tuesday time and I’ve finally managed to put some words into a blog post again.  It’s been a few months since I’ve posted anything on my blog, things have been busy, and my motivation has been a little lacking.  However, this topic has inspired me to try and pull something together. Thanks to TJay(&lt;a class="link" href="https://tjaybelt.blogspot.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/tjaybelt" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month and for choosing a great topic.&lt;/p>
&lt;p>TJay has asked us to share our personal journey with managing work/life balance.  It’s an interesting prompt, especially at this point in a global pandemic.  The last 18 months have caused a massive shift in my work/life balance, and at this point I’d have to say overall it’s been a positive change.&lt;/p>
&lt;p>Like many of us, in March 2020 I was told to work from home full time for who knows how long.  Previously I had been working from home one day a week and commuting into the office the other four.  My commute wasn’t particularly long, but it did take about 45 mins each way for me to walk to the station and take the train into Southampton where my office is located.  Gaining that commute time back has given me a lot more flexibility as far as my working hours – which is in turn where I’ve been able to improve my work/life balance.&lt;/p>
&lt;p>I’m lucky I work at a company, and on a team, that has transitioned smoothly into the work from home lifestyle.  The ability to work more flexibly is the biggest benefit that I see when working from home and it does allow for a more manageable work/life balance in my opinion.&lt;/p>
&lt;p>I’ve talked about my love of CrossFit before, and it’s something I missed a lot while gyms were closed during the lockdowns over the last year.  Now that gyms are back open again I’ve got back into the habit of going 5-6 days per week.  Pre-working from home I would go to the gym in the evenings, checking in for either the 5pm or 530pm classes. Afterwards there was time for dinner, a little TV and not much else in the evenings.&lt;/p>
&lt;p>Now that I work from home, I’ve been able to change my schedule to go to the lunch time class. It’s created a great break both mentally and physically in the middle of my workdays, allowing me to get away from my desk and spend an hour at the gym with my wife and friends. It really breaks up the day and since I’m not commuting, I don’t end my day any later than I used to.&lt;/p>
&lt;p>I will say that it was a struggle at first to end my workdays on time. I found myself slipping into working later than I would if I had to leave the office to commute home. This was especially hard when we were in lockdown and there was nothing really to do. Luckily my wife tended to keep me on track here. We’d go out and walk together after work which meant I couldn’t just sit at my desk all evening. That helped a lot, and I feel like now I’ve got into a pretty good routine of ending my days at a reasonable time.&lt;/p>
&lt;p>Although 2020/2021 has been a hard year I think overall at this point the changes have had a positive impact on my work/life balance. I’ll leave you with my keys to success:&lt;/p>
&lt;ul>
&lt;li>A little fitness – I always feel better when I exercise, even if it’s just a walk and some fresh air so I ensure I make time for this.&lt;/li>
&lt;li>A little support – I’m lucky that my wife always keeps me on track, but having someone in your corner be it friends or family, can make a huge difference.&lt;/li>
&lt;/ul>
&lt;p>Thanks again to TJay for hosting, and I look forward to reading everyone else&amp;rsquo;s responses.&lt;/p></description></item><item><title>Searching Stored Procedures for a pattern made easy with dbatools</title><link>https://jpomfret.github.io/searching-stored-procedures-for-a-pattern-made-easy-with-dbatools/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/searching-stored-procedures-for-a-pattern-made-easy-with-dbatools/</guid><description>&lt;p>Well folks, after starting the year off on a strong foot it’s been a while since I’ve published any blog posts. Hope you didn’t miss me too much, but I’m back now and I’ve got a useful dbatools snippet for you today.  Last week at the day job I had a situation where I needed to find all stored procedures that referenced a certain string, across any database on a specific server.  This is a pretty trivial task in SSMS when you’re just talking about one database. For example, if we’re looking for any reference to ‘Person’ perhaps we could run this T-SQL within the context of the database:&lt;/p>
&lt;p>select o.name, sc.text, o.type
from sys.objects o
inner join sys.syscomments sc
on sc.id = o.object_id
where text like &amp;lsquo;%Person%&amp;rsquo;
and o.type = &amp;lsquo;P&amp;rsquo; &amp;ndash; filtered for just stored procedures&lt;/p>
&lt;p>You can see I’ve found one procedure in my TestDb that references the ‘Person’ table, so it has been returned.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/TSQL.jpg"
loading="lazy"
alt="T-SQL code to find procedures with &amp;lsquo;person&amp;rsquo; in"
>&lt;/p>
&lt;p>However, if I want to search all databases on the server I now need to start thinking about a cursor, or using something like &lt;code>sp_MSforeachdb&lt;/code> to iterate over the databases.  A quick warning here- &lt;code>sp_MSforeachdb&lt;/code> is an undocumented procedure and there are some known issues with it.&lt;/p>
&lt;p>The natural next step here when we’re thinking about handling multiple databases is to switch to PowerShell and use dbatools.&lt;/p>
&lt;h2 id="find-the-dbatools-command-for-the-job">Find the dbatools command for the job&lt;/h2>
&lt;p>When we’re looking for the command we need within dbatools to fulfil our needs I cannot recommend &lt;code>Find-DbaCommand&lt;/code> highly enough.  This command will search all other commands for the pattern you pass in.  Today we know we want to find references in stored procedures so let’s see if there is a command that will help.&lt;/p>
&lt;p>Find-DbaCommand *stored*proc*&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/findCommand-1024x368.jpg"
loading="lazy"
alt="Find-DbaCommand helping us to find what we need"
>&lt;/p>
&lt;p>Looks like &lt;code>Get-DbaDbStoredProcedure&lt;/code> is what we need here.  Since this is our first time using this particular command I always have a quick look through the help content. I highly recommend running &lt;code>Get-Help Get-DbaDbStoredProcedure -ShowWindow&lt;/code>, this will open a separate window from your console and allow you to keep that open to refer back to if needed.  The last section of the help gives us several examples on how to use this command- let’s run a simple one against a test database to see what we get. I’m also going to pipe the output to &lt;code>Select-Object&lt;/code> so I can just sample the first 2 results.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Select-Object -First 2&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/twoProcs-1024x539.jpg"
loading="lazy"
alt="Get-DbaDbStoredProcedure results"
>&lt;/p>
&lt;p>This is handy, but this output doesn’t look like it’s going to help answer the question and find references of a string within the stored procedure code. There’s more than meets the eyes though.&lt;/p>
&lt;h2 id="sql-server-management-objects-smo">SQL Server Management Objects (SMO)&lt;/h2>
&lt;p>dbatools deals mostly with SQL Server Management Objects (SMO), which means that what you see in the output for commands is not always all there is available.  SMO is a hierarchy of objects which can be easily traversed from the output of the commands.  You can tell that we’re dealing with SMO instead of standard PowerShell objects by using &lt;code>Get-Member&lt;/code> and looking at the TypeName.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Get-Member&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/getMember.jpg"
loading="lazy"
alt="using Get-Member to see what&amp;rsquo;s available"
>&lt;/p>
&lt;p>&lt;code>Get-Member&lt;/code> is also really useful for looking to see what’s available from the object that is returned. In the screenshot above you can see multiple methods that can be used. If you run this in your console you’ll also get a list of all available properties.  That’s a hint for what we need for our scenario.&lt;/p>
&lt;h2 id="find-our-string-within-all-stored-procedures-in-any-database">Find our string within all Stored Procedures in any database&lt;/h2>
&lt;p>Now that we know the &lt;code>Get-DbaDbStoredProcedure&lt;/code> command is going to return SMO StoredProcedure objects we can look at some of the properties not returned by default.  We already saw one option for this- using &lt;code>Get-Member&lt;/code> will list all the properties available to us.  Another option is to select all the properties for the first result.&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -Database testDb | Select-Object -First 1 *&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/SelectAll-1024x315.png"
loading="lazy"
alt="Select all the properties from Get-DbaDbStoredProcedure"
>&lt;/p>
&lt;p>In the output you can see there are a lot of properties available that weren’t returned by default, this includes &lt;code>TextBody&lt;/code> which is what we need to search for our reference string.  All we need to do now is pipe the output of the command through to &lt;code>Where-Object&lt;/code> to find what we need:&lt;/p>
&lt;p>Get-DbaDbStoredProcedure -SqlInstance &amp;rsquo;localhost,2500&amp;rsquo; -ExcludeSystemSp | Where-Object TextBody -like &amp;lsquo;*Person*&amp;rsquo;&lt;/p>
&lt;p>You’ll notice two more changes to the code above. I dropped the &lt;code>Database&lt;/code> parameter, opening the search up to the whole server. I also added &lt;code>ExcludeSystemSp&lt;/code>, which means I’m only interested in user defined stored procedures. It is important to note if you have a lot of stored procedures this command could take a little while to run.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/SearchAllSPs-1024x237.jpg"
loading="lazy"
alt="search the TextBody for key words"
>&lt;/p>
&lt;p>PowerShell also supports other &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_comparison_operators?view=powershell-7.1" target="_blank" rel="noopener"
>comparison operators&lt;/a> by default, including `match` which can be used to find regex patterns within your procedures.  This opens up a lot more possibilities when looking for more complicated patterns within your database.&lt;/p>
&lt;h2 id="so-many-more-options">So many more options…&lt;/h2>
&lt;p>Today we were only looking for results on one SQL Server instance, but since dbatools makes handling multiple SQL instances easy we could also widen the search and instead search our entire estate for references to a certain string.&lt;/p>
&lt;p>We also only looked at Stored Procedures today, but if you do a little research with &lt;code>Find-DbaCommand&lt;/code>, &lt;code>Get-Help&lt;/code> and &lt;code>Get-Member&lt;/code> you’ll soon find what you need to search through functions, views and more.&lt;/p>
&lt;p>Happy searching!&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 3</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-3/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-3/</guid><description>&lt;p>This is now the third post in a series on Azure tags. You can read the other two posts here to get up to speed with where we’ve been, however that isn’t required for this post .&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://jesspomfret.com/azure-tags-part1/" target="_blank" rel="noopener"
>Keeping track of Azure resources with tags – Part 1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://jesspomfret.com/azure-tags-part2/" target="_blank" rel="noopener"
>Keeping track of Azure resources with tags – Part 2&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In part one I discussed how useful Azure tags can be, and specifically about how adding a ‘dateCreated’ tag can help you keep track of your resources, and how to find resources with certain tags using PowerShell.  Part 2 and 3 are based around the fact that adding the ‘dateCreated’ tag is a great idea, but relying on a human to remember to add it is less than ideal. In part 2 we looked at using Azure Policy to automatically add the tag. Today’s post will cover another option using Azure Functions.&lt;/p>
&lt;p>Azure Functions gives us a way of running serverless code, written in a number of different languages, triggered by specific events or timings.  Looking through the &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview" target="_blank" rel="noopener"
>documentation&lt;/a> there are many use cases from processing files to analysing IoT workstreams.  Our use case is to run a PowerShell script that tags any resources that are missing the ‘dateCreated’.&lt;/p>
&lt;h2 id="step-1---create-a-function">Step 1 - Create a function&lt;/h2>
&lt;p>Azure Functions live within a function app, so the first thing we have to do is create this logical container. At this level we’ll decide on a ‘Function App name’, I’ve called mine ‘resourceTagJp’, and choosing ‘Code’ for the publish option we can then choose PowerShell as our language of choice. There are some other options for selecting a storage account and configuring ‘Application Insights’, but for now I’ve left those all as the defaults.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/functionApp.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/functionApp.png"
loading="lazy"
alt="create a function app pane"
>&lt;/a>&lt;/p>
&lt;p>Once the ‘Function App’ is created we are ready to create our function.  On the left hand pane choose ‘Functions’ and then ‘Add’. This will open a pane for you to choose how to develop the function, either in the portal or on your local machine in VSCode, for example, and the template to base your function off of.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/CreateFunction-1.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/CreateFunction-1.png"
loading="lazy"
alt="Create an Azure Function"
>&lt;/a>&lt;/p>
&lt;p>One of the simplest options is to choose ‘Timer Trigger’, which as expected will execute the function code based on a schedule. The schedule is set using a cron expression. For it to run once an hour at the top of the hour we’ll use the following:&lt;/p>
&lt;p>0 0 * * * *&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/cronSchedule.png"
loading="lazy"
alt="cron schedule popup"
>&lt;/p>
&lt;p>Once the function is created we’ll choose ‘Code + Test’ on the left hand pane of the portal to actually add the function code. The code for my function is going to be pretty simple, but if you are writing more complicated functions there is a &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions" target="_blank" rel="noopener"
>VSCode extension&lt;/a> that can be used to develop and test functions locally before publishing them to Azure.&lt;/p>
&lt;p>We have three files within our function:&lt;/p>
&lt;ul>
&lt;li>Readme.md – for documentation in markdown&lt;/li>
&lt;li>Function.json – the config file, currently contains our timer binding information&lt;/li>
&lt;li>Run.ps1 – the main function code, in PowerShell as that’s what we chose&lt;/li>
&lt;/ul>
&lt;p>The code for the function is below and makes use of the &lt;code>Update-AzTag&lt;/code> cmdlet to add the ‘dateCreated’ tag. In this example, since I’m using PowerShell, I can easily format the date to be exactly how I want it to be displayed. If you read part 2 in this series, that was a downfall of using Azure Policy, I only had one datetime format option. The &lt;code>Update-AzTag&lt;/code> also has a &lt;code>-Merge&lt;/code> parameter which ensures any tags already on the resource aren’t overwritten by this function.&lt;/p>
&lt;p># Input bindings are passed in via param block.
param($Timer)&lt;/p>
&lt;h1 id="select-the-subscription">Select the subscription&lt;/h1>
&lt;p>$null = Select-AzSubscription -SubscriptionId (Get-AzSubscription -SubscriptionName &amp;lsquo;MSDN Platforms&amp;rsquo;)&lt;/p>
&lt;h1 id="tag-any-resources-that-are-missing-tags">tag any resources that are missing tags&lt;/h1>
&lt;p>$res = Get-AzResource -ResourceGroupName functionTest | Where { $_.tags.keys -notcontains &amp;lsquo;dateCreated&amp;rsquo; }
$res.Foreach{
Update-AzTag -ResourceId $psitem.ResourceId -Tag @{&amp;lsquo;dateCreated&amp;rsquo; = (Get-Date -Format &amp;ldquo;yyyy-MM-dd&amp;rdquo;)} -Operation Merge
}&lt;/p>
&lt;p>That’s all it is to create our function – however, it doesn’t currently have the authorisation to view or update resources.&lt;/p>
&lt;h2 id="step-2---add-a-managed-identity">Step 2 - Add a managed identity&lt;/h2>
&lt;p>To provision access to allow our function to work we can make use of &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/app-service/overview-managed-identity?tabs=dotnet" target="_blank" rel="noopener"
>Azure Managed Identities&lt;/a>. These are similar to Managed Service Accounts in that there is no need to set or rotate passwords. This means we can configure our function with a managed identity and then forget about it- we know it’ll remain secure and the password/secret will be rotated often.&lt;/p>
&lt;p>There are two options for managed identities: system-assigned or user-assigned. The system-assigned identity is tied directly to your application – if we delete the function the identity will also be deleted, and that’s fine for this purpose.&lt;/p>
&lt;p>Configuring the system-assigned identity is pretty straightforward. On our function pane under identity, change the status to ‘On’.  After a couple of minutes the identity will be deployed and you’ll see the option to assign ‘Azure role assignments’.&lt;/p>
&lt;p>Clicking on ‘Azure role assignments’ will open a pane where you can assign whatever permissions your function will need to run. This can be scoped at the subscription or resource group level. For this example my function is just tagging anything within the ‘functionTest’ resource group so I can set the permissions to that scope.  I have chosen the ‘contributor’ role as that gives us enough permissions to view and tag resources.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/ManagedIdentity.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/ManagedIdentity.png"
loading="lazy"
alt="Setting up a system assigned managed identity for our function"
>&lt;/a>&lt;/p>
&lt;h2 id="step-3---test-our-function">Step 3 - Test our function&lt;/h2>
&lt;p>We have created our function and set up the managed identity to enable the function to access our resources, now it’s time to make sure it’s working as expected. From the ‘Code + Test’ page there is a ‘Test/Run’ at the top that brings out the pane on the right. In that pane pressing run will simulate the time trigger being met and our function executing.&lt;/p>
&lt;p>In the console you can see exactly what the function does and any output you’ve configured – in this example you can see a storage account was tagged.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/TestFunction-1.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/TestFunction-1.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>We can also test this function by creating an untagged resource in the ‘functionTest’ resource group and waiting for the timer to be triggered.&lt;/p>
&lt;p>However we test our function we can see the tag is now on our storage account and we no longer have to rely on a human to remember the tag when they create resources.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/storageAccountFunctionTagged.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/storageAccountFunctionTagged.png"
loading="lazy"
alt="Showing a storage account has been tagged with our specified date format"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This has been as much a learning experience for me as it hopefully has been for you. My journey into Azure is still pretty new but I’m enjoying the adventure.&lt;/p>
&lt;p>To wrap this up, having a tagging strategy is important and there are multiple ways to ensure that tagging strategy is followed. Both Azure Policy and Azure Functions give us a good option for automatically tagging resources that are missing tags. If you’re using Terraform to deploy Azure resources &lt;a class="link" href="https://jqmartin.info/2021/03/02/terraform-timestamps-and-tagging/" target="_blank" rel="noopener"
>John Martin has written about adding the a tag for date created to all resources as they are deployed&lt;/a>, which is definitely worth a read.&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 2</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-2/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-2/</guid><description>&lt;p>Last week, in &lt;a class="link" href="https://jesspomfret.com/azure-tags-part1/" target="_blank" rel="noopener"
>Part 1&lt;/a>, we talked about how to easily keep track of our resources with tags. There are many strategies for tagging your resources but I specifically focused on adding a ‘dateCreated’ tag so we could see when resources were created – since this isn’t available by default.  During that post we identified the biggest issue we had was that we were relying on a human to remember to add the ‘dateCreated’ tag for every resource they created. I’ve got two ideas on how to fix that – today we’ll look at the first option, using &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/governance/policy/overview" target="_blank" rel="noopener"
>Azure Policy&lt;/a>.&lt;/p>
&lt;p>Azure Policy is a way of comparing your Azure estate to defined requirements. You can either use predefined definitions (of which there are many) or create your own specific rules.  These definitions can be assigned to certain scopes (subscriptions, resource groups). Azure Policy then reports on whether you’re in the expected state and in some cases can alter resources to ensure you are.&lt;/p>
&lt;h2 id="step-1--define-a-policy">&lt;strong>Step 1 – Define a policy&lt;/strong>&lt;/h2>
&lt;p>In our example, all resources should have a ‘dateCreated’ tag, and if Azure Policy finds the tag is missing, it should add that tag with the current date.&lt;/p>
&lt;p>There are a few steps to set up our policy for ensuring the ‘dateCreated’ tag exists on all resources. First we need to write a policy definition in JSON.  Don’t panic yet though – there are a lot of examples in the &lt;a class="link" href="https://github.com/Azure/azure-policy" target="_blank" rel="noopener"
>Azure/azure-policy&lt;/a> GitHub repo that we can start with.  By browsing the repo you can find the ‘&lt;a class="link" href="https://github.com/Azure/azure-policy/tree/master/samples/Tags/add-tag" target="_blank" rel="noopener"
>add-tag&lt;/a>’ policy which is the perfect base for us to build off of.  &lt;/p>
&lt;p>When viewing that GitHub page there is a ‘Deploy to Azure’ button- clicking that (presuming you’re logged into the portal) will take you straight to the ‘New Policy Definition’ wizard where we can modify our policy to meet our needs and save it. &lt;/p>
&lt;p>You’ll need to choose a ‘Definition Location’ (which is the subscription this policy should reside in), name your policy, and edit the description if needed. The GitHub template has already specified this will go in the ‘Tags’ category,but you can change that if you’re keeping custom policies in a new category. Then we get to the policy rule, it’s JSON time.&lt;/p>
&lt;p>Since we imported the sample from GitHub the JSON is almost exactly what we need. The first section defines the rules and the second section defines parameters.  In this case we’re going to remove the parameter section and change the tag name and values expected to be static. This means that the policy will only ever be used for adding the specific ‘dateCreated’ tag.  The reason for this is we’re going to add some logic to the tag value so it contains the current date. (It is possible that this can be achieved with parameters, but I couldn’t get it to work. Please let me know in the comments if you know differently). &lt;/p>
&lt;p>The JSON below is pretty simple. There are two main sections: the condition to be met and then the operation to carry out if the conditions are met.  In the conditions section we’re looking to see if the dateCreated tag exists, if it doesn’t we’ll move onto the second section of the JSON. This defines what to do about it, and in this case it’s pretty simple, we’ll modify the target and add the dateCreated tag.  The tag value is dynamic and uses a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions" target="_blank" rel="noopener"
>resource manager template function&lt;/a> to get the current date.  There are a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/governance/policy/concepts/definition-structure#policy-functions" target="_blank" rel="noopener"
>few restrictions&lt;/a> on using functions within policy definitions, one being that we can’t overload the uctNow function with a format parameter so we’ll only be able to get a date in ISO 8601 (yyyyMMddTHHmmssZ) format.&lt;/p>
&lt;p>{
&amp;ldquo;mode&amp;rdquo;: &amp;ldquo;Indexed&amp;rdquo;,
&amp;ldquo;policyRule&amp;rdquo;: {
&amp;ldquo;if&amp;rdquo;: {
&amp;ldquo;field&amp;rdquo;: &amp;ldquo;tags[&amp;lsquo;dateCreated&amp;rsquo;]&amp;rdquo;,
&amp;ldquo;exists&amp;rdquo;: &amp;ldquo;false&amp;rdquo;
},
&amp;ldquo;then&amp;rdquo;: {
&amp;ldquo;effect&amp;rdquo;: &amp;ldquo;modify&amp;rdquo;,
&amp;ldquo;details&amp;rdquo;: {
&amp;ldquo;roleDefinitionIds&amp;rdquo;: [
&amp;ldquo;/providers/microsoft.authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c&amp;rdquo;
],
&amp;ldquo;operations&amp;rdquo;: [
{
&amp;ldquo;operation&amp;rdquo;: &amp;ldquo;add&amp;rdquo;,
&amp;ldquo;field&amp;rdquo;: &amp;ldquo;tags[&amp;lsquo;dateCreated&amp;rsquo;]&amp;rdquo;,
&amp;ldquo;value&amp;rdquo;: &amp;ldquo;[utcNow()]&amp;rdquo;
}
]
}
}
}
}&lt;/p>
&lt;p>The final decision to make for our new policy definition is the ‘Role definition’. Since our policy has remediation actions, the operation to add tags if needed, a &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview" target="_blank" rel="noopener"
>managed identity&lt;/a> will be created for the policy.  The ‘role definition’ is the permissions that will be granted to that managed identity.  The default is ‘Contributor’ which assigns ‘full access to manage all resources’. This can be changed based on what your policy needs access to.&lt;/p>
&lt;h2 id="step-2--assign-the-policy">&lt;strong>Step 2 – Assign the policy&lt;/strong>&lt;/h2>
&lt;p>We’ve now defined our policy, but the second part of this process is to assign that policy a scope. This outlines what this policy applies to. The scope can be a subscription or resource group, and can be more finely tuned by excluding specific resources that you don’t want to apply the policy too.  Then it’s as easy as selecting the policy we defined, setting an ‘Assignment name’, which could be a combination of policy name and assigned score, and adding an optional description.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/assignPolicy.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/assignPolicy.png"
loading="lazy"
alt="Assign policy screen in Azure"
>&lt;/a>&lt;/p>
&lt;h2 id="step-3--test-the-policy">&lt;strong>Step 3 – Test the policy&lt;/strong>&lt;/h2>
&lt;p>The easiest way to test our policy is to create a resource without the ‘dateCreated’ tag and see what happens. We have scoped our policy assignment to the ‘policyTest’ subscription so I’ll run the following PowerShell to create a new storage account that’s missing my required tag.&lt;/p>
&lt;p>New-AzStorageAccount -ResourceGroupName policyTest -AccountName mystorageaccountjp77 -Location uksouth -SkuName Standard_GRS&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/createStorageAccount.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/createStorageAccount-1024x124.png"
loading="lazy"
alt="create storage account with New-AzStorageAccount"
>&lt;/a>&lt;/p>
&lt;p>You can see there is no &lt;code>-Tags&lt;/code> parameter specified, so this storage account was created without any tags. If we now run &lt;code>Get-AzStorageAccount&lt;/code> we can see it has the &amp;lsquo;dateCreated&amp;rsquo; tag.&lt;/p>
&lt;p>Get-AzStorageAccount -ResourceGroupName policyTest | Select-Object StorageAccountName, Tags&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/storageAccountTagged.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/storageAccountTagged-1024x129.png"
loading="lazy"
alt="Get-AzStorageAccount results show the tags"
>&lt;/a>&lt;/p>
&lt;p>You can also see the tag in the portal view of our storage account.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/storageAccountTaggedPortal.png"
loading="lazy"
>&lt;/p>
&lt;p>Finally, if we check out the Azure Policy we can see that we are in compliance. All resources in the ‘policyTest’ subscription have the required ‘dateCreated’ tag.  We can also see the specific resources that are in compliance, in our case just the storage account we created.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/03/policyCompliance.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/policyCompliance-1024x390.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>This is just one option for automatically assigning the ‘dateCreated’ tag on all new resources. In this case we scoped the policy to a specific resource group but have assigned it at the subscription level to cover all resources.  Note – this is specifically to tag resources. If you want to also tag resource groups the policy definition will need to be altered slightly.&lt;/p>
&lt;p>One downside of this method is I haven’t found a way to control the format of the date in the tag value. This isn’t a big concern but does lack a little flexibility, especially if we wanted to add the date in a different time zone.&lt;/p>
&lt;p>Next week we’ll look at adding the same functionality using Azure Functions to auto tag new resources.&lt;/p></description></item><item><title>Keeping track of Azure resources with tags – Part 1</title><link>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-1/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-track-of-azure-resources-with-tags-part-1/</guid><description>&lt;p>I’ve been working on some Azure exams recently, and I personally learn best by fiddling with things.  The &lt;a class="link" href="https://docs.microsoft.com/en-us/learn/" target="_blank" rel="noopener"
>Microsoft learn&lt;/a> content is excellent, and I’d highly recommend that for any of the Azure exams I’ve taken so far.  However, I also like to build things myself and experiment a little with all the available options.&lt;/p>
&lt;p>One of the vital parts of this learning and experimenting needs to be cleaning up after myself.  We all know the risks of leaving things running in Azure- it’s likely to drain your training budget pretty quickly.  To be fair, this is also a good lesson for real world scenarios. Getting used to turning off or scaling down resources based on need is a good way to reduce your Azure spend.&lt;/p>
&lt;p>This brings me to one morning last week. I logged in to the portal and got a pop up that my credit was down to under $5, which is not what I was expecting. I started looking around and wondering what I’d left running – it isn’t always easy to spot though.&lt;/p>
&lt;p>Luckily, John Martin (&lt;a class="link" href="https://jqmartin.info/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/jqmtweets" target="_blank" rel="noopener"
>t&lt;/a>) has instilled in me the importance of adding a tag for creation date on all resources, as it’s not tracked automatically. This means we can easily see what we last deployed and what we might have forgotten about.&lt;/p>
&lt;p>In Azure, tags are just key value pairs that can be applied to resources and subscriptions to add metadata. You can use them to organise resources by environment, cost centre, business criticality, and anything else that might be important to your individual situation. There is a limit of 50 tags per resource. If you’re getting close to having 50 tags per resource you might need to rethink your tagging strategy to reduce the complexity.&lt;/p>
&lt;p>You can view tags through the portal either through the dedicated ‘Tags’ pane, on each individual resource, or on the ‘Cost Management’ area. Here you can view your Azure spend broken down by tags, which can be very useful. You can also view tags using either the Azure CLI or PowerShell. I usually opt for PowerShell, so let’s have a look at how we can view resources with certain tags using the &lt;a class="link" href="https://www.powershellgallery.com/packages/Az/" target="_blank" rel="noopener"
>Az module&lt;/a>.  If you don’t already have the module installed you can run &lt;code>Install-Module az&lt;/code> to get started. More details on prerequisites and options available can be found in the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-5.6.0" target="_blank" rel="noopener"
>Install Azure PowerShell with PowerShellGet&lt;/a> docs.&lt;/p>
&lt;h2 id="find-resources-with-a-certain-tag">&lt;strong>Find resources with a certain tag&lt;/strong>&lt;/h2>
&lt;p>I already mentioned I add a ‘dateCreated’ tag to all resources, but when I’m playing around in Azure and working through training courses I also add a ‘training’ tag and set the value to ‘true’.  This is an easy way for me to find all the resources I’ve created while training and clean them up.&lt;/p>
&lt;p>We can easily list these resources in PowerShell using the following one liner:&lt;/p>
&lt;p>Get-AzResource -TagName training -TagValue &amp;rsquo;true&amp;rsquo; |
Select-Object Name, ResourceGroupName,ResourceType, Tags&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/trainingTrue-1024x198.jpg"
loading="lazy"
alt="results of Get-AzResource"
>&lt;/p>
&lt;p>If we’re ready to clean them up we can pipe the results from &lt;code>Get-AzResource&lt;/code> straight into &lt;code>Remove-AzResource&lt;/code>, ensuring we haven’t got anything left running and costing us credit.&lt;/p>
&lt;p>Get-AzResource -TagName training -TagValue &amp;rsquo;true&amp;rsquo; | Remove-AzResource&lt;/p>
&lt;h2 id="resources-created-in-the-last-x-days">&lt;strong>Resources created in the last x days&lt;/strong>&lt;/h2>
&lt;p>Another useful snippet since we’re adding ‘dateCreated’ tags to all our resources is to get any resources that have been created in the last few days. Since I have formatted my dates as yyyy-MM-dd in my tags I can easily convert them into dates with PowerShell and then filter based on them.&lt;/p>
&lt;p>Get-AzResource -TagName dateCreated |
Select-Object Name, ResourceType, @{l=&amp;lsquo;dateCreated&amp;rsquo;;e={get-date($_.Tags[&amp;lsquo;dateCreated&amp;rsquo;])}} |
Where-Object dateCreated -gt (get-date).AddDays(-7)&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/resourcesCreatedinLastXDays-1024x327.jpg"
loading="lazy"
alt="List of resources created in last 7 days in PowerShell"
>&lt;/p>
&lt;h2 id="resources-without-a-tag">&lt;strong>Resources without a tag&lt;/strong>&lt;/h2>
&lt;p>The final tag related snippets I have for today is to make sure all our resources have the &amp;lsquo;dateCreated&amp;rsquo; tag. Since currently I’m manually adding these tags there is a chance I forget or get lazy and some resources make it through without the tags.&lt;/p>
&lt;p>We can find these tags by interrogating the key values of the tags:&lt;/p>
&lt;p>Get-AzResource | where {$_.tags.Keys -notcontains &amp;lsquo;dateCreated&amp;rsquo;} |
Select-Object Name, ResourceType, Tags&lt;/p>
&lt;p>Once we know which resources are missing tags we can easily update them using &lt;code>Update-AzTag&lt;/code>, the operation parameter that controls what should happen if there are existing tags. Merge will ensure we don’t overwrite the current tags.&lt;/p>
&lt;p>$resources = Get-AzResource -ResourceGroupName missingtag |
Where-Object {$_.tags.Keys -notcontains &amp;lsquo;dateCreated&amp;rsquo;}&lt;/p>
&lt;p>Update-AzTag -ResourceId $resources.ResourceId -Tag @{&amp;lsquo;dateCreated&amp;rsquo; = (Get-Date -Format &amp;ldquo;yyyy-MM-dd&amp;rdquo;)} -Operation Merge&lt;/p>
&lt;p>The main problem with this whole idea is we are relying on whoever creates the resources to both remember to create the tag and put the values of the tag in a standard format.  Next week we’ll look at a couple of ways to automate this process.&lt;/p></description></item><item><title>T-SQL Tuesday #136: Blog About Your Favorite Data Type (Or Least Favorite)</title><link>https://jpomfret.github.io/t-sql-tuesday-#136-blog-about-your-favorite-data-type-or-least-favorite/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#136-blog-about-your-favorite-data-type-or-least-favorite/</guid><description>&lt;p>&lt;a class="link" href="https://www.brentozar.com/archive/2021/03/tsql2sday-136-invitation-blog-about-your-favorite-data-type-or-least-favorite/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Shout out to Brent Ozar (&lt;a class="link" href="http://brentozar.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/BrentO" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month&amp;rsquo;s TSQL2sday. It’s time again for this monthly blog party and he wants to know all about our favourite or least favourite data types. To start with I was having a hard time thinking of a favourite data type. I know I have favourite words (merge and plethora, in case you’re wondering), but it seems a bit wrong to pick favourites here – I mean lots of them are great in their own right. Then it came to me- my favourite data type is the right one for the job at hand. Feels like I’m skirting the question a little here, but bear with me.&lt;/p>
&lt;p>Let’s talk about accuracy and precision, and how much of it you actually need.  The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).&lt;/p>
&lt;p>First things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:&lt;/p>
&lt;p>Let’s talk about accuracy and precision, and how much of it you actually need.  The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).&lt;/p>
&lt;p>First things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Datatype&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;th>Storage Size&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>One day&lt;/td>
&lt;td>3 bytes&lt;/td>
&lt;td>Doesn&amp;rsquo;t include time&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SmallDateTime&lt;/td>
&lt;td>One minute&lt;/td>
&lt;td>4 bytes&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTime&lt;/td>
&lt;td>Rounded to increments of .000, .003, or .007 seconds&lt;/td>
&lt;td>8 bytes&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTime2&lt;/td>
&lt;td>100 nanoseconds&lt;/td>
&lt;td>6 bytes for precision less than 3.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7 bytes for precision 3 or 4.&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All other precision require 8 bytes.&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Time&lt;/td>
&lt;td>100 nanoseconds (1 millisecond in Informatica)&lt;/td>
&lt;td>5 bytes&lt;/td>
&lt;td>Doesn&amp;rsquo;t include date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DateTimeOffset&lt;/td>
&lt;td>100 nanoseconds&lt;/td>
&lt;td>10 bytes&lt;/td>
&lt;td>Includes time zone awareness&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This table was created from information on Microsoft Docs, and there is plenty more information if you’re interested: &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener"
>Data types (Transact-SQL) - SQL Server | Microsoft Docs&lt;/a>.&lt;/p>
&lt;h2 id="the-setup">The Setup&lt;/h2>
&lt;p>For this example, let’s say I have a table that gets loaded by a daily batch job. The column we will discuss needs to store just the date, no time and no need for time zone awareness. This leaves us with 4 options to investigate.&lt;/p>
&lt;p>In order to demonstrate the importance of choosing the right datatype I’m going to create 4 really simple tables. They will each have one column, with a default value of today&amp;rsquo;s date, and no time information.  These tables are totally unrealistic, but it isolates the storage required for a single column and will allow us to focus in on the differences our decision here can cause in a small test case.&lt;/p>
&lt;p>create table dataTypeDate (
batchDate date default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeSmallDateTime (
batchSmallDateTime smalldatetime default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeDateTime (
batchDateTime datetime default (convert(date,getdate()))
)&lt;/p>
&lt;p>create table dataTypeDateTime2 (
batchDateTime2 datetime2 default (convert(date,getdate()))
)&lt;/p>
&lt;p>Once we have four tables we can run the following to load our batch for the day. To simulate this we’re going to insert the default values, 500,000 times.  You can see here I’m using the &lt;code>GO 500000&lt;/code> syntax to run each insert half a million times.&lt;/p>
&lt;p>insert into dataTypeDate
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeSmallDateTime
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeDateTime
default values;
GO 500000&lt;/p>
&lt;p>insert into dataTypeDateTime2
default values;
GO 500000&lt;/p>
&lt;h2 id="the-comparison">The Comparison&lt;/h2>
&lt;p>First, let’s take a look at the first row in each table so we can see the different accuracies of our columns:&lt;/p>
&lt;p>SELECT TOP 1 batchDate FROM dataTypeDate
SELECT TOP 1 batchSmallDateTime FROM dataTypeSmallDateTime
SELECT TOP 1 batchDateTime FROM dataTypeDateTime
SELECT TOP 1 batchDateTime2 FROM dataTypeDateTime2&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/topOneRow.jpg"
loading="lazy"
alt="query results from SSMS for select top 1 from each table"
>&lt;/p>
&lt;p>We have the exact same data in these columns, just a date, no time. However, because of the different datatypes we used when we defined our tables, you can clearly see the different accuracies that were available to us.  The requirement for the datatype to be able to meet stricter accuracies is where the additional storage comes in. As you saw in the table above, storing just a date, which is all we need in this situation, will cost us 3 bytes. Any other datatype we choose will add unnecessary storage. While we’re talking bytes, and it doesn’t seem like a big deal for one row with one column, it adds up quickly when we’re talking millions of rows with multiple columns that are inappropriate typed.&lt;/p>
&lt;p>The following query will display some information about our four tables, including the number of 8k pages that each table is using.&lt;/p>
&lt;p>SELECT
schema_name(obj.SCHEMA_ID) as SchemaName,
obj.name as TableName,
ind.type_desc as IndexType,
pas.row_count as NumberOfRows,
pas.used_page_count as UsedPageCount,
(pas.used_page_count * 8)/1024 as SizeUsedMB,
par.data_compression_desc as DataCompression,
(pas.reserved_page_count * 8)/1024 as SizeReservedMB
FROM sys.objects obj
INNER JOIN sys.indexes ind
ON obj.object_id = ind.object_id
INNER JOIN sys.partitions par
ON par.index_id = ind.index_id
AND par.object_id = obj.object_id
INNER JOIN sys.dm_db_partition_stats pas
ON pas.partition_id = par.partition_id
WHERE obj.schema_id &amp;lt;&amp;gt; 4
ORDER BY UsedPageCount desc&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/tables-1024x135.jpg"
loading="lazy"
alt="results of query to show number of pages used by each table"
>&lt;/p>
&lt;p>We can see that there are 500,000 rows in each of our tables and even with only one column there is a sizable difference in the number of pages needed if we chose DateTime or DateTime2 over just the Date type. It’s about a 25% savings- multiply that out by multiple columns across multiple tables and we’re going to start seeing a pretty sizable difference in our storage needs.&lt;/p>
&lt;p>This is still a pretty small dataset, but it does clearly show that there is a significant difference in the amount of storage needed if we choose an unnecessarily accurate datatype for our date values.&lt;/p>
&lt;h2 id="storage-is-cheap--why-do-i-care">&lt;strong>Storage is cheap – why do I care?&lt;/strong>&lt;/h2>
&lt;p>Storage might not be our biggest concern, although enterprise grade storage is not as cheap as the USB drives at the supermarket checkout, but there are several other reasons why this wasted space is a big deal. Here’s a few:&lt;/p>
&lt;p>&lt;strong>Buffer Cache&lt;/strong> – When SQL Server needs to interact with our data it first reads it into memory. Wasted space on disk then becomes wasted space in memory. That means we can store less data in the buffer cache and will have to flush out pages more quickly than if they were optimised.&lt;/p>
&lt;p>&lt;strong>Backup\restore&lt;/strong> – The bigger your database the longer it’s going to take to perform backup and restore activities.&lt;/p>
&lt;p>&lt;strong>Transaction log activity&lt;/strong> – The bigger the record in SQL Server the more space it’ll need when any operations are written to the transaction log. This means you’ll need more disk space for your transaction log and your t-log backups will be larger. More wasted space.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>So in a truly diplomatic fashion, I rate all the datatypes equally. We’ve only looked at types specific to datetimes in this post, but each and every datatype is suitable for storing certain data. The most important point is that we make solid decisions on both the type of data we want to store and the accuracy/precision needed to store that data.&lt;/p>
&lt;p>Thanks for reading, and thanks again to Brent for hosting.&lt;/p></description></item><item><title>Quickly Execute a Folder of SQL Scripts against a SQL Server</title><link>https://jpomfret.github.io/quickly-execute-a-folder-of-sql-scripts-against-a-sql-server/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/quickly-execute-a-folder-of-sql-scripts-against-a-sql-server/</guid><description>&lt;p>Another week and another useful dbatools snippet for you today.  Last week at work I was given a folder of 1,500 scripts – each containing a create table statement. Can you imagine having to open each file in Management Studio to be able to execute it? Thank goodness we have PowerShell and &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> on our side.&lt;/p>
&lt;p>The code for this example is pretty short, but there are a couple of things to point out. &lt;/p>
&lt;p>First, I used &lt;code>Connect-DbaInstance&lt;/code> to create a server object to use to run the queries.  This means that we’re efficiently reusing the connection rather than opening a new one for each file we want to execute. &lt;/p>
&lt;p>Second, I’m using the foreach method which takes each script file returned from the &lt;code>Get-ChildItem&lt;/code> call, and executes &lt;code>Invoke-DbaQuery&lt;/code>.  With this we can use the &lt;code>-File&lt;/code> parameter to pass in the sql file and that’s really all we need.  This will loop through each file running the sql scripts.&lt;/p>
&lt;p>$SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
$destinationDatabase = &amp;lsquo;AdventureWorks2021&amp;rsquo;
$folderPath = &amp;lsquo;.\output\AdventureWorks2017&amp;rsquo;&lt;/p>
&lt;h1 id="create-a-connection-to-the-server-that-we-will-reuse---can-use-sqlcredential-for-alternative-creds">Create a connection to the server that we will reuse - can use SqlCredential for alternative creds&lt;/h1>
&lt;p>$sqlInst = Connect-DbaInstance -SqlInstance $SqlInstance&lt;/p>
&lt;p>(Get-ChildItem $folderPath).Foreach{
Invoke-DbaQuery -SqlInstance $sqlInst -Database $destinationDatabase -File $psitem.FullName
}&lt;/p>
&lt;p>That’s really all we need for this blog post, but in order to set this up for a demo I did use a few other dbatools commands. I’ve posted the script above, along with the setup scripts on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/08_ExecuteFolderOfScripts.ps1" target="_blank" rel="noopener"
>GitHub&lt;/a>. This includes creating a new database, scripting out all the tables into individual script files, and ensuring all the schemas and other dependencies were ready in the new database.&lt;/p>
&lt;p>Thanks for reading, and hope this is a useful snippet. It sure saved me a lot of time this week.&lt;/p></description></item><item><title>Troubleshooting SPN Troubles - Cannot generate SSPI context</title><link>https://jpomfret.github.io/troubleshooting-spn-troubles-cannot-generate-sspi-context/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/troubleshooting-spn-troubles-cannot-generate-sspi-context/</guid><description>&lt;p>I was working in my lab environment this weekend, playing with some SQL Servers that I had built with PowerShell DSC a while ago.  I had installed SQL Server with mostly defaults, including not changing the engine and agent service accounts.  For the blog post I thought I was going to write next, I wanted to change these to be active directory accounts – it did not go smoothly, and I figured this might be useful to document for future Jess, or anyone else who might stumble across this problem.&lt;/p>
&lt;h2 id="create-the-issue-change-sql-server-service-accounts">Create the Issue: Change SQL Server Service Accounts&lt;/h2>
&lt;p>First off, I created two new Active Directory users that I’ll use for my service accounts. The below code will create a prompt for each account for the password to be entered.&lt;/p>
&lt;p>$engSvcAccount = &amp;lsquo;svc-dscsvr1-eng2&amp;rsquo;
$agSvcAccount = &amp;lsquo;svc-dscsvr1-ag2&amp;rsquo;&lt;/p>
&lt;p>$EngSvcAccount = @{
Name = $engSvcAccount
UserPrincipalName = $engSvcAccount
AccountPassword = (Get-Credential -Credential EnterPassword).Password
PasswordNeverExpires = $true
Enabled = $true
}
New-AdUser @EngSvcAccount&lt;/p>
&lt;p>$AgentSvcAccount = @{
Name = $agSvcAccount
UserPrincipalName = $agSvcAccount
AccountPassword = (Get-Credential -Credential EnterPassword).Password
PasswordNeverExpires = $true
Enabled = $true
}
New-AdUser @AgentSvcAccount&lt;/p>
&lt;p>We can view the current SQL services with &lt;code>Get-DbaService&lt;/code>. This is useful to see what account they are currently running under, as well as the service names.&lt;/p>
&lt;p>Get-DbaService -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/GetDbaService.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaService.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>There is also a command for updating service accounts in dbatools. I will note, sometimes I have issues with the command being able to update the accounts and I’m not sure why. It worked perfectly in this scenario though running the following.&lt;/p>
&lt;p>This again creates a prompt to enter the service account password, before setting the service &amp;lsquo;StartName&amp;rsquo;.&lt;/p>
&lt;p>Update-DbaServiceAccount -ComputerName dscsvr1 -ServiceName MSSQLSERVER -ServiceCredential (Get-Credential -Credential &amp;ldquo;Pomfret\$engSvcAccount&amp;rdquo; )
Update-DbaServiceAccount -ComputerName dscsvr1 -ServiceName SQLSERVERAGENT -ServiceCredential (Get-Credential -Credential &amp;ldquo;Pomfret\$agSvcAccount&amp;rdquo; )&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/UpdateDbaServiceAccount.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/UpdateDbaServiceAccount.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>If I rerun &lt;code>Get-DbaService&lt;/code> I can see all looks good. &lt;code>StartName&lt;/code> shows my new accounts and the services for both engine and agent are running.&lt;/p>
&lt;p>Get-DbaService -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/GetDbaService_post.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaService_post.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="the-issue">The Issue&lt;/h2>
&lt;p>At this point I was still planning on writing a blog post on a totally different topic. I ran the following to determine what databases I already had on dscsvr1:&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance dscsvr1 -ExcludeSystem | Format-Table&lt;/p>
&lt;p>But instead of getting a quick answer to my question, I just got the following error:&lt;/p>
&lt;p>&lt;em>WARNING: [15:19:49][Get-DbaDatabase] Error occurred while establishing connection to dscsvr1 | The target principal name is incorrect. Cannot generate SSPI context.&lt;/em>&lt;/p>
&lt;p>I checked a few things as I started troubleshooting:&lt;/p>
&lt;ul>
&lt;li>Are the services running – we already checked this with &lt;code>Get-DbaService&lt;/code> and they are&lt;/li>
&lt;li>Was it firewall related – inbound rules were in place, and I was able to previously connect&lt;/li>
&lt;li>Was it certificate related – I’m not forcing encryption, and there are no certificates set up&lt;/li>
&lt;li>Was it Service Principal Name (SPN) related – bingo&lt;/li>
&lt;/ul>
&lt;p>I have seen this happen before when changing service accounts for SQL services. I’m not an Active Directory expert, and I’m certainly not a Kerberos expert – in fact I’m as surprised as you that Kerberos has actually appeared on this blog. &lt;/p>
&lt;p>What I do know is that due to permissions, the SPNs needed were not able to be registered for the new service accounts.  The easiest way to investigate SPN issues is with dbatools, saving us again!&lt;/p>
&lt;p>&lt;code>Test-DbaSpn&lt;/code> works out exactly what SPNs are needed for our SQL instances and determines if they are in place.&lt;/p>
&lt;p>Test-DbaSpn -ComputerName dscsvr1 | Format-Table&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/Test-DbaSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Test-DbaSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>This shows we should have two SPNs set for the default instance (MSSQLSERVER) on DscSvr1.  The ‘IsSet’ column shows they aren’t set – and this is why we can’t connect to our instance remotely.&lt;/p>
&lt;h2 id="lets-fix-it">Let&amp;rsquo;s Fix It&lt;/h2>
&lt;p>The fix for this issue seems simple- register the required SPNs.  dbatools again tries to make this as easy as possible for us. We can take the output from &lt;code>Test-DbaSpn&lt;/code> and pipe it straight into &lt;code>Set-DbaSpn&lt;/code> and dbatools will take care of the rest – won’t it?&lt;/p>
&lt;p>Test-DbaSpn -ComputerName dscsvr1 | Set-DbaSpn&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/FailedToSetSPN.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/FailedToSetSPN.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>As you can see from the warning message, dbatools wasn’t able to set our required SPNs either. It complains about &amp;lsquo;A constraint violation occurred&amp;rsquo;.&lt;/p>
&lt;p>The reason is each SPN can only be registered once, and these SPNs were created for the previous service accounts and never cleaned up due to a lack of permissions.&lt;/p>
&lt;p>We now need to use the &lt;code>setspn&lt;/code> command line tool that is built into Windows and available when you have AD windows features installed, but the output from the dbatools command is still very useful for building the inputs for &lt;code>setspn&lt;/code>.&lt;/p>
&lt;p>First we’ll try and register the required SPNs manually. For this we’ll use the -a parameter on &lt;code>setspn&lt;/code>, the format being:&lt;/p>
&lt;p>setspn -a &amp;laquo;SPN&amp;raquo; &amp;laquo;ServiceAccount&amp;raquo;&lt;/p>
&lt;p>So we’ll run the following, getting the SPN from the ‘RequiredSPN’ column of the &lt;code>Test-DbaSpn&lt;/code> output. You’ll notice there are two required SPNs, one without a port specified and one with 1433 – we’ll want to fix both.&lt;/p>
&lt;p>setspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\svc-dscsvr1-eng2&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/duplicateSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/duplicateSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see in the output, the problem is highlighted – ‘Duplicate SPN found’.  The useful part of this output is on the second line.  I’ve highlighted the current owner of the SPN – we need this to be able to resolve the problem.  Not surprising, it is the computer account since I was previously running SQL Server as the default &lt;code>NT SERVICE\MSSQLSERVER&lt;/code> account.&lt;/p>
&lt;p>Now we know what the duplicate is we can remove it, again using setspn, but this time with the -d parameter. The format is:&lt;/p>
&lt;p>setspn -d &amp;laquo;SPN&amp;raquo; &amp;laquo;ServiceAccount&amp;raquo;&lt;/p>
&lt;p>We’ll run the following two commands to clear up both old SPNs:&lt;/p>
&lt;p>setspn -d MSSQLSvc/DscSvr1.pomfret.com DSCSVR1
setspn -d MSSQLSvc/DscSvr1.pomfret.com:1433 DSCSVR1&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/DeleteSPN.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/DeleteSPN.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Finally we can add the required SPNs. We can either use dbatools with the code we tried earlier or &lt;code>setspn&lt;/code>.&lt;/p>
&lt;p>setspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\svc-dscsvr1-eng2
setspn -a MSSQLSvc/DscSvr1.pomfret.com:1433 Pomfret\svc-dscsvr1-eng2&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/addSpn.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/addSpn.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see the output now states ‘Updated object’ which means we were successful. If we try and view the databases again now we should see the output we were expecting.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/02/Get-DbaDatabase.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Get-DbaDatabase.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>As I mentioned, this was not at all what I was expecting to write about – but I hope it’ll be useful if you ever find yourself in this situation while trying to change service accounts for SQL Server.&lt;/p>
&lt;p>I was in the end able to resolve the permissions problems for my service accounts by following this great blog post &amp;lsquo;&lt;a class="link" href="http://www.alexandreviot.net/2014/09/30/sql-server-could-not-register-the-service-principal-name-spn/" target="_blank" rel="noopener"
>SQL Server - Could not register the Service Principal Name&lt;/a>&amp;rsquo;. Once I applied these permissions when I changed service accounts they were able to delete and recreate the required SPNs.&lt;/p>
&lt;p>Another great blog post for more reading on SPNs is this post by &lt;a class="link" href="https://www.twitter.com/pittfurg" target="_blank" rel="noopener"
>Drew Furgiuele&lt;/a> on how to use the &lt;a class="link" href="https://dbatools.io/schwifty/" target="_blank" rel="noopener"
>dbatools SPN commands&lt;/a>.&lt;/p></description></item><item><title>T-SQL Tuesday #135: The outstanding tools of the trade that make your job awesome</title><link>https://jpomfret.github.io/t-sql-tuesday-#135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/</guid><description>&lt;p>&lt;a class="link" href="https://www.bronowski.it/blog/2021/02/t-sql-tuesday-135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>It’s time for February’s monthly blog party. This month is hosted by Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/MikeyBronowski" target="_blank" rel="noopener"
>t&lt;/a>) and he’s asking us to write about our ‘tools of the trade’.  He’s looking for those tools that make our lives easier, ones we couldn’t imagine going without. Thanks for hosting Mikey, can’t wait to read everyone’s contributions and add some tools to my toolbelt.&lt;/p>
&lt;p>I’m going to split this into a couple of sections. I’m sure you can all guess what’s up first though…&lt;/p>
&lt;h2 id="powershell">PowerShell&lt;/h2>
&lt;p>If I could only choose one tool for my toolbelt it would be PowerShell, which is actually probably cheating because there are so many options to import modules and add functionality.  I’m going to highlight five modules I use a lot below.&lt;/p>
&lt;ol>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbatools" target="_blank" rel="noopener"
>dbatools&lt;/a> – If you’ve read much of my blog before, or seen me present, it should be no surprise that dbatools is number one.  I use dbatools every day, whether it’s to check diskspace, update database owners, or a plethora of other uses.  In fact I previously wrote a post ‘&lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-101/" target="_blank" rel="noopener"
>The Multitool of my DBA toolbox&lt;/a>’ that highlights five great use cases.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a> – A close friend of dbatools, dbachecks combines Pester and dbatools to create an amazing infrastructure testing module.  This is perfect for creating a morning checks dashboard, or quickly checking certain parts of your estate. For example, in my post ‘&lt;a class="link" href="https://jesspomfret.com/dbachecks-importexcel/" target="_blank" rel="noopener"
>dbachecks meets ImportExcel&lt;/a>’ we check up on backups and database status before exporting to create an Excel report.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/pester/Pester" target="_blank" rel="noopener"
>Pester&lt;/a> – Originally designed for unit/integration testing, I personally use this framework to test anything you can write in PowerShell. It quickly provides a clear and easy to read answer for whether everything is as expected. I’ve written about it previously to ‘&lt;a class="link" href="https://jesspomfret.com/pester-test-cluster-role-owners/" target="_blank" rel="noopener"
>Pester test your Cluster Role Owners&lt;/a>’.&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a> – This module lets you work with Excel objects, without having Excel installed.  Easily read data from spreadsheets into PowerShell, or export data to create detailed reports with a few lines of code. Our host for this T-SQL Tuesday has written a great series on this module, if you’re looking for inspiration. &lt;a class="link" href="https://www.bronowski.it/blog/tag/importexcel/" target="_blank" rel="noopener"
>importexcel Archives - Mikey Bronowski - Blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/PowershellFrameworkCollective/psframework" target="_blank" rel="noopener"
>PSFramework&lt;/a> – Finally, I want to highlight PSFramework.  Portions of this module are used within both dbatools and dbachecks.  It provides great options for both setting configuration options that can be then used in your modules as well as for creating great logging. I’ve switched to using Write-PSFMessage instead of Write-Host\Verbose\Output as it provides a lot more flexibility as well as writing to a physical log file.&lt;/li>
&lt;/ol>
&lt;p>I also recently wrote about &lt;a class="link" href="https://jesspomfret.com/psreadline-search-history/" target="_blank" rel="noopener"
>PowerShell’s interactive search functionality&lt;/a>, and after a poll on Twitter was pretty shocked by how few people knew about it.  I recommend checking it out, as it is a really handy built in feature.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/jpomfret/status/1357014555638042624" target="_blank" rel="noopener"
>https://twitter.com/jpomfret/status/1357014555638042624&lt;/a>&lt;/p>
&lt;h2 id="microsoft-excel">Microsoft Excel&lt;/h2>
&lt;p>Since I’ve written a lot about PowerShell previously, I wanted to highlight some other tools that I depend on. I’ve always been a fan of Excel, my personal life is full of spreadsheets – most decisions end with a spreadsheet (lucky for me, my wife is also a big fan of Excel!).  I often find myself copying data into Excel to keep track of work, or to quickly analyse data.  It’s also a great way of sharing data with a clear structure.  I’m also a big fan of shortcuts – so here’s a few I use often.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ctrl+;&lt;/strong> - Insert today’s date into the current cell – really useful, and avoids you having to remember we’re now in 2021!&lt;/li>
&lt;li>&lt;strong>Ctrl+l&lt;/strong> – Select a cell within a dataset, press Ctrl+l (lowercase L), press enter. Your data is transformed into a table.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ctrlL.gif"
loading="lazy"
alt="Gif showing using Ctrl&amp;#43;L in Excel to create a table"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ctrl+D&lt;/strong> – Fill down, this will copy the contents of the cell above into your current cell.  Also smart enough to handle continuation of formulas.&lt;/li>
&lt;li>&lt;strong>Ctrl+R&lt;/strong> – Fill right, same as above but it’ll copy the contents of the cell to your left into your current cell.&lt;/li>
&lt;li>&lt;strong>Ctrl+Up/Down arrow&lt;/strong> – This will move your cursor to either the first value in the current column, or the last.  I use this a lot for navigating around worksheets/tables.&lt;/li>
&lt;li>&lt;strong>F2&lt;/strong> – This edits a cell&amp;rsquo;s contents. It puts your cursor at the end of the value, but you can now use your arrow keys to move about in the cell. It also stops you accidentally overwriting what was already in the cell.&lt;/li>
&lt;/ul>
&lt;h2 id="my-bike">My Bike&lt;/h2>
&lt;p>My final tool is my bike. Not technical at all, but a tool I use to keep fit and have some fun.  I love cycling, and in the current times it’s my best option for fitness (I’m in England – we’re deep into lockdown 3 and gyms are closed). &lt;/p>
&lt;p>Honestly, I have a really hard time working out at home. I enjoy going to the gym, seeing some friendly faces and having someone tell me what to do for an hour.  It’s not the same at home, and my mood is instantly affected by not being active.&lt;/p>
&lt;p>However, I’m happy to go out for a ride, and living in the South of England the weather is reasonably kind all year round.  Previously, living in Ohio there weren’t many options for winter bike riding, unless you had fat tyres and loved the snow!  I’m also lucky to be close to the South Downs (pictured below), as well as plenty of country lanes to explore.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Bike_SouthDowns-1024x768.jpg"
loading="lazy"
alt="My bike on the south downs"
>&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Thanks for reading and hope you’ve enjoyed digging through my toolbox. Thanks again to Mikey for hosting. I always enjoy participating in these T-SQL Tuesday’s, partly because it gives me a prompt to write about, partly because it’s fun to see what everyone else wrote about.&lt;/p>
&lt;p>Stay safe folks.&lt;/p></description></item><item><title>Easily Search PowerShell Command History With PSReadLine</title><link>https://jpomfret.github.io/easily-search-powershell-command-history-with-psreadline/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/easily-search-powershell-command-history-with-psreadline/</guid><description>&lt;p>I was listening to a &lt;a class="link" href="http://runasradio.com/Shows/Show/760" target="_blank" rel="noopener"
>podcast last week about PowerShell&lt;/a>, when one of the hosts mentioned having to ‘up arrow’ back through your history to find a command you wanted to rerun.  This made me realise that I should write this quick post on using &lt;a class="link" href="https://github.com/PowerShell/PSReadLine" target="_blank" rel="noopener"
>PSReadLine&lt;/a>’s interactive search function.  This tip is a serious time saver and I rely on it heavily.&lt;/p>
&lt;p>The great news is that if you are using Windows PowerShell on Windows 10 or if you’re using PowerShell 6+, PSReadLine is already installed and you can immediately start using this tip.  If you don’t have the module though, it’s easy enough to install from the PowerShell Gallery:    &lt;/p>
&lt;p>Install-Module PSReadLine&lt;/p>
&lt;p>The main goal of this module is to enhance the command line experience for users. There is a lot of great stuff in this module, some of which you’re probably already using, without even realising it’s coming from PSReadLine. A fun experiment for this is to open a new console, run &lt;code>Remove-Module PSReadline&lt;/code> and then see what’s missing. The two biggest things I notice (on top of the interactive search) is there’s no history available from previous sessions, and no colour coding to show the differences between variables, parameters and input.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/removePSReadLine.jpg"
loading="lazy"
alt="Showing no colour coding after removing PSReadLine"
>&lt;/p>
&lt;p>The screenshot above is using Windows Terminal and PowerShell 7, but the same thing happens using older versions of PowerShell. Shout out to &lt;a class="link" href="http://twitter.com/cl" target="_blank" rel="noopener"
>Chrissy LeMaire&lt;/a> for the beautiful &lt;a class="link" href="https://blog.netnerds.net/2020/07/my-windows-terminal-retro-theme/" target="_blank" rel="noopener"
>Windows Terminal theme&lt;/a>.&lt;/p>
&lt;p>As I mentioned earlier, when you’re running PowerShell in a console it tracks the commands you run, building up a history of all the things you’ve executed.  This is really useful if you want to slightly change the command you just ran, perhaps fixing a typo, or piping the output to another command. The problem comes when you know you ran something recently, and you start ‘up arrowing’ furiously through the history trying to find what you’re looking for.&lt;/p>
&lt;p>You can also view the history of your current session by executing &lt;code>history&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/history-1.jpg"
loading="lazy"
alt="powershell command history "
>&lt;/p>
&lt;p>PSReadLine makes looking for a specific command in the history haystack even easier with ‘Bash/zsh style interactive history search’.  I have created a gif below to demonstrate this, but it’s as simple as pressing ‘Ctrl+R’ from the console. That will create a second line under your prompt where you can start typing your search terms.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/interactiveSearch.jpg"
loading="lazy"
alt="Interactive back search"
>&lt;/p>
&lt;p>As you type PSReadLine is going back through your history to find the most recent command that matches what you’ve typed so far. As you continue typing it hones in on the exact command you’re looking for.  To look further back in your history, press ‘Ctrl+R’ again to find the next time you used that search term in a command.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/psreadline-1.gif"
loading="lazy"
>&lt;/p>
&lt;p>If you press `Ctrl+R` one too many times, you can press `Ctrl+S` to search forward, basically taking you forward in time one search result.&lt;/p>
&lt;p>Hope you find this as useful as I do. As I mentioned I rely heavily on interactive search to rerun commands I’ve used before.&lt;/p></description></item><item><title>Discover SQL Server Permissions hidden via AD Group Membership</title><link>https://jpomfret.github.io/discover-sql-server-permissions-hidden-via-ad-group-membership/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/discover-sql-server-permissions-hidden-via-ad-group-membership/</guid><description>&lt;p>When granting permissions to SQL Server resources we have a few options. One option is to grant permissions to Active Directory groups instead of individual users.  This has several benefits, for example, improved security over using SQL logins, and the ability to create a separation of duties when controlling database access.&lt;/p>
&lt;p>However, it does add an extra step when trying to determine who has what access to your SQL Servers. It can also make troubleshooting permission issues more challenging.  This post is going to aim to simplify this by combining dbatools and ActiveDirectory PowerShell modules to provide a clear solution.&lt;/p>
&lt;h2 id="setup">&lt;strong>Setup&lt;/strong>&lt;/h2>
&lt;p>This is obviously not needed in our actual environments, this is just how I prepared my lab so I could demonstrate how we can solve this problem.  Feel free to skip ahead to the solution if you already have plenty of AD groups to investigate.&lt;/p>
&lt;p>The full code sample is available in my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/07_PermisssionsGrantedViaADGroups.ps1" target="_blank" rel="noopener"
>GitHub demos repo&lt;/a>.&lt;/p>
&lt;p>First, I created some AD users and groups to use in my lab. This is easily achieved with the AD PowerShell module using &lt;code>New-AdUser&lt;/code> and &lt;code>New-AdGroup&lt;/code>. Then, using &lt;code>Add-AdGroupMember&lt;/code>, I added two users into the newly created group.&lt;/p>
&lt;p># setup - create some AD users\groups uing the ActiveDirectory module&lt;/p>
&lt;h1 id="create-several-new-ad-users">create several new ad users&lt;/h1>
&lt;p>(&amp;lsquo;pomfretJ&amp;rsquo;,&amp;lsquo;smithA&amp;rsquo;, &amp;lsquo;jonesP&amp;rsquo;,&amp;lsquo;barnesR&amp;rsquo;).foreach{New-AdUser $_}&lt;/p>
&lt;h1 id="view-newly-created-users">view newly created users&lt;/h1>
&lt;p>$date = (get-date).AddHours(-1)
get-aduser -filter {created -gt $date} | select name&lt;/p>
&lt;h1 id="create-a-new-ad-group">create a new Ad group&lt;/h1>
&lt;p>$newAdGroup = @{
Name = &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;
GroupCategory = &amp;lsquo;Security&amp;rsquo;
GroupScope = &amp;lsquo;Global&amp;rsquo;
Path = &amp;lsquo;CN=Users,DC=pomfret,DC=com&amp;rsquo;
}
New-ADGroup @newAdGroup&lt;/p>
&lt;h1 id="add-users-to-group">add users to group&lt;/h1>
&lt;p>$addMemberGroup = @{
Identity = &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;
Members = &amp;lsquo;pomfretj&amp;rsquo;, &amp;lsquo;jonesP&amp;rsquo;
}
Add-ADGroupMember @addMemberGroup&lt;/p>
&lt;p>The second part of the setup was to add an AD group and an AD user to the SQL Server and grant some permissions using dbatools.&lt;/p>
&lt;p># setup - grant permissions to ad users\groups using dbatools&lt;/p>
&lt;h1 id="add-ad-group-and-grant-permissions-db_datareader-to-adventureworks">add ad group and grant permissions (db_datareader to AdventureWorks)&lt;/h1>
&lt;p>New-DbaLogin -SqlInstance dscsvr1 -Login &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo;
New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo;
Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_datareader -User &amp;lsquo;Pomfret\AdventureWorksReadOnly&amp;rsquo; -Confirm:$false&lt;/p>
&lt;h1 id="add-ad-user-to-sql-server-and-provide-permissions-db_owner-to-adventureworks">add ad user to sql server and provide permissions (db_owner to AdventureWorks)&lt;/h1>
&lt;p>New-DbaLogin -SqlInstance dscsvr1 -Login &amp;lsquo;Pomfret\smithA&amp;rsquo;
New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login &amp;lsquo;Pomfret\smithA&amp;rsquo;
Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_owner -User &amp;lsquo;Pomfret\smithA&amp;rsquo; -Confirm:$false&lt;/p>
&lt;h2 id="viewing-database-access">&lt;strong>Viewing database access&lt;/strong>&lt;/h2>
&lt;p>Now that my lab environment is set up, let’s take a look at database users that have access to the AdventureWorks2017 database.  This is an easy task thanks to dbatools, we can just use &lt;code>Get-DbaDbUser&lt;/code>. Shown below, you can clearly see there is a WindowsUser &amp;lsquo;smithA&amp;rsquo; that has access, as well as a WindowsGroup &amp;lsquo;AdventureWorksReadOnly&amp;rsquo;.&lt;/p>
&lt;p># Find users that have permissions through group membership
Get-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -ExcludeSystemUser | Select-Object SqlInstance, Database, Login, LoginType, HasDbAccess&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/GetDbaDbUser.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/GetDbaDbUser.png"
loading="lazy"
alt="Get-DbaDbUser results"
>&lt;/a>&lt;/p>
&lt;p>We can also use &lt;code>Get-DbaDbRoleMember&lt;/code> to see exactly which database roles these users have been granted. &lt;/p>
&lt;p>Get-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 | Select-Object SqlInstance, Database, role, Login&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/Get-DbaDbRoleMember.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRoleMember.png"
loading="lazy"
alt="Get-DbaDbRoleMember output"
>&lt;/a>&lt;/p>
&lt;p>The issue is the same for both these examples, we don’t know which users are inheriting the permissions granted to the &amp;lsquo;AdventureWorksReadOnly&amp;rsquo; group. This is where we need to combine these two modules to get the answers we need.&lt;/p>
&lt;p>There are several ways you could combine the output of two functions. For this example I’m going to use a calculated property.&lt;/p>
&lt;p>If I run the exact same code as before to get a list of role members from the dbatools function &lt;code>Get-DbaDbRoleMember&lt;/code>, I can add a calculated property in the Select-Object to lookup the members of that specific group from active directory.  In the example below you can see the &amp;lsquo;AdventureWorksReadOnly&amp;rsquo; group has two members, and we now know that both &amp;lsquo;pomfretJ&amp;rsquo; and &amp;lsquo;jonesP&amp;rsquo; have read access to the AdventureWorks2017 database. &lt;/p>
&lt;p>You can also still see the WindowsUser, &amp;lsquo;smithA&amp;rsquo;, has db_owner permissions.  Since that lookup didn’t return any results (obviously, since it’s a user not a group), the GroupMembers property remains empty.&lt;/p>
&lt;p>Get-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 |
Select-Object SqlInstance, Database, Role, LoginType, Login, @{l=&amp;lsquo;GroupMembers&amp;rsquo;;e={ (Get-AdGroupMember -Identity ($_.Login).Split(&amp;rsquo;\&amp;rsquo;)[1]).Name }}&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2021/01/FinalOutput.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/FinalOutput.png"
loading="lazy"
alt="Combining Get-DbaDbRoleMember &amp; Get-AdGroupMember"
>&lt;/a>&lt;/p>
&lt;p>You can also use this same code to determine specific user access, for example, by adding a Where-Object to see just the permissions granted to &amp;lsquo;pomfretJ&amp;rsquo;.&lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>This should give you an easy option for determining specific user access that is hidden behind AD groups, and I think reduces one of the negatives of using AD groups in this situation.  It also shows us that we can combine multiple functions into one to get all the information we need with one easy line of code.&lt;/p>
&lt;p>I would also encourage you to explore the other permission related dbatools functions available, including &lt;code>Get-DbaServerRole&lt;/code> and &lt;code>Get-DbaPermission&lt;/code>. These can also be used in combination with &lt;code>Get-AdGroupMember&lt;/code> to enhance the results.&lt;/p></description></item><item><title>Easily Create A Copy Of Your Database For Testing</title><link>https://jpomfret.github.io/easily-create-a-copy-of-your-database-for-testing/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/easily-create-a-copy-of-your-database-for-testing/</guid><description>&lt;p>Have you ever wanted to quickly backup/restore a database to the same instance to do some side by side testing? Perhaps to make some index changes or code changes, without actually changing the live copy of the database?  Ideally you’d already have another environment for this sort of work, but even then sometimes it’s handy to have a quick option.&lt;/p>
&lt;p>Let’s first take a look at the databases on my SQL Server- we can use a GUI tool for that (SSMS, ADS) or we can use dbatools.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1 | Select-Object SqlInstance, Name, Status, Size&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDatabase.jpg"
loading="lazy"
alt="Get-DbaDatabase"
>&lt;/p>
&lt;p>We’re working hard on the AdventureWorks2017 database, perhaps getting it ready for an upgrade – since it’s now 3+ years out of date.&lt;/p>
&lt;p>dbatools has so many functions, and I know I’ve mentioned it before, but &lt;code>Find-DbaCommand&lt;/code> is a great way of looking for what we need. I want to know what the default backup path is set to, and since I’m just backing up and restoring to the same server, we already know that the instance has the required permissions here. If only there was an easy button for this…&lt;/p>
&lt;p>Find-DbaCommand *default*path*backup*&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/findcommand.jpg"
loading="lazy"
alt="results of Find-DbaCommand"
>&lt;/p>
&lt;p>Even just reading the synopsis, I can see that &lt;code>Get-DbaDefaultPath&lt;/code> will give me exactly what I need.  I recommend the next step is running &lt;code>Get-Help Get-DbaDefaultPath -ShowWindow&lt;/code>, that’ll create a popup that provides all the information you need about the function.&lt;/p>
&lt;p>The only required parameter is a SqlInstance, and you can see the backup property returns gives us the path we need for our copy.&lt;/p>
&lt;p>Get-DbaDefaultPath -SqlInstance mssql1&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDbaDefaultPath.jpg"
loading="lazy"
alt="Get-DbaDefaultPath output"
>&lt;/p>
&lt;p>That’s all the groundwork done- we have our instance, database, and a location to backup/restore from.  We’re going to want to check we have enough disk space available on both the instance and that backup path, then we’re ready to go.&lt;/p>
&lt;h2 id="using-copy-dbadatabase">&lt;strong>Using Copy-DbaDatabase&lt;/strong>&lt;/h2>
&lt;p>I’ve already spoken and blogged a lot about the power of this command (related links at the end of this post), but today’s tip is centred around a less than well-known parameter.  Hidden deep in the comment based help (another great reason to read all of &lt;code>Get-Help Copy-DbaDatabase -ShowWindow&lt;/code>) you’ll find the ‘Prefix’ parameter. This will allow us to easily add a prefix to both the database and the associated files, meaning we won’t have any issues restoring the database to the same server.&lt;/p>
&lt;p>-Prefix &lt;!-- raw HTML omitted --> &lt;br>
All copied database names and physical files will be prefixed with this string&lt;/p>
&lt;p>This option is mutually exclusive of NewName&lt;/p>
&lt;p>Required? false
Position? named
Default value &lt;br>
Accept pipeline input? False
Accept wildcard characters? false&lt;/p>
&lt;p>Here I’ve set a SqlInstance variable so I can reuse the same value multiple times in my code. Then created a hash table ‘$copySplat’ with the necessary parameters so we can utilise splatting (a way to improve code readability) to pass the whole set into &lt;code>Copy-DbaDatabase&lt;/code>. &lt;/p>
&lt;p>Two parameters I want to highlight- I’ve set ‘Prefix’, meaning the database and files for the restored database will start with ‘Test’.  I’ve also set SharedPath and used the code we already wrote to get the default backup path.&lt;/p>
&lt;p>$sqlInstance = &amp;lsquo;mssql1&amp;rsquo;&lt;/p>
&lt;p>$copySplat = @{
Source = $sqlInstance
Destination = $sqlInstance
Database = &amp;lsquo;AdventureWorks2017&amp;rsquo;
BackupRestore = $true
SharedPath = (Get-DbaDefaultPath -SqlInstance $sqlInstance).Backup
Prefix = &amp;lsquo;Test&amp;rsquo;
}
Copy-DbaDatabase @copySplat&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/copyDatabase.jpg"
loading="lazy"
alt="Copy-DbaDatabase"
>&lt;/p>
&lt;p>The output below shows the migration was successful, and there were no warnings or errors (those would appear in the notes column).&lt;/p>
&lt;p>Finally, let’s confirm it worked by rerunning our &lt;code>Get-DbaDatabase&lt;/code> command again:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDatabaseAfter.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Extra proof, it’s now accessible through Azure Data Studio (ADS) and we’re ready to start our testing.  One note, if you are on the same server it’s important to confirm any code you run isn’t referencing the original database name.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ADSView.jpg"
loading="lazy"
>&lt;/p>
&lt;h2 id="additional-content">&lt;strong>Additional content&lt;/strong>&lt;/h2>
&lt;p>As I mentioned I have already spoken and written about the power of &lt;code>Copy-DbaDatabase&lt;/code>, one of my favourite commands.  If you’d like to read more, I’ve written a post on the dbatools blog, &lt;a class="link" href="https://dbatools.io/migrating-application-dbs/" target="_blank" rel="noopener"
>migrating application databases with dbatools&lt;/a>.&lt;/p>
&lt;p>I’ve also recorded a short ‘Life hack’ video, &lt;a class="link" href="https://www.youtube.com/watch?v=Fraig15pwxE&amp;amp;t=1s" target="_blank" rel="noopener"
>easy database migrations with dbatools&lt;/a> that I’ve published on my YouTube channel.&lt;/p></description></item><item><title>T-SQL Tuesday #134 - Give me a break</title><link>https://jpomfret.github.io/t-sql-tuesday-#134-give-me-a-break/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#134-give-me-a-break/</guid><description>&lt;p>&lt;a class="link" href="https://jimbabwe.co.za/2021/01/04/tsqltuesday-134-invitation/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>It’s the first T-SQL Tuesday of 2021, and James Mcgillivray (&lt;a class="link" href="https://jimbabwe.co.za/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/JamesMcG_MSBI" target="_blank" rel="noopener"
>t&lt;/a>) is our host for January’s edition of this monthly blog party.  This month’s topic is around how we escape with vacation/holiday plans.  They gave us a few options on what to blog about and I’m going to take you on a virtual trip to Hawaii.&lt;/p>
&lt;h2 id="what-is-the-best-vacationholiday-youve-ever-had">What is the best vacation/holiday you’ve ever had?&lt;/h2>
&lt;p>The best holiday I’ve ever had would have to be my honeymoon to Hawaii in 2018.  We spent 10 days exploring two islands and it was fantastic.  This is a great prompt as it has encouraged me to look back at my photos from that time and record some memories.  I have a pretty horrible memory, but that trip I will never forget. I hope that as you follow along with this post it will take you on a virtual holiday, welcome to ‘Team Jelcie’s Hawaii Honeymoon’.&lt;/p>
&lt;h3 id="first-stop-maui">First stop Maui&lt;/h3>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FB_IMG_1540745531257.jpg"
loading="lazy"
alt="Jess &amp; Kelcie in Wedding outfits"
>&lt;/p>
&lt;p>We got married on Saturday 27th of October 2018 in Ohio, not far from where we were living.  We were lucky to be surrounded by friends and family from both sides of the Atlantic, and apart from the constant rain all day, we had an amazing time. &lt;/p>
&lt;p>The following Monday we boarded a plane for Maui, Hawaii. It’s actually a longer flight time from Ohio to Maui than it is from Ohio to England.  Until we started planning this trip I had no idea how far off the coast of the continental USA it actually is.&lt;/p>
&lt;p>One of the benefits of this long flight time, jet lag had us waking up really early the first few days. We brewed some coffee in the room and took it down to the beach.  We were treated to the most amazing sunrises. The longer we stood there the more impressive the colours.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/20181031_062105-scaled.jpg"
loading="lazy"
alt="Hawaii sunrise from the beach on Maui"
>&lt;/p>
&lt;p>We’d planned our honeymoon using a travel agent,  so we pre-booked several excursions for our time away. The first being a sunset sail, which didn’t disappoint, since the sunsets were almost as beautiful as the sunrises.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/20181031_174225-1024x768.jpg"
loading="lazy"
alt="Picture of the sunset from our sailing boat"
>&lt;/p>
&lt;p>The second excursion was one of the top memories of the trip, a snorkelling excursion to &lt;a class="link" href="https://en.wikipedia.org/wiki/Molokini" target="_blank" rel="noopener"
>Molokini&lt;/a>, a ‘crescent-shaped, partially submerged volcanic crater’. I have never seen such clear water- you could see the bottom of the ocean and it was 40m deep in parts.  Also so many fish, every shape and size and colour. It was fantastic. &lt;/p>
&lt;p>We also hiked, dropped into a Crossfit gym, relaxed and ate plenty of delicious food, before it was time to board a flight to the next stop of our trip.&lt;/p>
&lt;h3 id="second-island--kauai">Second island – Kauai&lt;/h3>
&lt;p>A short flight and we were on our second island.  At the airport I surprised Kelc by renting a Jeep for us to drive, her dream car, and it made our options of exploring Kauai a lot easier. &lt;/p>
&lt;p>The first picture I have, apart from the Jeep, was this Poke bowl.  We found this stand inside a grocery store, and it was probably the most delicious thing I’ve ever eaten – this was not the only time we ate this within our few days here.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/20181103_132824-768x1024.jpg"
loading="lazy"
alt="Poke bowl"
>&lt;/p>
&lt;p>This island felt totally different from Maui. Maui wasn’t that commercialised, but Kauai was even less so. We definitely felt like we were enjoying island life in all its glory.&lt;/p>
&lt;p>We love to hike and we were not disappointed. One of the days we took the Jeep up to Waimea Canyon and hiked several miles into these hidden waterfalls.  There was a lot of elevation gain and at some points there were helicopters flying in the canyon beneath us.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/20181104_112329-768x1024.jpg"
loading="lazy"
alt="Waimea Canyon waterfall"
>&lt;/p>
&lt;p>We were also lucky to see some Hawaiian Monk Seals. I love seals, so it was amazing to see them rock up on to the beach to rest in the sun for the day.  They are protected so when they do beach themselves volunteers run out and cordon them off so they are protected for the day.  When the sun sets they roll and wiggle down the beach and back into the ocean, prompting the volunteers to collect their signs and wait to see where they appear the next day. &lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/20181104_072245-768x1024.jpg"
loading="lazy"
alt="Monk seals resting in the sun"
>&lt;/p>
&lt;p>Our third excursion was an ATV trip through a historic sugar plantation. This was great- we got filthy and had a lot of fun touring around beautiful scenery that’s featured in many movies, including Jurassic Park, although we saw no dinosaurs.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/IMG-20181105-WA0081-769x1024.jpg"
loading="lazy"
alt="Muddy, driving an ATV"
>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Overall we both thoroughly enjoyed this holiday, a great mix of relaxation and adventure.  I think we would both say that Maui was our favourite of the two islands, but Kauai offered a great second perspective of what Hawaii is all about.&lt;/p>
&lt;p>I really enjoyed looking back over my Hawaii trip, and I hope you enjoyed this virtual holiday in one of the most beautiful spots in the world.&lt;/p>
&lt;h3 id="whats-next">What’s next?&lt;/h3>
&lt;p>If I was to write about my next dream holiday, it would be the Maldives. Back in June, when we had almost nothing to look forward to, we booked a 10 day holiday in the Maldives for January 16th, 2021. It felt like ages away, and surely things would be better by then. Well, if you’re reading this when it was posted, that’s next week, and it’s already been postponed.&lt;/p>
&lt;p>We’re now a couple of weeks into our third lockdown in the UK and we’re hoping we’ll get to take the trip in April now, fingers crossed.&lt;/p></description></item><item><title>Some 2021 Goals Guaranteed To Keep Me On Track</title><link>https://jpomfret.github.io/some-2021-goals-guaranteed-to-keep-me-on-track/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/some-2021-goals-guaranteed-to-keep-me-on-track/</guid><description>&lt;p>Well 2020 was certainly an interesting year. I’m pretty sure none of us could have predicted how the world would look now and it won’t be regarded as the best year ever. However, there were definitely still some bright points I’d like to make note of before setting some targets for 2021.&lt;/p>
&lt;h2 id="looking-back-on-2020">Looking back on 2020&lt;/h2>
&lt;p>One of my biggest achievements of the year was being awarded &lt;a class="link" href="https://mvp.microsoft.com/en-us/PublicProfile/5003655?fullName=Jess%20Pomfret" target="_blank" rel="noopener"
>MVP for Cloud and Datacenter Management&lt;/a> in February. I’m really proud of this and my ability to positively contribute to our data and PowerShell communities. Along with that I managed to publish 25 blog posts and added two videos to my &lt;a class="link" href="https://www.youtube.com/channel/UC0D3eA_natUs91YcuPE_tLg" target="_blank" rel="noopener"
>YouTube channel&lt;/a>.  I also gave 10 presentations, only one in person, and also sat on two diversity and inclusion panels. I’ve also continued to contribute to dbatools, dbachecks and other open source software.&lt;/p>
&lt;p>Outside of the tech world, it was a year of cancelled plans and lockdowns.  I will say this gave me and my wife plenty of time to explore our local footpaths. I recorded over 460 miles of walking last year. Something we usually wouldn’t have had time for.  I also cycled just over 700 miles after getting a new bike in June.&lt;/p>
&lt;h2 id="looking-forward-to-2021">Looking forward to 2021&lt;/h2>
&lt;p>Now, I’m not going to say 2021 is going to be back to normal, who knows if we’ll ever go back to that normal.  However, I’m hopeful that things will be better than 2020, and I hope sooner rather than later we can get back to travelling and actually meeting up with people in person!&lt;/p>
&lt;p>I’m going to set some targets to aim for that won’t depend on what 2021 looks like.&lt;/p>
&lt;ul>
&lt;li>Publish 26 blog posts – this would average to one every fortnight. I also aim to publish at a regular cadence.&lt;/li>
&lt;li>Create 10 YouTube videos – my short ‘life hack’ videos that I’ve published have got positive reviews. I aim to get more of those recorded and available on YouTube&lt;/li>
&lt;li>Speaking – I don’t have a specific number target here, but I’d like to think I can beat last year&amp;rsquo;s total of 10 presentations. Perhaps even some at in person events?!?&lt;/li>
&lt;li>Open source contributions – Again, no specific numbers here but just a goal to continue contributing. I also have some ideas for this area that need a little love and time, but I hope to work on those this year as well.&lt;/li>
&lt;li>Cycle 1000 miles – this would be more than I’ve ever cycled in a year, but without a commute in 2020 I found a lot more time for cycling before/after work.&lt;/li>
&lt;li>Duolingo streak – another non tech goal. I’ve been using Duolingo to work on my French and hope to keep the streak alive throughout 2021.&lt;/li>
&lt;/ul>
&lt;p>All in all, I hope 2021 is an improvement on 2020 – here’s to doing everything I can to make it a good one. I&amp;rsquo;ll leave you with a couple of local views I&amp;rsquo;ve enjoyed while cycling\walking this past year.&lt;/p>
&lt;p>&lt;a class="link" href="https://www.instagram.com/p/CDhAVEXBjLq/?utm" target="_blank" rel="noopener"
>https://www.instagram.com/p/CDhAVEXBjLq/?utm&lt;/a>_source=ig_web_copy_link&lt;/p>
&lt;p>&lt;a class="link" href="https://www.instagram.com/p/CAP8ta7huch/?utm" target="_blank" rel="noopener"
>https://www.instagram.com/p/CAP8ta7huch/?utm&lt;/a>_source=ig_web_copy_link&lt;/p></description></item><item><title>Advent of Code 2020</title><link>https://jpomfret.github.io/advent-of-code-2020/</link><pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/advent-of-code-2020/</guid><description>&lt;p>This was the 3rd year I participated in the &lt;a class="link" href="https://adventofcode.com/" target="_blank" rel="noopener"
>Advent of Code&lt;/a> (AoC). If you haven’t heard of AoC, it’s an advent calendar of coding puzzles.  Each day between December 1st and 25th a two part puzzle is released, you can use whatever language you want to solve it, with the goal being just to get the right answer.  Once you solve part 1 for the day, part 2 is unlocked and that builds on top of the story you had for part 1. For each part of each puzzle you complete you get a star, so there are two available per day.&lt;/p>
&lt;p>This year I managed to complete both parts of the first 9 days of the calendar, and then just the first part of days 10 and 15, that’s 20 total stars out of a possible 50.  That doesn’t sound great, less than 50%, so why am I writing a blog about this mediocre performance?&lt;/p>
&lt;p>My goal was to gain more stars than last year, which I succeeded at. I only got 6 total stars last year. Now my goal for next year will be to beat this year&amp;rsquo;s performance.  I did learn several neat things while working on these puzzles and those I thought were worth sharing.&lt;/p>
&lt;h2 id="named-loops-in-powershell">&lt;strong>Named Loops in PowerShell&lt;/strong>&lt;/h2>
&lt;p>A lot of the puzzles involve iterating over an object and manipulating it. I depended on a lot of loops for this. My day 1, part 1 solution is below.  You can see I nested two loops to iterate over the array and calculate the total. Without the named loops this worked – it just didn’t stop when it found the correct answer and I got duplicates.  By naming the outer loop with &lt;code>:expenses&lt;/code> you can then break all the way out of that loop with &lt;code>break expenses&lt;/code>.  Pretty useful!&lt;/p>
&lt;p>$expenses = Get-Content .\Day01\Input.txt&lt;/p>
&lt;h1 id="part-1---514579">Part 1 - 514579&lt;/h1>
&lt;p>:expenses
foreach ($e in $expenses) {
foreach ($f in $expenses) {
if ([int]$e + [int]$f -eq 2020) {
(&amp;ldquo;Part 1 answer: {0}&amp;rdquo; -f ([int]$e * [int]$f))
break expenses
}
}
}&lt;/p>
&lt;h2 id="split-string-into-multiple-variables-at-once">&lt;strong>Split string into multiple variables at once&lt;/strong>&lt;/h2>
&lt;p>A lot of the puzzles input required some string manipulation. Some of this I used regex for, and if it wasn’t that complicated I could use the split method.  Previously I had always split strings into variables by first splitting into an array, and then specifying the index of the item for each variable:&lt;/p>
&lt;p>$split = (&amp;rsquo;test string&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;)
$firstVariable = $split[0]
$secondVariable = $split[1]&lt;/p>
&lt;p>PowerShell has an easier option though- you can accomplish this same behaviour with just one line:&lt;/p>
&lt;p>$firstVariable, $secondVariable = (&amp;rsquo;test string&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;)&lt;/p>
&lt;h2 id="split-a-string-into-a-maximum-number-of-substrings">&lt;strong>Split a string into a maximum number of substrings&lt;/strong>&lt;/h2>
&lt;p>Another string splitting tip is if you only want to split a certain number of times there is an overload for the string method for that. I have used the split method for years, but I have never looked any further into what it can do.  A handy reminder that reading the docs for even simple methods/functions is a worthwhile endeavour (perhaps a 2021 goal?!?).  &lt;/p>
&lt;p>(&amp;lsquo;I only want two substrings&amp;rsquo;).split(&amp;rsquo; &amp;lsquo;,2)&lt;/p>
&lt;p>This results in:&lt;/p>
&lt;p>I
only want two substrings.&lt;/p>
&lt;h2 id="im-not-a-computer-scientist">&lt;strong>I’m not a Computer Scientist&lt;/strong>&lt;/h2>
&lt;p>The final thing I learnt is that I’m not a computer scientist.  The first few days of puzzles were pretty straightforward – I had no problem working out what was needed and writing a solution.  Was it the most effective and beautiful code ever, probably not, but it got the right answer and that was all we needed.  Once we got into the second week the difficulty picked up- I didn’t study maths or computer science and found I was severely lacking when it came to needing more complicated algorithms to solve the puzzles.&lt;/p>
&lt;p>That’s ok though. Although it’s definitely a gap in knowledge when it comes to solving code puzzles, it hasn’t really caused problems or issues in my day to day work. I’m still able to use PowerShell to automate tasks and manage a large database estate.&lt;/p>
&lt;p>Saying that, I am interested in learning more about these topics. I love a puzzle and the Advent of Code is a great way to finish the year with some challenges and learning.&lt;/p>
&lt;p>If you&amp;rsquo;re interested in my efforts, all of my code is on &lt;a class="link" href="https://github.com/jpomfret/AdventOfCode2020" target="_blank" rel="noopener"
>Github&lt;/a>.&lt;/p></description></item><item><title>T-SQL Tuesday #133: What (Else) Have You Learned from Presenting?</title><link>https://jpomfret.github.io/t-sql-tuesday-#133-what-else-have-you-learned-from-presenting/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#133-what-else-have-you-learned-from-presenting/</guid><description>&lt;p>&lt;a class="link" href="https://lisagb.info/archives/77" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>It’s December, the last T-SQL Tuesday for 2020. I’ve managed to participate in seven this year, including hosting in February – I wonder if in 2021 I will be able to complete a full year. I always look forward to these monthly blog parties, so thanks to Lisa for hosting this month. &lt;/p>
&lt;p>Lisa has asked us to share something we’ve learnt from presenting that didn’t relate directly to the topic we were presenting on.  I think this is a great topic. We already know that to present on a topic you have to really know it, so preparing for a presentation does wonders for your own personal knowledge on that topic.  Lisa has identified another bonus- all the ancillary knowledge that comes along with it.&lt;/p>
&lt;p>I gave my first presentation in June 2018, and although it’s only been around 2.5 years since then, I have learnt so much from both presenting and contributing to the community.  I have a couple of areas I’m going to mention here.&lt;/p>
&lt;h2 id="git--github">&lt;strong>Git &amp;amp; GitHub&lt;/strong>&lt;/h2>
&lt;p>My first real contributions were not presenting but writing code and tests for dbatools.  Writing PowerShell was not something that was new to me, so I felt comfortable writing the function I wanted to add. Through code review it did get some much needed tweaking to ensure it met the standards and format of the project.  Getting the code from my laptop into the dbatools GitHub repo was another story – it was totally foreign to me.&lt;/p>
&lt;p>Luckily the dbatools team is fantastic. They have written great guides on using git and GitHub in order to make your first pull request, and Chrissy even coached me through how to get my code into GitHub that first time. &lt;/p>
&lt;p>As I continued to contribute I got more familiar with using Git and GitHub for source control, and since then there have been many times where I’ve needed that knowledge both for community projects as well as my real job. I even host my &lt;a class="link" href="https://github.com/jpomfret/demos" target="_blank" rel="noopener"
>presentation demos and slides&lt;/a> there now.&lt;/p>
&lt;h2 id="docker">&lt;strong>Docker&lt;/strong>&lt;/h2>
&lt;p>I think one thing that every presenter has had to learn is how to set up some form of a lab to be able to run demos in a reliable and repeatable way.  I started off with a VM on my local laptop that I could connect to and run demos against. This took some learning to get the networking setup. Something I still consider a weak area!&lt;/p>
&lt;p>After a while I started reading more about containers, and specifically running SQL Server in containers.  I now (where possible) run all my demos for presentations against demo environments in containers that run on my laptop. I have already written a post on this, &lt;a class="link" href="https://jesspomfret.com/data-compression-containers/" target="_blank" rel="noopener"
>Data Compression Demos in Containers&lt;/a>, but this has given me a great opportunity to learn and play with a really interesting technology.&lt;/p>
&lt;p>One of the main benefits I see when running demos against containers is how easy it is to wipe away your practice runs and have a fresh environment ready for the presentation.  I’ve even built pester tests into my setup script to ensure everything is in the perfect state for the demo gods. I’ve also written about that if you’re interested in what I test, &lt;a class="link" href="https://jesspomfret.com/demo-gods-pester/" target="_blank" rel="noopener"
>Keeping the demo gods at bay with Pester&lt;/a>.&lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>It took me a long time to make the first step into giving back and presenting content in front of people, but if I look back at the fun I’ve had over the last 2.5 years and all the knowledge I’ve gained it’s easy to see what a great decision that was. If you are considering speaking I would highly recommend giving it a go – as Lisa mentioned in her prompt, Allen White’s famous speech, everyone has something to teach.&lt;/p>
&lt;p>Thanks again for hosting Lisa, and I look forward to reading all the other responses.&lt;/p></description></item><item><title>dbachecks and Azure SQL Databases</title><link>https://jpomfret.github.io/dbachecks-and-azure-sql-databases/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbachecks-and-azure-sql-databases/</guid><description>&lt;p>Last week I gave a presentation at &lt;a class="link" href="https://www.meetup.com/SQL-South-West/" target="_blank" rel="noopener"
>Data South West&lt;/a> on dbachecks and dbatools. One of the questions I got was whether you could run dbachecks against Azure SQL Databases, to which I had no idea. I always try to be prepared for potential questions that might come up, but I had only been thinking about on-premises environments and hadn’t even considered the cloud.  The benefit is this gives me a great topic for a blog post.&lt;/p>
&lt;h2 id="step-1--create-an-azure-sql-database">&lt;strong>Step 1 – Create an Azure SQL Database&lt;/strong>&lt;/h2>
&lt;p>I created a SQL Database through the &lt;a class="link" href="https://portal.azure.com/" target="_blank" rel="noopener"
>Azure Portal&lt;/a>. The wizard is pretty straightforward and the only real decisions needed were around sizing. Since this is just going to be for a test environment I chose a small ‘Basic’ database.&lt;/p>
&lt;h2 id="step-2--connect-to-the-database">&lt;strong>Step 2 – Connect to the Database&lt;/strong>&lt;/h2>
&lt;p>Once the database had been created I navigated to the resource pane in the portal. At the top there is a drop down that helps you get connected using Azure Data Studio.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Connect-1024x203.jpg"
loading="lazy"
alt="Azure SQL Database pane in Azure Portal"
>&lt;/p>
&lt;p>Once Azure Data Studio opened, I was asked to confirm I wanted to connect:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ConnectSure.jpg"
loading="lazy"
alt="Confirmation prompt for connection"
>&lt;/p>
&lt;p>Then a pane opened which enabled me to easily add a firewall rule so client IP could access the Azure SQL Database.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/firewall.jpg"
loading="lazy"
alt="Pane in ADS to configure a firewall rule"
>&lt;/p>
&lt;p>Once that was completed I was connected through Azure Data Studio and able to interact with my server and database.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ADSConnected-1024x382.jpg"
loading="lazy"
alt="ADS connected to Azure SQL Database"
>&lt;/p>
&lt;p>Connecting first through Azure Data Studio is not a requirement, but it does help us to get the firewall rules configured and confirm that connecting from our client machine will not be an issue.&lt;/p>
&lt;p>Another good check to ensure we can connect to our database from PowerShell is to use dbatools’ &lt;code>Connect-DbaInstance&lt;/code>:&lt;/p>
&lt;p>$cred = Get-Credential
Connect-DbaInstance -SqlInstance &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo; -SqlCredential $cred&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/connectDbatools-1024x90.jpg"
loading="lazy"
alt="Connect-DbaInstance results"
>&lt;/p>
&lt;p>The results show we were able to connect successfully.&lt;/p>
&lt;h2 id="step-3--run-some-dbachecks">&lt;strong>Step 3 – Run some dbachecks&lt;/strong>&lt;/h2>
&lt;p>First of all let’s run a single check to ensure our database is online and in the expected state. For this we can use the ‘DatabaseStatus’ check.&lt;/p>
&lt;p>$checkSplat = @{
SqlInstance = &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo;
SqlCredential = $cred
Check = &amp;lsquo;DatabaseStatus&amp;rsquo;
}
Invoke-DbcCheck @checkSplat&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/databaseStatus-1-1024x210.jpg"
loading="lazy"
alt="DatabaseStatus checks succeed"
>&lt;/p>
&lt;p>Here you can easily see, both because of the green result text and plus icon to the left, that our tests were successful. Both the database we created, AzDb01, and the master database are online and have the expected status.&lt;/p>
&lt;p>dbachecks uses tags on the pester tests to enable you to either call specific tests or groups of checks. Each check has a unique tag. In our previous example it was &lt;code>DatabaseStatus&lt;/code> as well as tags that group like checks, for example &lt;code>Database&lt;/code>.&lt;/p>
&lt;p>$checkSplat = @{
SqlInstance = &amp;lsquo;xxxxxx.database.windows.net&amp;rsquo;
SqlCredential = $cred
Check = &amp;lsquo;Database’
}
Invoke-DbcCheck @checkSplat&lt;/p>
&lt;p>Running all the database checks against our Azure SQL Database we get some failures.&lt;/p>
&lt;p>Tests completed in 70.68s
Tests Passed: 37, Failed: 28, Skipped: 8, Pending: 0, Inconclusive: 0&lt;/p>
&lt;p>There are a lot of tests that pass or fail with valid reasons. However, some of the failures are due to errors running the check. These are to be expected since this is a PaaS (Platform as a Service) database offering. One example is the suspect pages check.&lt;/p>
&lt;p>The test failed due to an error in the context block, and it clearly states that the &amp;lsquo;msdb.dbo.suspect_pages&amp;rsquo; table isn’t available in this version of SQL Server.&lt;/p>
&lt;p>SqlException: Reference to database and/or server name in &amp;lsquo;msdb.dbo.suspect_pages&amp;rsquo; is not supported in this version of SQL Server.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FailedSuspectPages-1024x192.jpg"
loading="lazy"
alt="dbachecks output fails for suspect pages"
>&lt;/p>
&lt;p>There are plenty of tests that do work against an Azure SQL Database though, allowing you to keep tabs on many different aspects of your database including:&lt;/p>
&lt;ul>
&lt;li>Database Collation&lt;/li>
&lt;li>Database Owners&lt;/li>
&lt;li>Column Identity Usage&lt;/li>
&lt;li>Duplicate Index&lt;/li>
&lt;li>Disabled Index&lt;/li>
&lt;li>Auto Shrink&lt;/li>
&lt;li>Database Orphaned User&lt;/li>
&lt;li>Compatibility Level&lt;/li>
&lt;li>Database Status&lt;/li>
&lt;li>Database Exists&lt;/li>
&lt;li>And more…&lt;/li>
&lt;/ul>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>So to answer the question: yes, we can run dbachecks against our Azure SQL Databases. As long as we can connect and the version of SQL Supports the features needed to run the test we can ensure our databases in the cloud are configured just how we like them.&lt;/p></description></item><item><title>Ensure Query Store meets best practice across your environment</title><link>https://jpomfret.github.io/ensure-query-store-meets-best-practice-across-your-environment/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/ensure-query-store-meets-best-practice-across-your-environment/</guid><description>&lt;p>It’s no secret that I love dbatools and dbachecks. I am certain that I run a dbatools command at least once a day. It has fundamentally changed how I work as a dba, and it makes my life so much easier.  I mention this when I’m presenting on these topics, but today I want to highlight what I consider the special sauce of open-source software.&lt;/p>
&lt;p>dbatools not only provides us with easy to run functions that get/set/test so many aspects of our environments, but it also encapsulates the knowledge of industry experts, and that right there is part of the magic. For example, I no longer have to remember &lt;a class="link" href="https://www.sqlskills.com/blogs/jonathan/how-much-memory-does-my-sql-server-actually-need/" target="_blank" rel="noopener"
>Jonathan Kehayias’ detailed calculations&lt;/a> for max memory. I can just run &lt;a class="link" href="https://docs.dbatools.io/#Test-DbaMaxMemory" target="_blank" rel="noopener"
>Test-DbaMaxMemory&lt;/a> to check whether I need to adjust my settings.&lt;/p>
&lt;p>I would like to just say I’m in no way suggesting that we can skip the learning here, reading posts from experts and understanding why is vital – just it’s nice to be able to quickly call this knowledge from PowerShell rather from the depths of my brain (and that’s hoping it’s still stored in there).&lt;/p>
&lt;h2 id="adding-query-store-expertise-to-dbatools">&lt;strong>Adding Query Store Expertise to dbatools&lt;/strong>&lt;/h2>
&lt;p>Last week I was working on configuring Query Store, and knowing that Erin Stellato (&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/erinstellato" target="_blank" rel="noopener"
>t&lt;/a>) is the expert on that I headed over to her blog. I found exactly what I needed. Erin has a bunch of great posts on query store, but this one caught my eye: &amp;lsquo;&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/query-store-best-practices" target="_blank" rel="noopener"
>Query Store Best Practices&lt;/a>&amp;rsquo;.&lt;/p>
&lt;p>I read through her suggestions and could easily translate those using dbatools to optimally configure Query Store. First by setting several query store options using &lt;code>Set-DbaDbQueryStoreOption&lt;/code>:&lt;/p>
&lt;p>$queryStoreBP = @{
SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
Database = &amp;lsquo;TestDb&amp;rsquo;
State = &amp;lsquo;ReadWrite&amp;rsquo;
MaxSize = 2048
CaptureMode = &amp;lsquo;Auto&amp;rsquo;
CollectionInterval = 30
}
Set-DbaDbQueryStoreOption @queryStoreBP&lt;/p>
&lt;p>Secondly by configuring two trace flags using &lt;code>Set-DbaStartupParameter&lt;/code> (reboot required):&lt;/p>
&lt;p>Set-DbaStartupParameter -SqlInstance mssql1 -TraceFlag 7745,7752&lt;/p>
&lt;p>Once I was happy with my settings, I realised we were missing a ‘test’ command for dbatools. The suite of ‘test’ functions in dbatools (a lot that end up as checks in dbachecks btw!), give us an easy way to check our environment against best practices, or our desired settings.&lt;/p>
&lt;p>Since dbatools is open-source I was able to write this function (&lt;a class="link" href="https://docs.dbatools.io/#Test-DbaDbQueryStore" target="_blank" rel="noopener"
>Test-DbaDbQueryStore&lt;/a>) and get it added into the module. It’s included as of version 1.0.131, so make sure you’re up to date.  Taking Erin’s suggestions and wrapping them in a little PowerShell, I can make it easier for myself and everyone else to make sure we’re following her guidelines.&lt;/p>
&lt;p>To test a single database you can use the following. It will output each setting, it’s current value, the recommended value, as well as a note from Erin’s blog post on why we should choose that. Again, I recommend you read her post to fully understand the why.&lt;/p>
&lt;p>Test-DbaDbQueryStore -SqlInstance mssql1 -Database testdb&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/querystoreBP.jpg"
loading="lazy"
alt="Sample output from Test-DbaDbQueryStore showing a couple of best practices"
>&lt;/p>
&lt;p>This also means we can find any databases across many instances that aren’t set up to meet best practices:&lt;/p>
&lt;p>$results = Test-DbaDbQueryStore -SqlInstance mssql1, mssql2 |
Where-Object {-not $_.IsBestPractice} |
Select-Object SqlInstance, Database, Name, Value, RecommendedValue
$results | Format-Table&lt;/p>
&lt;h2 id="so-over-to-you">So, over to you&lt;/h2>
&lt;p>Step 1 – Go and read the why - &amp;lsquo;&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/query-store-best-practices" target="_blank" rel="noopener"
>Query Store Best Practices&lt;/a>&amp;rsquo;.&lt;/p>
&lt;p>Step 2 – Easily make sure your environment is up to par.&lt;/p></description></item><item><title>T-SQL Tuesday #132: How Are You Coping with Pandemic?</title><link>https://jpomfret.github.io/t-sql-tuesday-#132-how-are-you-coping-with-pandemic/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#132-how-are-you-coping-with-pandemic/</guid><description>&lt;p>&lt;a class="link" href="https://sqlworldwide.com/t-sql-tuesday-132-how-are-you-coping-with-pandemic/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>This month’s blog party is hosted by Taiob Ali (&lt;a class="link" href="https://sqlworldwide.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SqlWorldWide" target="_blank" rel="noopener"
>t&lt;/a>), and they ask how are we coping with the pandemic. Last week England entered its second national lockdown, which is slated to last until December 2nd, so now more than ever is a good time to reflect on some effective coping strategies.  Thanks Taiob for hosting and I’m looking forward to reading everyone else’s mechanisms.&lt;/p>
&lt;p>In the prompt it was suggested we could break this post down into three buckets: mental health, physical health and professional growth. Three really important areas so I’ll do just that.&lt;/p>
&lt;h2 id="physical-health">&lt;strong>Physical Health&lt;/strong>&lt;/h2>
&lt;p>Changing the order slightly, I’m going to start with physical health – as it’s easier for me to talk about. &lt;/p>
&lt;p>Fitness and being active has always been a big part of my life.  I played football (soccer) from the time I could walk until I graduated college in 2009. After that my fitness floundered until around 2011 when a friend encouraged me to try CrossFit. &lt;/p>
&lt;p>CrossFit gets a bad rap in some circles, but my experiences with the sport have been almost all positive.  I know that when I show up to class someone is going to tell me exactly what the plan for the day is and then I’m going to get after it with a group of people from every corner of life.  This is perfect for me, it feels like I’m playing a team sport again, and the workouts are varied and challenging.&lt;/p>
&lt;p>In the UK, gyms were closed during the first national lockdown from March until almost the end of July. Our gym was kind enough to let members checkout some equipment and still posted daily workouts that we could complete at home.  This was great, except I struggled to stay motivated without the structure of going to the gym for a class. I started to skip workouts, which made me feel worse about the whole situation (physically and mentally).&lt;/p>
&lt;p>When gyms were able to re-open on July 25th things felt a lot easier for me, even though there were still a lot of restrictions in place. To start with I was really anxious about whether it was going to be safe, but the gym did a fantastic job of implementing COVID measures to enable us to workout safely.  My wife and I figured out a new routine, with me working from home we were able to go to the gym at lunch most days, and I almost immediately started feeling more in shape and healthier again. We found a new ‘normal’ that worked pretty well for us.&lt;/p>
&lt;p>Then November 5th happened. England has now entered lockdown 2 and gyms are back to being closed. Add in another factor, sunset today is 4:29pm! The days are getting shorter and it’s dark after work which is going to make it harder to stay motivated.  It’s been a bit of a ramble up until now, but this is where my coping strategies come in!&lt;/p>
&lt;p>My strategy is just to do something. I know that I feel better when I’m active so I just need to dig in and do it, easier said than done. I’m going to split this into three parts:&lt;/p>
&lt;p>1.      Lunchtime workouts&lt;/p>
&lt;p>I’m unbelievably lucky that my wife is the most motivated person ever and I know she’ll work out most days. I just need to stick with her and I’ll be ok. My goal is to workout, bike, or walk on lunch as many days as possible.&lt;/p>
&lt;p>2.      Yoga (3-4x a week)&lt;/p>
&lt;p>I also want to add in some yoga. My flexibility is shockingly bad and I can easily do yoga inside which will be a good after work activity.&lt;/p>
&lt;p>3.      Plank Challenge&lt;/p>
&lt;p>I saw a tweet from Kendra Little last week on her fitness plans for lockdown and she included working on planks every day. I am also terrible at planks so I’m going to jump in on this and join her with a plank a day.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/Kendra" target="_blank" rel="noopener"
>https://twitter.com/Kendra&lt;/a>_Little/status/1323192706064142336&lt;/p>
&lt;p>With a bit of a plan under my belt I hope that I can stay more active this time around and I know it’ll make everything else associated with this lockdown easier.&lt;/p>
&lt;h2 id="mental-health">&lt;strong>Mental Health&lt;/strong>&lt;/h2>
&lt;p>The second topic is something that I (and I think a lot of people) find harder to talk about. This year has been tough (understatement!) as my wife and I uprooted our lives in Ohio and moved to England in January.  I had a great career opportunity and we decided to take the leap, with the biggest pros being the opportunity to travel around Europe more easily and to be able to spend time with my family, neither of which have really worked out.&lt;/p>
&lt;p>Instead we had just a couple of months of that plan before we were locked down, unable to travel and unable to even visit my family for several months. We now lived in a place where we knew basically no-one, perfect for a global pandemic.&lt;/p>
&lt;p>However, we managed to make the most of what we did have available.  The rules during the first lockdown were that we could leave our house 1 time per day for exercise, so we did a lot of walking.  We are lucky that where we live there are miles of public footpaths through beautiful English countryside.  We had fun making new routes and mostly being able to follow them (I have zero sense of direction).&lt;/p>
&lt;p>&lt;a class="link" href="https://www.instagram.com/p/B" target="_blank" rel="noopener"
>https://www.instagram.com/p/B&lt;/a>_AuvMuBsal/?utm_source=ig_web_copy_link&lt;/p>
&lt;p>We also watched lots of movies. I’ve probably watched more movies in 2020 than I have in the rest of my life all together. It was nice to have some time to do that.&lt;/p>
&lt;p>The coping strategy here is to simply make the most of what you have. I will work on being grateful for the things I do have available and overall try and stay positive.&lt;/p>
&lt;p>&lt;strong>Professional Growth&lt;/strong>&lt;/p>
&lt;p>One benefit of this global pandemic is I have more time on my hands. Without the commute to work everyday I get almost 2 hours back in my day.  Also my calendar has never been this low on social events, so there is free time everywhere! Have I put this to good use and worked hard in this professional growth category? Not really, no. &lt;/p>
&lt;p>I’ve found it really hard to be motivated to blog, something I’d previously worked hard on making more consistent. I have made a couple of short videos and spoken at a couple of virtual conferences, but overall I’ve been struggling to get content out.&lt;/p>
&lt;p>I did manage to find some motivation during &lt;a class="link" href="https://hacktoberfest.digitalocean.com/" target="_blank" rel="noopener"
>Hacktoberfest&lt;/a>, which is a month-long party to get people contributing to open source projects.  I submitted some PRs and was reminded how much fun it is to fix bugs and write new dbatools commands.&lt;/p>
&lt;p>My coping strategy here is to ride this renewed excitement and get some content created.  I have a whole list of ideas, I just need to set aside some time and work on them. &lt;/p>
&lt;h2 id="summary">&lt;strong>Summary&lt;/strong>&lt;/h2>
&lt;p>This has turned into quite the ramble, so thanks for reading if you’re still with me! To sum it up, I know I need to stay active – this is really the base for everything else. If I workout I feel better about everything else. It’s as simple as that.&lt;/p>
&lt;p>I know that this year has been tough for everyone. I hope some of these strategies will be useful to other people too.  Feel free to reach out to me on Twitter if there is anything I can do to help. We can get through this together.&lt;/p></description></item><item><title>Pester test your Cluster Role Owners</title><link>https://jpomfret.github.io/pester-test-your-cluster-role-owners/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/pester-test-your-cluster-role-owners/</guid><description>&lt;p>In an ideal situation it probably shouldn’t matter which node of a failover cluster your resources and roles are hosted on, but the real world is often far from ideal.  This post will talk through how we can record the current owner nodes and then use Pester to ensure we’re in the ideal configuration. This could be useful post maintenance activities or as a daily check to ensure things are as you expect.&lt;/p>
&lt;h2 id="step-1--store-the-current-resource-owners">&lt;strong>Step 1 – Store the current resource owners&lt;/strong>&lt;/h2>
&lt;p>If we are going to test that we’re in our expected configuration, we need to record what that configuration looks like.  I have a hard coded list of cluster names. However, you could easily pull them from a text file, or a database.  Once we have the list of clusters we can use &lt;code>Get-ClusterGroup&lt;/code> to determine the cluster roles and their current owners.&lt;/p>
&lt;p>To persist this owner information I’m using &lt;code>ConvertTo-Json&lt;/code> and then outputting it to a file. This creates a file that can easily be read back into PowerShell as an object using &lt;code>ConvertFrom-Json&lt;/code>.&lt;/p>
&lt;p>It’s also probably worth mentioning that this ideal configuration can be stored in source control. That’ll keep the file safe and you can easily keep track of any changes that are made to it.&lt;/p>
&lt;p>$clusters = ‘ClusterName1’,’ClusterName2’
$owners = $clusters | % { Get-ClusterGroup -Cluster $PSItem | select Cluster, Name, State, OwnerNode }
$owners | % {
[PSCustomObject] @{
Cluster = $_.Cluster.Name
Name = $_.Name
OwnerNode = $_.OwnerNode.Name
State = $_.State -as [string]
}
} | ConvertTo-Json | Out-File ClusterGroupOwners.json&lt;/p>
&lt;p>You’ll notice I’m creating a &lt;code>PSCustomObject&lt;/code> to pipe to the &lt;code>ConvertTo-Json&lt;/code>. Without that, the object from &lt;code>Get-ClusterGroup&lt;/code> is exploded, with all properties, including nested properties exported into the JSON output. This is more than we need, and I think there is some value in having a clear concise output file. &lt;/p>
&lt;p>I’m also using &lt;code>-as [string]&lt;/code> on the state property. PowerShell automatically translates the real state to a text value when outputted as it’s an enumeration type – but when you pipe that to &lt;code>ConvertTo-Json&lt;/code> you get the raw integer value.&lt;/p>
&lt;h2 id="step-2--test-the-current-configuration">&lt;strong>Step 2 – Test the current configuration&lt;/strong>&lt;/h2>
&lt;p>When it’s time to test our configuration we can read in our ClusterGroupOwners.json and then convert it back to a PowerShell object using &lt;code>ConvertFrom-Json&lt;/code>.  Now we have a PowerShell object of our ideal configuration we can loop through each cluster, checking the current group owners using &lt;code>Get-ClusterGroup&lt;/code> again.  This current state can then be matched against the desired configuration.&lt;/p>
&lt;p>I am using a pretty simple pester test for this work, saving it as Check-ClusterOwners.tests.ps1.&lt;/p>
&lt;p>$desiredConfig = Get-Content ClusterGroupOwners.json | ConvertFrom-Json
$clusters = $desiredConfig | Select -Unique Cluster&lt;/p>
&lt;p>Describe &amp;lsquo;The cluster resources should be owned by the same node as before&amp;rsquo; -Tag ClusterOwner {
Foreach ($cls in $clusters) {
Context (&amp;lsquo;Cluster owners are the same for {0}&amp;rsquo; -f $cls.Cluster) {
$groups = $desiredConfig | Where-Object Cluster -eq $cls.Cluster
$currentOwner = Get-ClusterGroup -Cluster $cls.Cluster
foreach ($grp in $groups) {
It (&amp;rsquo;{0} should be owned by {1}&amp;rsquo; -f $grp.Name, $grp.OwnerNode) {
($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode
}
}
}
}
}&lt;/p>
&lt;p>We’ll call this test using &lt;code>Invoke-Pester .\Check-ClusterOwners.tests.ps1&lt;/code>.&lt;/p>
&lt;p>If everything is as expected we’ll get output similar to this for each cluster – depending on the resources you have set up in your cluster.&lt;/p>
&lt;p>Describing The cluster resources should be owned by the same node as before
Context Cluster owners are the same for clustername
[+] RoleName should be owned by nodename 56ms
[+] RoleName2 should be owned by nodename 92ms&lt;/p>
&lt;p>If you have a resource that is not on the node that is expected, you’ll easily be able to see that in the output:&lt;/p>
&lt;p>Context Cluster owners are the same for clusterName
[-] RoleName should be owned by NodeB 102ms
Expected strings to be the same, but they were different.
String lengths are both 13.
Strings differ at index 12.
Expected: &amp;lsquo;NodeB&amp;rsquo;
But was: &amp;lsquo;NodeA&amp;rsquo;
13: ($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode&lt;/p>
&lt;p>This method of testing can be useful to ensure you’re in the ideal state in many scenarios. For example you could store any databases in your estate that are not ‘online’ and then confirm post reboots/patching that all the databases are in the expected state.&lt;/p></description></item><item><title>T-SQL Tuesday #130 – Automate Your Stress Away</title><link>https://jpomfret.github.io/t-sql-tuesday-#130-automate-your-stress-away/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#130-automate-your-stress-away/</guid><description>&lt;p>&lt;a class="link" href="https://sqlzelda.wordpress.com/2020/09/01/t-sql-tuesday-130-automate-your-stress-away/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>Thanks to Elizabeth Nobel (&lt;a class="link" href="https://sqlzelda.wordpress.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SQLZelda" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month’s T-SQL Tuesday party and apologies for being as late as possible to the party! I love the topic of automation so felt sure I’d write something and then time slipped away. Luckily Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/MikeyBronowski" target="_blank" rel="noopener"
>t&lt;/a>) convinced me that it wasn’t too late to write something on my lunch break today (Wednesday in the UK) as it’s still Tuesday on Baker Island. Interesting fact Baker Island uses UTC-12:00 because since it’s uninhabited the islands time zone is unspecified (&lt;a class="link" href="https://en.wikipedia.org/wiki/Baker_Island" target="_blank" rel="noopener"
>Wikipedia&lt;/a>).&lt;/p>
&lt;h2 id="automating-dbachecks-with-scheduled-task">Automating dbachecks with scheduled task&lt;/h2>
&lt;p>I wanted to write about automating your daily checks with dbachecks, there are many ways of expanding on this post, but this should give you a good basis to build from.&lt;/p>
&lt;h3 id="the-environment">The environment&lt;/h3>
&lt;p>I have two docker containers running on my laptop, one running SQL Server 2017 and one running SQL Server 2019. I will use these SQL Server instances to run my sample daily checks against.&lt;/p>
&lt;p>I have also created a database on the 2019 instance (mssql2) called dbachecks to store our daily check results.&lt;/p>
&lt;h3 id="the-checks">The checks&lt;/h3>
&lt;p>There are hundreds of checks available within the dbachecks module, and on top of that you can even &lt;a class="link" href="https://nocolumnname.blog/2018/02/22/adding-your-own-checks-to-dbachecks/" target="_blank" rel="noopener"
>write your own&lt;/a> and include those. For this example I’m going to use the ‘DatabaseStatus’ check to ensure all my databases are online as expected.&lt;/p>
&lt;h3 id="the-automation">The automation&lt;/h3>
&lt;p>To automate the running of our daily checks we’ll first create a PowerShell script and then schedule that using task scheduler.  If you have other enterprise scheduling tools available you could easily use those instead to invoke the PowerShell script.&lt;/p>
&lt;p>The script for my example, shown below, is pretty simple. I have a section to define where the data will be stored (the ability to save dbachecks result information straight into a database was introduced with dbachecks 2.0 and so I would highly recommend updating if you’re on an earlier version).&lt;/p>
&lt;p>The next section (lines 7-9) lists my SQL instances that I want to check, and the checks that should be run.  The list of SQL instances could easily be pulled from a text file, a central management server (CMS) or a database to enhance the script.&lt;/p>
&lt;p>The final three lines (lines 11-13) run the checks, apply a label of ‘MorningChecks’ (this allows for grouping of test results in the reports) and then inserts the results into the database.&lt;/p>
&lt;p>Import-Module dbachecks, dbatools&lt;/p>
&lt;h2 id="dbachecks-database-connection">Dbachecks Database Connection&lt;/h2>
&lt;p>$dbachecksServer = &amp;lsquo;mssql2&amp;rsquo;
$dbachecksDatabase = &amp;lsquo;dbachecks&amp;rsquo;&lt;/p>
&lt;h2 id="define-instances-and-checks-to-run">Define instances and checks to run&lt;/h2>
&lt;p>$SqlInstance = &amp;lsquo;mssql1&amp;rsquo;,&amp;lsquo;mssql2&amp;rsquo;
$checks = &amp;lsquo;DatabaseStatus&amp;rsquo;&lt;/p>
&lt;p>Invoke-DbcCheck -SqlInstance $SqlInstance -Checks $checks -PassThru |
Convert-DbcResult -Label &amp;lsquo;MorningChecks&amp;rsquo; |
Write-DbcTable -SqlInstance $dbachecksServer -Database $dbachecksDatabase&lt;/p>
&lt;p>I saved this script to &lt;code>C:\dbachecks\dbachecks.ps1&lt;/code> and then ran the following PowerShell to schedule the execution of the script daily at 7am.&lt;/p>
&lt;p>$RunAs = Get-Credential
$taskSplat = @{
TaskName = &amp;lsquo;Daily dbachecks&amp;rsquo;
Action = (New-ScheduledTaskAction -Execute &amp;lsquo;powershell&amp;rsquo; -Argument &amp;lsquo;-File dbachecks.ps1&amp;rsquo; -WorkingDirectory C:\dbachecks)
Trigger = (New-ScheduledTaskTrigger -Daily -At &amp;lsquo;07:00&amp;rsquo;)
User = $RunAs.UserName
Password = ($RunAs.GetNetworkCredential().Password)
RunLevel = &amp;lsquo;Highest&amp;rsquo;
}
Register-ScheduledTask @taskSplat&lt;/p>
&lt;p>It’s important to note that the account used to run this scheduled task needs to be an account that has access to all of the SQL instances you want to check, as well as the SQL instance you are writing the final data to.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>Since this is now scheduled daily we can grab our morning coffee, sit down at our desk and immediately review our estate and ensure everything is as expected.&lt;/p>
&lt;p>We wrote the data to a SQL Server so you can go and query the data directly. By default there will be two tables created in the database.&lt;/p>
&lt;ul>
&lt;li>CheckResults – contains the actual results of the checks against your server&lt;/li>
&lt;li>dbachecksChecks – contains the metadata of the checks including tags and descriptions for each check you have invoked.&lt;/li>
&lt;/ul>
&lt;p>The other option is to use the dbachecks PowerBi dashboard, by running the following you can load the dashboard and connect to your dbachecks results database:&lt;/p>
&lt;p>Start-DbcPowerBi -FromDatabase&lt;/p>
&lt;p>When this opens you can see there were some failures on mssql1, right clicking on the orange bar you can drill through to see the details.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/09/dashboard-1.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/dashboard-1-1024x583.jpg"
loading="lazy"
alt="dbachecks main dashboard"
>&lt;/a>&lt;/p>
&lt;p>On the details pane you can see there are two offline databases that I need to look into.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/09/drillthrough.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/drillthrough-1024x279.jpg"
loading="lazy"
alt="details view of dbachecks PowerBi"
>&lt;/a>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Finally, this automation is just the starting piece of automating your daily checks. There are many ways to expand on this, but this is how you can get started with automating daily health checks with dbachecks.&lt;/p>
&lt;p>Thanks again for hosting, and sorry for being so late!&lt;/p></description></item><item><title>dbachecks meets ImportExcel</title><link>https://jpomfret.github.io/dbachecks-meets-importexcel/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbachecks-meets-importexcel/</guid><description>&lt;p>I got a message from a friend on Twitter last night asking ‘Is there an easy way to get dbachecks backup info into an Excel spreadsheet?’.  I sent them a couple of ideas, but figured this is a great use case that many people might be interested in. Pairing infrastructure testing using dbachecks with creating Excel reports with the ImportExcel module is a great addition to your automation tool belt. I also had ImportExcel on my mind this week after watching some great demos from Mikey Bronowski (&lt;a class="link" href="https://www.bronowski.it/blog/2020/06/powershell-into-excelimportexcel-module-part-1/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/mikeybronowski" target="_blank" rel="noopener"
>t&lt;/a>) at a user group earlier this week.&lt;/p>
&lt;h2 id="run-the-checks">&lt;strong>Run the Checks&lt;/strong>&lt;/h2>
&lt;p>First step is to run some checks. I’ve previously written about using &lt;a class="link" href="https://jesspomfret.com/checking-backups-with-dbachecks/" target="_blank" rel="noopener"
>dbachecks to check on your SQL Server database backups&lt;/a>, so I’m going to use that as a base here.&lt;/p>
&lt;p>I’m not going to change any of the configuration options, but that is covered in the post I linked to above. I am going to add the &lt;code>DatabaseStatus&lt;/code> check with the default configuration to ensure all my databases are online.&lt;/p>
&lt;p>$sqlinstances = &amp;lsquo;mssql1&amp;rsquo;,&amp;lsquo;mssql2&amp;rsquo;,&amp;lsquo;mssql3&amp;rsquo;,&amp;lsquo;mssql4&amp;rsquo;
$testResults = Invoke-DbcCheck -SqlInstance $sqlinstances -Check LastBackup, DatabaseStatus -PassThru&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/08/tests.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tests.jpg"
loading="lazy"
alt="dbachecks results in PowerShell"
>&lt;/a>&lt;/p>
&lt;p>You can see I have a nice balance of green (passed tests) and red (failed tests). Not really the balance we’re looking for in production, but perfect for a demo environment.&lt;/p>
&lt;p>Using the &lt;code>-PassThru&lt;/code> parameter means that the test results are both displayed on screen and saved to my $testResults variable. We’ll use that to create our report.&lt;/p>
&lt;h2 id="create-the-report--option-1-export-csv">&lt;strong>Create the Report – Option 1: Export-Csv&lt;/strong>&lt;/h2>
&lt;p>The first option we have here is to just get the data into a csv. We can do that natively in PowerShell using the &lt;code>Export-Csv&lt;/code> function.&lt;/p>
&lt;p>$testResults.TestResult |
Select-Object Describe, Context, Name, Result, FailureMessage |
Export-Csv c:\temp\backups.csv -NoTypeInformation&lt;/p>
&lt;p>This will get our data into a csv, which we can then manipulate in Excel.&lt;/p>
&lt;h2 id="create-the-report--option-2-export-excel">&lt;strong>Create the Report – Option 2: Export-Excel&lt;/strong>&lt;/h2>
&lt;p>The second option is to use the ImportExcel module. This is easily in my top 5 all-time favourite PowerShell modules. With this module we can create a great looking Excel report in just a few lines. The following will take our test results and create two worksheets in one Excel file.  The first sheet will contain our raw data, formatted as an Excel table with some conditional formatting to highlight the failed tests. The second tab will contain a pivot table/chart of our results broken down by the test type and result.&lt;/p>
&lt;p>$ConditionalFormat =$(
New-ConditionalText -Text Failed -Range &amp;lsquo;D:D&amp;rsquo;
)&lt;/p>
&lt;p>$excelSplat = @{
Path = &amp;lsquo;C:\Temp\Backups.xlsx&amp;rsquo;
WorkSheetName = &amp;lsquo;TestResults&amp;rsquo;
TableName = &amp;lsquo;Results&amp;rsquo;
Autosize = $true
ConditionalFormat = $ConditionalFormat
IncludePivotTable = $true
PivotRows = &amp;lsquo;Describe&amp;rsquo;
PivotData = @{Describe=&amp;lsquo;Count&amp;rsquo;}
PivotColumns = &amp;lsquo;Result&amp;rsquo;
IncludePivotChart = $true
ChartType = &amp;lsquo;ColumnStacked&amp;rsquo;
}&lt;/p>
&lt;p>$testResults.TestResult |
Select-Object Describe, Context, Name, Result, FailureMessage |
Export-Excel @excelSplat&lt;/p>
&lt;p>The final results are shown below. With so many more checks available in the dbachecks module it would be easy to expand on this example and get a comprehensive report of your environment.&lt;/p>
&lt;p>The full script is available on &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/06_dbachecksToExcel.ps1" target="_blank" rel="noopener"
>my Github&lt;/a> demos repo.&lt;/p>
&lt;p>Results Worksheet:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/excelSheet-1024x369.jpg"
loading="lazy"
alt="excel screenshot showing results worksheet"
>&lt;/p>
&lt;p>Pivot Table/Chart:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/pivot-1024x524.jpg"
loading="lazy"
alt="excel screenshot of pivot chart and table"
>&lt;/p></description></item><item><title>Get a list of databases from named SQL Instances</title><link>https://jpomfret.github.io/get-a-list-of-databases-from-named-sql-instances/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/get-a-list-of-databases-from-named-sql-instances/</guid><description>&lt;p>Have you ever had someone send you the name of a SQL Server and database to do some work, but when you try to connect to the server you can’t? Then,come to find out, there are four named instances on the box and you don’t know which one hosts the database? No? Just me?&lt;/p>
&lt;p>Luckily, dbatools has a couple of commands that can help us out with this. Firstly, we can use &lt;code>Get-DbaService&lt;/code> to get a list of instances that are running on the server:&lt;/p>
&lt;p>$SqlInstances = Get-DbaService -ComputerName mssql1 -Type Engine |
Select @{L=&amp;lsquo;SqlInstance&amp;rsquo;;e={(&amp;rsquo;{0}\{1}&amp;rsquo; -f $_.ComputerName, $_.InstanceName)}}&lt;/p>
&lt;p>I went ahead and piped this to the Select-Object and built the SqlInstance property to be ‘ServerName\InstanceName’.  We can now use this in any of the other dbatools commands. For my use case I wanted database information, so I went with &lt;code>Get-DbaDatabase&lt;/code>:&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance $SqlInstances.SqlInstance |
Format-Table SqlInstance, Name, Status, RecoveryModel -AutoSize&lt;/p>
&lt;p>This made it easy for me to find the database in question without having to connect to each instance manually.&lt;/p>
&lt;p>You could also use this if you had a list of servers by just passing in a comma seperated list to the &lt;code>-ComputerName&lt;/code> parameter on &lt;code>Get-DbaService&lt;/code>.&lt;/p>
&lt;p>Just a short post today, but hopefully useful to somebody.&lt;/p></description></item><item><title>Using Get-WinEvent to look into the past</title><link>https://jpomfret.github.io/using-get-winevent-to-look-into-the-past/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-get-winevent-to-look-into-the-past/</guid><description>&lt;p>Recently I was tasked with troubleshooting an incident on a SQL Server at a certain point in the past, the issue being a high CPU alert.  It’s hard (without monitoring solutions set up) to go back in time and determine what the issue is.  However, one thing we can check is the windows event log to see if there was anything happening on the server at that time.&lt;/p>
&lt;p>Now, you probably know that my favourite tool of choice is PowerShell, so let&amp;rsquo;s take a look at how we can use &lt;code>Get-WinEvent&lt;/code> to see what was happening in the past.&lt;/p>
&lt;p>&lt;code>Get-WinEvent&lt;/code> is the newer revamped version of &lt;code>Get-EventLog&lt;/code>, and there are two improvements I believe are worth mentioning. Firstly, with the introduction of filter parameters we can now find certain events much easier, which we’ll talk about a little later. Secondly, the performance of &lt;code>Get-WinEvent&lt;/code> is much faster than using the legacy command.  I believe this is due to the filtering happening at the event engine instead of within PowerShell.&lt;/p>
&lt;h2 id="finding-some-logs">Finding some logs&lt;/h2>
&lt;p>First things first, let&amp;rsquo;s see what logs we have available on our target machine. I’m targeting a remote machine with the code below, but if we’re investigating an issue on our local machine we can just exclude the &lt;code>-ComputerName&lt;/code> parameter.&lt;/p>
&lt;p>This snippet will output all of the logs on my remote machine that contain records to a gridview. I love to use &lt;code>Out-GridView&lt;/code> for these kinds of tasks because this returned 101 logs and I can now add some text into the search bar to filter for things I might be interested in.&lt;/p>
&lt;p>Get-WinEvent -ListLog * -ComputerName dscsvr2 |
Where-Object RecordCount |
Out-GridView&lt;/p>
&lt;p>You can see if I add dsc into the search bar of &lt;code>Out-Grid View&lt;/code> I have one log with records in that I could investigate further.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/viewlogs.jpg"
loading="lazy"
alt="Out-GridView displaying results from Get-WinEvent -ListLog *"
>&lt;/p>
&lt;h2 id="filtering-events">Filtering events&lt;/h2>
&lt;p>I already mentioned this, but the new &lt;code>Get-WinEvent&lt;/code> gives us three options for filtering events. I’ll show this example using &lt;code>-FilterHashTable&lt;/code>, but just know there are two other options available, &lt;code>-FilterXPath&lt;/code> and &lt;code>-FilterXml&lt;/code>.&lt;/p>
&lt;p>The full list of key/value pairs that we can use to filter on are under the &lt;code>-FilterHashTable&lt;/code> parameter section of the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.diagnostics/get-winevent?view=powershell-7" target="_blank" rel="noopener"
>Microsoft Docs&lt;/a>. For now, we’ll look at filtering based on log name and time.&lt;/p>
&lt;p>So, say I have an alert for high CPU at 21:30 on 2020-07-31 and I want to know what was happening around that time on the server.&lt;/p>
&lt;p>First I’ll set up a few parameters. I’m going to set the &lt;code>$computerName&lt;/code> (I could add multiple here if I wanted to collect logs from more than one server) and the &lt;code>$issueDateTime&lt;/code>. I’ve then also specified the &lt;code>$windowMins&lt;/code>, I’m going to use to create a window of time around my issue to collect events for.&lt;/p>
&lt;p>$computerName = &amp;lsquo;dscsvr2&amp;rsquo;
$issueDateTime = get-date(&amp;lsquo;2020-07-31 21:30&amp;rsquo;)
$windowMins = 30&lt;/p>
&lt;p>Next we’ll build the filter hash table. You can do this inline when you call the command, but I personally like to break it out just for readability.&lt;/p>
&lt;p>$winEventFilterHash = @{
LogName = &amp;lsquo;system&amp;rsquo;,&amp;lsquo;application&amp;rsquo;
StartTime = $issueDateTime.AddMinutes(-($windowMins/2))
EndTime = $issueDateTime.AddMinutes(($windowMins/2))
}&lt;/p>
&lt;p>Finally, we’ll call &lt;code>Get-WinEvent&lt;/code>, then pass in the filter hash table and the computer name.  I’m selecting just a few standard properties, as well as a calculated property to get the username instead of just the sid. The final piece of this pipeline is to use &lt;code>Format-Table&lt;/code>. I would also recommend using Export-Excel to pipe it straight to an excel file for analysis.&lt;/p>
&lt;p>(Note - I use Export-Excel in this post if you&amp;rsquo;re interested in that - &lt;a class="link" href="https://jesspomfret.com/getting-versions/" target="_blank" rel="noopener"
>Getting OS and SQL Version information with dbatools&lt;/a>)&lt;/p>
&lt;p>Get-WinEvent -FilterHashtable $winEventFilterHash -ComputerName $computerName | Select-Object LogName,
ProviderName,
TimeCreated,
Id,
LevelDisplayName,
@{l=&amp;lsquo;UserName&amp;rsquo;;e={(New-Object System.Security.Principal.SecurityIdentifier($_.UserId)).Translate([System.Security.Principal.NTAccount])}},
Message | Format-Table&lt;/p>
&lt;p>As you can see below, the results are returned from both the application and system logs along with the time, level, username and message.  Now, this is just a test box in my lab, but this is a great way of grabbing a window of events from your server to help troubleshoot issues in the past.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/eventoutput.png"
loading="lazy"
alt="Results from Get-WinEvent"
>&lt;/p></description></item><item><title>Truncate all the Tables in a Database with PowerShell</title><link>https://jpomfret.github.io/truncate-all-the-tables-in-a-database-with-powershell/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/truncate-all-the-tables-in-a-database-with-powershell/</guid><description>&lt;p>&lt;strong>TLDR&lt;/strong>; &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/05_TruncateAllTables.ps1" target="_blank" rel="noopener"
>This code&lt;/a> will script out foreign keys and views (including object level permissions), drop the objects, truncate all the tables, and recreate the objects.&lt;/p>
&lt;h2 id="the-details">The Details&lt;/h2>
&lt;p>The most popular post on my blog so far was called ‘&lt;a class="link" href="https://jesspomfret.com/disable-all-triggers/" target="_blank" rel="noopener"
>Disable all Triggers on a Database&lt;/a>’ and this one is a good follow up from that post.&lt;/p>
&lt;p>The scenario here is you need to remove all the data from the tables in your database. This could be as part of a refresh process, or perhaps to clear out test data that has been entered through an application.  Either way, you want to truncate all the tables in your database.&lt;/p>
&lt;p>Using a copy of the AdventureWorks2017 database for my demos, the easiest option to truncate all the tables is to script out truncate statements using the metadata stored in &lt;code>sys.tables&lt;/code>.&lt;/p>
&lt;p>SELECT &amp;lsquo;Truncate table &amp;rsquo; + QUOTENAME(SCHEMA_NAME(schema_id)) + &amp;lsquo;.&amp;rsquo; + QUOTENAME(name)
FROM sys.tables&lt;/p>
&lt;p>You’ll get a results set like shown below which you can copy out into a new query window and execute.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/TruncateFromSysTables.png"
loading="lazy"
alt="select from sys.tables to generate truncate statements"
>&lt;/p>
&lt;p>The problem is if you have foreign keys, even if you order the truncate statements to remove the dependent data first, you can’t issue a truncate statement. The way around this is to script out the foreign keys, drop them, run the truncate statements and then recreate the foreign keys. This is not difficult in T-SQL, but it’s easier with PowerShell and a little bit of &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> magic.&lt;/p>
&lt;h2 id="truncate-tables-with-powershell">Truncate tables with PowerShell&lt;/h2>
&lt;p>The full script is available up on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/05_TruncateAllTables.ps1" target="_blank" rel="noopener"
>Github&lt;/a>, but I’ll walk through the process here. During my post on disabling triggers I stored the previously enabled triggers in a variable to reuse during the script.  I had a really great comment on this post that pointed out a problem: if the session crashed for some reason we would lose the list of triggers we wanted to enable. I will solve that problem in this post by instead of using a variable, saving the information in a temporary file.&lt;/p>
&lt;p>First things first, we need to set up a couple of variables to define our SqlInstance, database and the folder we’ll use as our workspace.&lt;/p>
&lt;p>I’ll then use &lt;code>Connect-DbaInstance&lt;/code> to connect to the instance and save the smo object. This will save having to reconnect to the instance multiple times.&lt;/p>
&lt;p>$sqlInstance = &amp;lsquo;mssql1&amp;rsquo;
$database = &amp;lsquo;AdventureWorks2017&amp;rsquo;
$tempFolder = &amp;lsquo;C:\temp&amp;rsquo;&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance $sqlInstance&lt;/p>
&lt;p>The next step is to collect the foreign keys that we’ll need to drop and recreate. It’s important to note here there are also some views that depend on these tables, so I can also collect that information at the same time. &lt;/p>
&lt;p># Collect up the objects we need to drop and recreate
$objects = @()
$objects += Get-DbaDbForeignKey -SqlInstance $svr -Database $database
$objects += Get-DbaDbView -SqlInstance $svr -Database $database -ExcludeSystemView&lt;/p>
&lt;p>Now that we have collected the objects into a variable we can pipe this to the &lt;code>Export-DbaScript&lt;/code> command to generate T-SQL scripts for both dropping and then recreating the objects. Something to take into consideration when dropping and recreating views is that if there are permissions set at the object level we need to include those in our create scripts.  We can use the &lt;code>New-DbaScriptingOption&lt;/code> command to set the options we care about when we create the scripts.&lt;/p>
&lt;p>Here we are including the permissions, the ‘ScriptBatchTerminator’, which will add ‘Go’ between objects, and finally setting the file type to ANSI.  When we call &lt;code>Export-DbaScript&lt;/code> we can then use these options for the &lt;code>-ScriptingOptionsObject&lt;/code> parameter.&lt;/p>
&lt;p># Script out the create statements for objects
$createOptions = New-DbaScriptingOption
$createOptions.Permissions = $true
$createOptions.ScriptBatchTerminator = $true
$createOptions.AnsiFile = $true&lt;/p>
&lt;p>$objects | Export-DbaScript -FilePath (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder) -ScriptingOptionsObject $createOptions&lt;/p>
&lt;p>We also need to script out the drop statements. To do that we’ll create another options object, this time setting &lt;code>ScriptDrops&lt;/code> to true. Then we’ll again call &lt;code>Export-DbaScript&lt;/code> with the &lt;code>-ScriptingOptionsObject&lt;/code> parameter.&lt;/p>
&lt;p># Script out the drop statements for objects
$options = New-DbaScriptingOption
$options.ScriptDrops = $true
$objects| Export-DbaScript -FilePath (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder) -ScriptingOptionsObject $options&lt;/p>
&lt;p>Once we have the scripts safely in our temporary folder we’ll run three simple statements.&lt;/p>
&lt;p>First, we’ll run the drop statements we scripted out.&lt;/p>
&lt;p>Second, remember we saved the smo connection to our server in the &lt;code>$svr&lt;/code> variable. We’ll use that to access all the tables in our database, pipe that to a &lt;code>Foreach-Object&lt;/code> and call the &lt;code>TruncateData&lt;/code> method.&lt;/p>
&lt;p>Third, we’ll call &lt;code>Invoke-DbaQuery&lt;/code> to recreate the foreign keys and the views we previously dropped.&lt;/p>
&lt;p># Run the drop scripts
Invoke-DbaQuery -SqlInstance $svr -Database $database -File (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;h1 id="truncate-the-tables">Truncate the tables&lt;/h1>
&lt;p>$svr.databases[$database].Tables | ForEach-Object { $_.TruncateData() }&lt;/p>
&lt;h1 id="run-the-create-scripts">Run the create scripts&lt;/h1>
&lt;p>Invoke-DbaQuery -SqlInstance $svr -Database $database -File (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;p>The final step is to clear up the script files we saved to the temporary folder.&lt;/p>
&lt;p># Clear up the script files
Remove-Item (&amp;rsquo;{0}\DropObjects.Sql&amp;rsquo; -f $tempFolder), (&amp;rsquo;{0}\CreateObjects.Sql&amp;rsquo; -f $tempFolder)&lt;/p>
&lt;p>This script can be reused for any database that you may need to clear out. As I was writing this post, I realised this could probably be a dbatools command… watch this space. ?&lt;/p></description></item><item><title>Using T-SQL to Aggregate Strings</title><link>https://jpomfret.github.io/using-t-sql-to-aggregate-strings/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-t-sql-to-aggregate-strings/</guid><description>&lt;p>I’m a SQL Server Database Engineer by day, but I must say my blog has a lot more PowerShell and automation posts than T-SQL.  However, last week I found a really great T-SQL aggregate function that I had no idea existed, so I thought I’d share it with you.&lt;/p>
&lt;p>I have been working on a project to document our SQL Server environment and create GitHub issues for things that need fixed. Issues are written in markdown so you can easily generate some pretty good looking issues with plenty of data using PowerShell. This is worth a blog post of it’s own, so keep an eye out for that soon.&lt;/p>
&lt;p>Long story short I wanted a way to be able to list all the SQL Server instances on the server I was logging the issue for. I have a database with two tables, one that contains server information and one that contains instance information. Running the following gets me one row per server/instance combination.&lt;/p>
&lt;p>SELECT s.ServerListId, s.ServerName, i.InstanceListId, i.InstanceName
FROM ServerList s
INNER JOIN InstanceList i
ON s.ServerListId = i.ServerListId&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ServerListId&lt;/th>
&lt;th>ServerName&lt;/th>
&lt;th>InstanceListId&lt;/th>
&lt;th>InstanceName&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>MSSQL1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>MSSQL3&lt;/td>
&lt;td>3&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>NAMEDINST1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>MSSQL2&lt;/td>
&lt;td>5&lt;/td>
&lt;td>NAMEDINST2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>I started thinking about how to group this data by server name and then concatenate the instance names together. Luckily a quick google found &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener"
>STRING_AGG()&lt;/a>. This T-SQL aggregate function has only been available since SQL Server 2017, and does exactly what I needed. It takes two parameters, the first being the column name that should be aggregated and the second a separator to use.&lt;/p>
&lt;p>For this example I’ll group by ServerName, and aggregate the InstanceName column using a comma to separate the values.&lt;/p>
&lt;p>SELECT ServerName, STRING_AGG(InstanceName,&amp;rsquo;, &amp;lsquo;) as InstanceName
FROM ServerList s
INNER JOIN InstanceList i
ON s.ServerListId = i.ServerListId
GROUP BY ServerName&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ServerName&lt;/th>
&lt;th>InstanceName&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MSSQL1&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MSSQL2&lt;/td>
&lt;td>MSSQLSERVER, NAMEDINST1, NAMEDINST2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MSSQL3&lt;/td>
&lt;td>MSSQLSERVER&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Hope some of you find this quick T-SQL post useful. It definitely fit my need well for this scenario.&lt;/p></description></item><item><title>T-SQL Tuesday #127 – Non SQL Tips &amp; Tricks</title><link>https://jpomfret.github.io/t-sql-tuesday-#127-non-sql-tips-tricks/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#127-non-sql-tips-tricks/</guid><description>&lt;p>&lt;a class="link" href="https://sqlstudies.com/2020/06/02/tsql-tuesday-127-invite-non-sql-tips-and-tricks/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-SQL Tuesday Logo"
>&lt;/a>&lt;/p>
&lt;p>I feel like I say this every month, but it’s already time for another edition of T-SQL Tuesday. This months blog party is hosted by Kenneth Fisher (&lt;a class="link" href="https://sqlstudies.com/" target="_blank" rel="noopener"
>B&lt;/a>|&lt;a class="link" href="https://twitter.com/sqlstudent144" target="_blank" rel="noopener"
>T&lt;/a>) and he’s looking for tips &amp;amp; tricks, but nothing DBMS related.  As you might already know, I love shortcuts and tips &amp;amp; tricks – so I was really excited to see this prompt. First, because I easily found a few I wanted to share, but secondly, I can’t wait to read about everyone else’s tips &amp;amp; tricks.&lt;/p>
&lt;p>I had a hard time narrowing this down to just one shortcut, so I’ve picked three – but they fit together nicely. First, I have to let you in on a small secret:&lt;/p>
&lt;h2 id="im-a-tab-hoarder">I’m a tab hoarder&lt;/h2>
&lt;p>Whether it’s chrome or SSMS, I cannot help myself when it comes to opening new tabs. It’s not uncommon for me to have so many chrome tabs open that you can only see the logos.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/tabsShort.png"
loading="lazy"
alt="chrome browser tabs"
>&lt;/p>
&lt;p>I even got called out by my good friend Andrew (&lt;a class="link" href="https://awickham.com/" target="_blank" rel="noopener"
>B&lt;/a>|&lt;a class="link" href="https://twitter.com/awickham" target="_blank" rel="noopener"
>T&lt;/a>) this last week:&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/awickham/status/1267503576571674624" target="_blank" rel="noopener"
>https://twitter.com/awickham/status/1267503576571674624&lt;/a>&lt;/p>
&lt;p>My tips &amp;amp; tricks are focused around managing tabs, and they work in all browsers (at least all that I have on my laptop. Chrome, Edge, IE11).&lt;/p>
&lt;ul>
&lt;li>&lt;code>Ctrl + Tab&lt;/code> – Switch tabs&lt;/li>
&lt;li>&lt;code>Ctrl + W&lt;/code> - Close a tab&lt;/li>
&lt;li>&lt;code>Ctrl + Shift + T&lt;/code> - Reopen the last closed tab&lt;/li>
&lt;/ul>
&lt;p>The first two are pretty self-explanatory, but the third one deserves a special mention.  I’m sure it’s happened to everyone- you close a tab and the instant it’s gone, you realise you actually needed that one.  Well &lt;code>Ctrl + Shift + T&lt;/code> to the rescue. This shortcut will revive that last closed tab from the dead and the magic here is if you keep pressing it, it’ll keep opening tabs you shut previously.&lt;/p>
&lt;p>In fact, if you close an entire browser window full of 20 tabs (that I’m sure you need all of) and in another browser window you press &lt;code>Ctrl + Shift + T&lt;/code>, like magic, all the tabs come back to you.&lt;/p>
&lt;h2 id="bonus-content">Bonus Content&lt;/h2>
&lt;p>Now I started off using these shortcuts in internet browsers, but then I discovered they work in my favourite code editor as well.  All three of the keyboard shortcuts above work in VSCode, so now you can easily switch between scripts with &lt;code>Ctrl + Tab&lt;/code>, close a script with &lt;code>Ctrl + W&lt;/code> and then reopen it with &lt;code>Ctrl + Shift + T&lt;/code>.&lt;/p>
&lt;p>I love this crossover functionality, and I feel like it adds so much when you can open a program and it works in a way that you’re used to.&lt;/p></description></item><item><title>Using PSDefaultParameterValues for connecting to SQL Server in containers</title><link>https://jpomfret.github.io/using-psdefaultparametervalues-for-connecting-to-sql-server-in-containers/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-psdefaultparametervalues-for-connecting-to-sql-server-in-containers/</guid><description>&lt;p>I’ve written previously about using containers for demos on my laptop, specifically for my &lt;a class="link" href="https://jesspomfret.com/data-compression-containers/" target="_blank" rel="noopener"
>data compression talk&lt;/a>.  Since I switched those demos over I haven’t looked back- if it’s possible to run my demos off of containers I always choose that option.&lt;/p>
&lt;p>I recently presented a talk called ‘Life Hacks: dbatools edition’ which walks through 6 scenarios where you can immediately implement dbatools to quickly reap the rewards.  The demos can all be run on containers, but I did need to get a little more complex to be able to show off dbatools migration commands. To do this I used a docker compose file.&lt;/p>
&lt;p>The compose file creates one instance straight from the Microsoft SQL Server 2019 image and a second one from a dockerfile that specifies the base SQL Server 2017 image, copies in the files needed to attach the AdventureWorks2017 database, and runs some SQL to get everything setup exactly as desired. Feel free to check out this &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/LifeHacks_dbatools/Docker/docker-compose.yml" target="_blank" rel="noopener"
>setup on my Github&lt;/a>.&lt;/p>
&lt;p>One of the things that bothered me about running my demos on containers was that I couldn’t use windows authentication. Instead I had to pass in a SQL login to connect for every command.&lt;/p>
&lt;h2 id="enter-psdefaultparametervalues">Enter PSDefaultParameterValues&lt;/h2>
&lt;p>I first heard about PSDefaultParameterValues from a &lt;a class="link" href="https://github.com/PSPowerHour/PSPowerHour/tree/master/materials/2018-08-21/potatoqualitee" target="_blank" rel="noopener"
>PSPowerHour session by Chrissy LeMaire&lt;/a> in 2018. After rewatching this recently, I realised she even mentioned this exact scenario. However, it took until I recently rediscovered this handy preference variable that it all clicked together.&lt;/p>
&lt;p>PSDefaultParameterValues does exactly what the name suggests- it lets you specify default values for parameters. PSDefaultParameterValues can be set as a hash table of parameter names and values that will be used in your session for any function that can use it.  A simple example is the verbose parameter. If you wanted to turn on the &lt;code>-Verbose&lt;/code> switch for every function you run you could add &lt;code>-Verbose&lt;/code> to each function call, or you could set PSDefaultParameterValues.&lt;/p>
&lt;h3 id="option-1--add--verbose-to-individual-commands">Option 1 – Add &lt;code>-Verbose&lt;/code> to individual commands&lt;/h3>
&lt;p>Get-DbaDbBackupHistory -SqlInstance mssql1 -Verbose
Repair-DbaDbOrphanUser -SqlInstance mssql1 -Verbose&lt;/p>
&lt;h3 id="option-2--set-psdefaultparametervalues">Option 2 – Set PSDefaultParameterValues&lt;/h3>
&lt;p>$PSDefaultParameterValues = @{ &amp;lsquo;*:Verbose&amp;rsquo; = $True }
Get-DbaDbBackupHistory -SqlInstance mssql1
Repair-DbaDbOrphanUser -SqlInstance mssql1&lt;/p>
&lt;p>One thing to note when specifying PSDefaultParameterValues as I have above: this will overwrite any parameters you already have saved to PSDefaultParameterValues, so be careful. Another way to set &lt;code>-Verbose&lt;/code> to true would be to use the following notation:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;rsquo;*:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;h2 id="getting-more-specific">Getting more specific&lt;/h2>
&lt;p>In the above examples I’m using a wildcard (*) on the left side to specify that this parameter is for all functions. You can also focus in PSDefaultParameterValues by specifying one certain function name that the parameter value will apply to:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;lsquo;Get-DbaDbTable:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;p>You can also specify just the dbatools commands by taking advantage of their naming conventions and using:&lt;/p>
&lt;p>$PSDefaultParameterValues[&amp;rsquo;*-Dba*:Verbose&amp;rsquo;] = $True&lt;/p>
&lt;h2 id="psdefaultparametervalues-for-connecting-to-containers">PSDefaultParameterValues for connecting to containers&lt;/h2>
&lt;p>As I mentioned, my use case was to avoid having to specify a credential for every function that connected to my SQL Server running in a container. To use this for dbatools I need to specify a few parameter names. Most dbatools functions take the credential for the &lt;code>-SqlCredential&lt;/code> parameter, but for the copy commands there is both &lt;code>-SourceSqlCredential&lt;/code> and &lt;code>-DestinationCredential&lt;/code> that need to be specified.&lt;/p>
&lt;p>First, I create a &lt;code>PSCredential&lt;/code> that contains my username and password (note: this is for a demo environment and is insecure as the password is in plain text. If you are using this for other scenarios you’ll want to protect this credential). &lt;/p>
&lt;p>$securePassword = (&amp;lsquo;Password1234!&amp;rsquo; | ConvertTo-SecureString -asPlainText -Force)
$credential = New-Object System.Management.Automation.PSCredential(&amp;lsquo;sa&amp;rsquo;, $securePassword)&lt;/p>
&lt;p>Once I have the credential I can specify all the parameters that should use that credential by default:&lt;/p>
&lt;p>$PSDefaultParameterValues = @{&amp;quot;*:SqlCredential&amp;quot;=$credential
&amp;ldquo;*:DestinationCredential&amp;rdquo;=$credential
&amp;ldquo;*:DestinationSqlCredential&amp;rdquo;=$credential
&amp;ldquo;*:SourceSqlCredential&amp;rdquo;=$credential}&lt;/p>
&lt;p>Now whenever I call a function within this session, the specified parameters will use my credential. Therefore I can run the following and it’ll automatically use my saved sa login credential.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1&lt;/p>
&lt;h2 id="psdefaultparametervalues-in-your-profile">PSDefaultParameterValues in your profile&lt;/h2>
&lt;p>Setting PSDefaultParameterValues will only persist in the current session, however you can add the code above to your profile so that these default values are always provided.  If I do, whenever I open a PowerShell window I can easily connect to my containers without having to specify the credential.&lt;/p>
&lt;p>One thing to note is that this might be overkill. In my situation this is my demo machine. I always use the same sa password for any containers I run, and the majority of the time I’m running commands with a &lt;code>SqlCredential&lt;/code> parameter I want to connect to those containers.&lt;/p>
&lt;h2 id="override">Override&lt;/h2>
&lt;p>Even if you have set PSDefaultParameterValues in your profile you can still override that default value on any command just by specifying a new value. For example, running the following will pop up the credential request window for you to enter new credentials.&lt;/p>
&lt;p>Get-DbaDatabase -SqlInstance mssql1 -SqlCredential (Get-Credential)&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>To wrap this up, I’ve found a lot of time savings by adding PSDefaultParameterValues to my profile. I can now quickly fire up PowerShell and start running functions against my containers.  It also keeps my demo scripts clean and easier to read. There is no need to specify the same parameters over and over again when it’s always going to be the same value.&lt;/p></description></item><item><title>T-SQL Tuesday #126 – Folding@Home</title><link>https://jpomfret.github.io/t-sql-tuesday-#126-foldinghome/</link><pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#126-foldinghome/</guid><description>&lt;p>&lt;a class="link" href="https://glennsqlperformance.com/2020/05/05/t-sql-tuesday-126-foldinghome/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
alt="T-Sql Tuesday logo"
>&lt;/a>&lt;/p>
&lt;p>Well it’s May T-SQL Tuesday time! Honestly I’m not sure if time is crawling or flying by, it seems like forever ago we got writing for the April prompt on unit testing databases.  Thanks to Glenn (&lt;a class="link" href="https://glennsqlperformance.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/GlennAlanBerry" target="_blank" rel="noopener"
>t&lt;/a>) this month for hosting an interesting topic. I’m looking forward to reading all the responses. Also a bigger thanks for publicising &lt;a class="link" href="https://foldingathome.org/" target="_blank" rel="noopener"
>Folding@Home&lt;/a> and setting up the &lt;a class="link" href="https://stats.foldingathome.org/team/236388" target="_blank" rel="noopener"
>#SQLFamily&lt;/a> team!&lt;/p>
&lt;p>Glenn wants to know what we’re doing in response to COVID-19 and if we’re contributing to the FAH #SQLFamily team, what our experience has been.&lt;/p>
&lt;h2 id="foldinghome">&lt;strong>Folding@Home&lt;/strong>&lt;/h2>
&lt;p>I installed the Folding@Home client almost a month ago now on my Intel NUC. The NUC is connected up to my TV and mainly used as a media server. Occasionally I’ll use it to build out a lab to test something but most of the time it’s idle. Perfect to donate.&lt;/p>
&lt;p>My FAH setup is pretty standard. I installed the client, requested a passkey, and set it loose. One thing I am a little uncomfortable with is the CPU is at 100% all the time FAH is running a workload, and it gets a little hot.  I started manually setting the workload to ‘finish’ (finish the workload currently running but then pause) in the evenings and then setting it back to ‘fold’ in the morning.&lt;/p>
&lt;p>Since I’m human, sometimes I forgot.&lt;/p>
&lt;p>Enter PowerShell.&lt;/p>
&lt;p>I found that you could pass commands into the FAHClient.exe and therefore set the status from PowerShell.  I created a simple module &lt;a class="link" href="https://github.com/jpomfret/PSFah" target="_blank" rel="noopener"
>PsFah&lt;/a> with a function to control the FAH client Status. When I say simple, it currently has one function that sets the local client status to fold, pause or finish. Perhaps I’ll add more over time (I started getting more side-tracked by this and then realised I needed to actually write this post).&lt;/p>
&lt;p>I then set up two scheduled tasks (details below) that set the client to start folding at 7am and then set it to finish folding at 8pm. Removing the human, and therefore improving the process.&lt;/p>
&lt;p>$taskSplat = @{
Action = New-ScheduledTaskAction -Execute PowerShell.exe -Argument &amp;lsquo;Set-FahStatus -Status Fold&amp;rsquo;
Trigger = New-ScheduledTaskTrigger -Daily -At &amp;lsquo;07:00&amp;rsquo;
Description = &amp;lsquo;Start Folding&amp;rsquo;
Principal = New-ScheduledTaskPrincipal -UserID &amp;ldquo;NT AUTHORITY\SYSTEM&amp;rdquo; -LogonType ServiceAccount -RunLevel Highest&lt;/p>
&lt;p>}
Register-ScheduledTask @taskSplat -TaskName &amp;lsquo;Start Folding&amp;rsquo;&lt;/p>
&lt;p>$taskSplat = @{
Action = New-ScheduledTaskAction -Execute PowerShell.exe -Argument &amp;lsquo;Set-FahStatus -Status Finish&amp;rsquo;
Trigger = New-ScheduledTaskTrigger -Daily -At &amp;lsquo;20:00&amp;rsquo;
Description = &amp;lsquo;Finish Folding&amp;rsquo;
Principal = New-ScheduledTaskPrincipal -UserID &amp;ldquo;NT AUTHORITY\SYSTEM&amp;rdquo; -LogonType ServiceAccount -RunLevel Highest
}
Register-ScheduledTask @taskSplat -TaskName &amp;lsquo;Finish Folding&amp;rsquo;&lt;/p>
&lt;h2 id="community-involvement">&lt;strong>Community Involvement&lt;/strong>&lt;/h2>
&lt;p>&lt;img src="https://i0.wp.com/jesspomfret.com/wp-content/uploads/2020/05/DW_Speaker-1.jpg?fit=650%2C640&amp;amp;ssl=1"
loading="lazy"
alt="data weekender camper van"
>&lt;/p>
&lt;p>Another way I’ve been trying to give back a little is with community involvement. I was lucky enough to be selected to speak as part of the &lt;a class="link" href="https://www.dataweekender.com/" target="_blank" rel="noopener"
>Data Weekender&lt;/a> event. It was great to be able to deliver a session and share some dbatools knowledge. It was even better to watch some sessions and chat with the community. I miss that interaction that you get at conferences, and this day helped to fill that void – if only virtually.&lt;/p>
&lt;p>I am also really excited to be speaking at the &lt;a class="link" href="https://groupby.org/may2020-schedule/" target="_blank" rel="noopener"
>GroupBy conference&lt;/a>, which when this post goes live will be today!&lt;/p>
&lt;p>I enjoy travelling and speaking and I was really disappointed that I had been selected to speak at a couple of events that were then cancelled because of COVID. Being able to still speak and contribute to the community virtually has been a really great experience.&lt;/p>
&lt;h2 id="finally">Finally&lt;/h2>
&lt;p>It’s easy to feel like you’re not doing enough during this crisis. it’s hard to stay motivated and to be productive when there is so much stress and anxiety in the world.  First things first, we have to look after ourselves and our people. If you have extra energy left over there are plenty of ways to give back, but it’s important for all of us to remember that just surviving right now takes more energy than usual.&lt;/p>
&lt;p>Stay safe folks.&lt;/p></description></item><item><title>Changing focus on code execution in VSCode</title><link>https://jpomfret.github.io/changing-focus-on-code-execution-in-vscode/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/changing-focus-on-code-execution-in-vscode/</guid><description>&lt;p>I wrote previously about how I loved little life hacks and shortcuts for my &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-123-summary" target="_blank" rel="noopener"
>February T-SQL Tuesday prompt&lt;/a>. If you read that you’ll know I use VSCode a lot and really love all the shortcuts available in that program. This is just a quick tip that I’m so glad I found, read – took the time to work out.&lt;/p>
&lt;p>I specifically write a lot of PowerShell in VSCode and so often find myself using F8 to run the selected line in the integrated console. One thing that always drove me a little crazy was that my cursor stayed in the integrated console after execution, rather than returning to the script I was writing.  I hadn’t managed to find the shortcut to return to the editor window I was working in until recently when I decided to figure it out.&lt;/p>
&lt;p>Since I started writing this post a couple of weeks ago, I discovered an even better solution thanks to the following tweet from &lt;a class="link" href="http://twitter.com/simon_sabin" target="_blank" rel="noopener"
>Simon Sabin&lt;/a>. The tweet also links to a GitHub issue where there is a discussion on why this is the default behaviour.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/simon" target="_blank" rel="noopener"
>https://twitter.com/simon&lt;/a>_sabin/status/1252253795603681281&lt;/p>
&lt;p>So by adding the following to your &lt;code>settings.json&lt;/code> file you can override that behaviour, and keep the focus in your script pane.&lt;/p>
&lt;p>&amp;ldquo;powershell.integratedConsole.focusConsoleOnExecute&amp;rdquo;: false&lt;/p>
&lt;h2 id="original-solution">&lt;strong>Original Solution&lt;/strong>&lt;/h2>
&lt;p>The original solution to this dilemma is to use the keyboard shortcut to return focus to the script pane. Using Ctrl+1 will return you to the first editor group (unless you are using ZoomIt!).&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/onepanev2-1.gif"
loading="lazy"
alt="Use Ctrl&amp;#43;1 to get to editor from console"
>&lt;/p>
&lt;p>If you have more than one editor group open you can use CTRL+2 to get to the second group, or CTRL+3 to get to the third group.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/twoPanev2-1.gif"
loading="lazy"
alt="ctrl&amp;#43;2 to get back to pane 2"
>&lt;/p>
&lt;p>Also, as with most shortcuts in VSCode, you can also customise the key bindings, by opening the command palette (F1) and choosing ‘Open Keyboard Shortcuts’ either in the GUI or JSON format.&lt;/p>
&lt;p>The commands to customise are below with the defaults:&lt;br>
- workbench.action.focusFirstEditorGroup: Ctrl+1&lt;br>
- workbench.action.focusSecondEditorGroup: Ctrl+2&lt;br>
- workbench.action.focusThirdEditorGroup: Ctrl+3 (and so on up to the eighth editor group)&lt;/p>
&lt;p>A bonus tip for you: If you’re in the GUI keyboard shortcut editor and you right click and copy, or press Ctrl+C, you’ll actually copy the JSON you’d need to customise your key bindings in the keyboard shortcuts (JSON) file.&lt;/p>
&lt;p>{
&amp;ldquo;key&amp;rdquo;: &amp;ldquo;ctrl+5&amp;rdquo;,
&amp;ldquo;command&amp;rdquo;: &amp;ldquo;workbench.action.focusFifthEditorGroup&amp;rdquo;
}&lt;/p>
&lt;p>I’m really excited to have discovered a couple of optimisations so now when I’m writing a script and I execute a line of PowerShell in the integrated console I will easily be able to navigate back to my editor pane using Ctrl+1 instead of having to reach for the mouse.&lt;/p></description></item><item><title>Anyone know what day it is?</title><link>https://jpomfret.github.io/anyone-know-what-day-it-is/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/anyone-know-what-day-it-is/</guid><description>&lt;p>It’s an interesting time we’re living in right now. I’m based in the UK and we just received the announcement last week that we have at least three more weeks of lockdown. I’ve been working at home for over a month now and I feel really lucky that I can work from home. There are a lot of folks who can’t and are instead working hard on the front lines to keep us safe and well. Firstly, a shout out to those folks! Thanks for all your doing.&lt;/p>
&lt;p>Now, this is not a post on how to set up your home office, or how to be the most productive work from home employee. This is a post to solve one simple problem.  I’ve no idea what day it is.&lt;/p>
&lt;p>Monday through Friday I wake up, drink some coffee and then head to my desk to work my day job. I usually workout in the garden with my wife at lunch and then we walk after work. Every day is pretty much the same, so I’ve written an earth-shattering PowerShell function for you.&lt;/p>
&lt;p>function Find-WhatDayAreWeDoing {
param (
[switch]$Raw
)&lt;/p>
&lt;pre>&lt;code>if($raw) {
(Get-Date).DayOfWeek
}
else {
(&amp;quot;We're doing {0} today!&amp;quot; -f (Get-Date).DayOfWeek)
}
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;p>You can add this into your profile and then simply just call the function to remind you what day we’re on.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FindWhatDay1.jpg"
loading="lazy"
alt="Find-WhatDayAreWeDoing"
>&lt;/p>
&lt;p>I’ve also added the &lt;code>-Raw&lt;/code> switch so you can use that to just return the day, this is useful if you wanted to add it into your prompt.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/FindWhatDay2.jpg"
loading="lazy"
>&lt;/p>
&lt;p>To change the prompt in PowerShell you just need to create a function called ‘Prompt’. For example, if I run this in my PowerShell session or put it in my profile, I’ll be able to see what day it is constantly.&lt;/p>
&lt;p>function Prompt {
Write-Host (&amp;quot;[{0}] PS&amp;gt; &amp;quot; -f (Find-WhatDayAreWeDoing -raw))
}&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/prompt1.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The prompt is a really useful snippet of code and there are many  great ideas out there. I use the amazing &lt;a class="link" href="https://dbatools.io/prompt/" target="_blank" rel="noopener"
>dbatools prompt&lt;/a>. Their prompt lists the current time and the execution time for the last statement run. You could change it to add in the day also:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/prompt2.jpg"
loading="lazy"
>&lt;/p>
&lt;p>I’ll be honest, development on &lt;code>Set-WhatDayWeAreDoing&lt;/code> is not going as well. Stay safe folks and keep keeping on.&lt;/p></description></item><item><title>T-SQL Tuesday – #125: Unit Testing Databases</title><link>https://jpomfret.github.io/t-sql-tuesday-#125-unit-testing-databases/</link><pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#125-unit-testing-databases/</guid><description>&lt;p>&lt;a class="link" href="https://hybriddbablog.com/2020/04/07/t-sql-tuesday-125-unit-testing-databases-we-need-to-do-this/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It’s T-SQL Tuesday time again! March felt really long for a lot of us as we got used to our new way of life, so I’m excited we made it to April’s prompt.&lt;/p>
&lt;p>This month’s topic is asking for a discussion around whether unit testing for databases is valuable. Since getting involved with dbatools and beginning to write more structured PowerShell (meaning modules and DSC resources rather than just an odd script) I have learnt a lot more about testing code. However, I have zero experience with testing databases. So I’m excited for this topic. Thanks for hosting Hamish (&lt;a class="link" href="https://hybriddbablog.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/TheHybridDBA" target="_blank" rel="noopener"
>t&lt;/a>).&lt;/p>
&lt;p>I recently watched Steve Jones give a session as part of &lt;a class="link" href="https://www.red-gate.com/hub/events/redgate-events/redgate-streamed/" target="_blank" rel="noopener"
>Redgate Streamed&lt;/a> on using &lt;a class="link" href="https://tsqlt.org/" target="_blank" rel="noopener"
>tSQLt&lt;/a> for database testing. It’s not the first time I’ve heard about this tool and it’s been on my list for way too long as something I want to investigate a little further. I figured this was a good excuse.&lt;/p>
&lt;h2 id="unit-tests-for-powershell">Unit tests for PowerShell&lt;/h2>
&lt;p>Writing tests for PowerShell code makes a lot of sense to me. There is something really nice about being able to test all the (covered) functionality of a function or cmdlet, after you’ve fiddled around with it.  It’s easy to get lost in fixing a bug, or adding some great new functionality and accidentally break something else. Having tests written ensures the function still works as desired, and as you add functionality or fix bugs you can add to the test cases, making the whole process more robust.  I’m 100% for unit and integration testing for code. No doubt about it.&lt;/p>
&lt;h2 id="so-why-not-databases">So why not databases?&lt;/h2>
&lt;p>Data is always the difficult bit when people talk about DevOps or agile development practices, and to be honest, I’d be way out of my depth to talk about how and why to test your data.  I’m instead going to look at using tSQLt to test functions and stored procedures. After all, there’s code in our databases too! After seeing the benefits of writing tests for PowerShell functions, I can easily see the benefits of testing the programmable objects in our databases.&lt;/p>
&lt;h2 id="installing-tsqlt">Installing tSQLt&lt;/h2>
&lt;p>Getting started with tSQLt is really easy- &lt;a class="link" href="https://tsqlt.org/downloads/" target="_blank" rel="noopener"
>you download a zip file&lt;/a>, unzip the contents and then run the tSQLt.class.sql script against your development database.&lt;/p>
&lt;p>There are a couple of requirements, including CLR integration must be enabled and your database must be set to trustworthy.  These could open up some security considerations, but for my development environment it’s no issue.&lt;/p>
&lt;p>After I ran the tSQLt.class.sql script I got the following in the messages tab of SSMS. We’re ready to go:&lt;/p>
&lt;p>Installed at 2020-04-07 15:45:03.200&lt;/p>
&lt;p>+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| |
| Thank you for using tSQLt. |
| |
| tSQLt Version: 1.0.5873.27393 |
| |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&lt;/p>
&lt;p>Completion time: 2020-04-11T14:25:21.6912488+01:00&lt;/p>
&lt;h2 id="test-a-simple-function">Test a simple function&lt;/h2>
&lt;p>To start with I made a really simple function to make sure I understood the tSQLt syntax and could get my first test written successfully.  It takes two numbers and returns the sum.&lt;/p>
&lt;p>CREATE OR ALTER FUNCTION dbo.AddTwoNumbers (@int1 int, @int2 int)
RETURNS INT AS
BEGIN
RETURN @int1 + @int2
END&lt;/p>
&lt;p>In tSQLt we’ll use a test class to group similar tests together, which will enable us to run a suite of tests in one go. Since this is for TSQLTuesday I’ll create a test class named testTSQLTuesday.&lt;/p>
&lt;p>EXEC tSQLt.NewTestClass &amp;rsquo;testTSQLTuesday&amp;rsquo;;
GO&lt;/p>
&lt;p>This is basically just a schema for our tests to live in. I’m now ready to create my first test. The documentation for tSQLt has some good examples to get us started, and I followed the ‘AssertEquals’ example from their &lt;a class="link" href="https://tsqlt.org/user-guide/tsqlt-tutorial/" target="_blank" rel="noopener"
>tutorial&lt;/a>.&lt;/p>
&lt;p>When we run our tSQLt tests the stored procedure name will be the name of the test, so it’s important to make those meaningful. I named this one ‘test the addTwoNumbers function works’.&lt;/p>
&lt;p>From the little I do know about test-driven development, I understand we should write the test to initially fail. That’ll confirm that I haven’t set it up in a way that will provide false positives. In the below test I’m saying that I expect the sum of 1 and 2 calculated by my function to be 4, obviously untrue.&lt;/p>
&lt;p>-- create the test to fail
CREATE OR ALTER PROCEDURE testTSQLTuesday.[test the addTwoNumbers function works]
AS
BEGIN
DECLARE @actual INT;
DECLARE @testInt1 INT = 1;
DECLARE @testInt2 INT = 2;&lt;/p>
&lt;pre>&lt;code>SELECT @actual = dbo.AddTwoNumbers(@testInt1, @testInt2);
DECLARE @expected INT = 4;
EXEC tSQLt.AssertEquals @expected, @actual;
&lt;/code>&lt;/pre>
&lt;p>END;&lt;/p>
&lt;p>GO&lt;/p>
&lt;p>You can see this is a pretty simple test to set up. I declared two numbers that I’ll pass to the function and then fill in what I expect the result to be (still the wrong answer at this point).&lt;/p>
&lt;p>To run the test I’ll use the tSQLt.Run stored procedure, passing in my test class name.&lt;/p>
&lt;p>EXEC tSQLt.Run &amp;rsquo;testTSQLTuesday';&lt;/p>
&lt;p>Reviewing the messages pane in SSMS I can see my test has failed, as expected, and it let’s you know it expected 4 but got 3.&lt;/p>
&lt;p>[testTSQLTuesday].[test the addTwoNumbers function works] failed: (Failure) Expected: &amp;lt;4&amp;gt; but was: &amp;lt;3&amp;gt;&lt;/p>
&lt;p>+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
|Test Execution Summary|
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p>
&lt;h2 id="1-testtsqltuesdaytest-the-addtwonumbers-function-works-13failure">|No|Test Case Name |Dur(ms)|Result |
+&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;-+&amp;mdash;&amp;mdash;-+
|1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 13|Failure|&lt;/h2>
&lt;h2 id="test-case-summary-1-test-cases-executed-0-succeeded-1-failed-0-errored">Msg 50000, Level 16, State 10, Line 43
Test Case Summary: 1 test case(s) executed, 0 succeeded, 1 failed, 0 errored.&lt;/h2>
&lt;p>Completion time: 2020-04-11T14:37:18.8297577+01:00&lt;/p>
&lt;p>Now I’ll fix the test so if the function is working as expected the test should pass.&lt;/p>
&lt;p>-- create the test
CREATE OR ALTER PROCEDURE testTSQLTuesday.[test the addTwoNumbers function works]
AS
BEGIN
DECLARE @actual INT;
DECLARE @testInt1 INT = 1;
DECLARE @testInt2 INT = 2;&lt;/p>
&lt;pre>&lt;code>SELECT @actual = dbo.AddTwoNumbers(@testInt1, @testInt2);
DECLARE @expected INT = (@testInt1 + @testInt2);
EXEC tSQLt.AssertEquals @expected, @actual;
&lt;/code>&lt;/pre>
&lt;p>END;&lt;/p>
&lt;p>GO&lt;/p>
&lt;p>Good news, our first tSQLt test has passed.&lt;/p>
&lt;p>+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
|Test Execution Summary|
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p>
&lt;h2 id="1-testtsqltuesdaytest-the-addtwonumbers-function-works--3success">|No|Test Case Name |Dur(ms)|Result |
+&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;-+&amp;mdash;&amp;mdash;-+
|1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 3|Success|&lt;/h2>
&lt;h2 id="test-case-summary-1-test-cases-executed-1-succeeded-0-failed-0-errored">Test Case Summary: 1 test case(s) executed, 1 succeeded, 0 failed, 0 errored.&lt;/h2>
&lt;p>Completion time: 2020-04-11T16:33:56.6201835+01:00&lt;/p>
&lt;h2 id="test-a-stored-procedure-that-changes-data">Test a stored procedure that changes data&lt;/h2>
&lt;p>One thing I thought that was really cool with tSQLt is that it executes the tests in a transaction, and then rolls it back after testing to ensure things are left the way they were before we started testing.  To show this I’ve created a simple stored procedure that allows us to update an email address in the Person.EmailAddress table from AdventureWorks2017.&lt;/p>
&lt;p>CREATE PROCEDURE Person.UpdateEmailAddress
@NewEmailAddress varchar(100),
@BusinessEntityID int
AS&lt;/p>
&lt;p>UPDATE Person.EmailAddress
SET EmailAddress = @NewEmailAddress
where BusinessEntityID = @BusinessEntityID&lt;/p>
&lt;p>GO&lt;/p>
&lt;p>Then I wrote a test, similar to the first example, which compares the actual value that is in the Person.EmailAddress table after running the procedure with what I would expect to be in there.&lt;/p>
&lt;p>CREATE OR ALTER PROCEDURE testTSQLTuesday.[test the UpdateEmailAddress procedure]
AS
BEGIN
DECLARE @actual varchar(100);
DECLARE @newEmail varchar(100) = &amp;rsquo;&lt;a class="link" href="mailto:test@test.com" >test@test.com&lt;/a>&amp;rsquo;;
DECLARE @businessEntityID INT = 2;&lt;/p>
&lt;pre>&lt;code>EXEC person.UpdateEmailAddress @newEmail, @businessEntityId
SELECT @actual = EmailAddress
FROM Person.EmailAddress
WHERE BusinessEntityID = @businessEntityID
DECLARE @expected varchar(100) = @newEmail
EXEC tSQLt.AssertEquals @expected, @actual;
&lt;/code>&lt;/pre>
&lt;p>END;
GO&lt;/p>
&lt;p>I added this to the same test class so we’ll execute both tests with the same call to tSQLt.Run.&lt;/p>
&lt;p>EXEC tSQLt.Run &amp;rsquo;testTSQLTuesday';&lt;/p>
&lt;p>The results now show I have two tests that passed.&lt;/p>
&lt;p>+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+
|Test Execution Summary|
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&lt;/p>
&lt;h2 id="2-testtsqltuesdaytest-the-updateemailaddress-procedure--7success">|No|Test Case Name |Dur(ms)|Result |
+&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;-+&amp;mdash;&amp;mdash;-+
|1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 6|Success|
|2 |[testTSQLTuesday].[test the UpdateEmailAddress procedure]| 7|Success|&lt;/h2>
&lt;h2 id="test-case-summary-2-test-cases-executed-2-succeeded-0-failed-0-errored">Test Case Summary: 2 test case(s) executed, 2 succeeded, 0 failed, 0 errored.&lt;/h2>
&lt;p>Completion time: 2020-04-11T17:17:36.4799073+01:00&lt;/p>
&lt;p>To run this test we had to actually call the stored procedure that changes data in our tables. As I mentioned though, this all happened within a transaction that was never committed. We can confirm our data was unchanged by checking the table.&lt;/p>
&lt;p>SELECT EmailAddress
FROM Person.EmailAddress
WHERE BusinessEntityID = 2;&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/tsql125Tests.jpg"
loading="lazy"
alt="select statement showing email unchanged"
>&lt;/p>
&lt;p>This is just scratching the surface of what tSQLt can do. The &lt;a class="link" href="https://tsqlt.org/full-user-guide/" target="_blank" rel="noopener"
>full user guide&lt;/a> is available online.  This goes through all the available assertions (we only looked at AssertEquals here) as well as more complicated topics such as how to isolate dependencies to be able to effectively unit test.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Writing this post has been a great excuse for me to try out tSQLt and write my first T-SQL unit tests. To be honest, it was a lot easier than I expected.  I will say I’m comfortable writing pester tests for my PowerShell code so that base was helpful, but the documentation for tSQLt made getting setup and starting to test databases pretty straightforward.&lt;/p>
&lt;p>The cost of unit testing your database code is the time investment to get comfortable with the tools and to write the tests as you develop the database code.  The benefit though is a more reliable, easier to maintain database with less bugs, which will in the end make your data safer.&lt;/p>
&lt;p>I’d say there is a significant benefit to applying unit testing to databases, and I believe we’ll see a significant increase in the number of folks applying unit tests to their database environments.&lt;/p></description></item><item><title>Interactive debugging in VSCode</title><link>https://jpomfret.github.io/interactive-debugging-in-vscode/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/interactive-debugging-in-vscode/</guid><description>&lt;p>I was browsing twitter the other day when a tweet about dbatools caught my eye (I use &lt;a class="link" href="https://tweetdeck.twitter.com/" target="_blank" rel="noopener"
>TweetDeck&lt;/a> and so have a column for tweets that contain &lt;a class="link" href="http://twitter.com/psdbatools" target="_blank" rel="noopener"
>@PSdbatools&lt;/a>).&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/way0utwest/status/1242891971473137666" target="_blank" rel="noopener"
>https://twitter.com/way0utwest/status/1242891971473137666&lt;/a>&lt;/p>
&lt;p>A dbatools bug!! Oh no!&lt;/p>
&lt;p>One of the reasons this caught my eye was that I’ve seen this error in my environment with that same command. I had discounted that it was a bug and figured it was instead something in my environment. I presumed it was something related to the fact I was using containers and Azure Data Studio connections.&lt;/p>
&lt;p>Step one for dbatools bug fixing is to check for an issue on the &lt;a class="link" href="http://dbatools.io/bugs" target="_blank" rel="noopener"
>GitHub repo&lt;/a> and create one if there isn’t one already. It turned out that there was &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools/issues/6292" target="_blank" rel="noopener"
>one already created&lt;/a> so we’re covered there.&lt;/p>
&lt;p>So I figured I’d take a look and see what was happening and how we could fix it. Now I’m going to be honest with you, my usual method of debugging involves adding &lt;code>Write-Host 'Hi&lt;/code>&amp;rsquo;, or piping objects to &lt;code>Out-GridView&lt;/code>. I did start down this route, but the &lt;code>Get-DbaRegServer&lt;/code> function calls an internal function, and things quickly got complicated.&lt;/p>
&lt;p>Luckily, the &lt;a class="link" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell" target="_blank" rel="noopener"
>PowerShell extension for VSCode&lt;/a> includes a debugger so we can level up our game and use that to track down our issues. Since I haven’t already used this for my dbatools folder when I click the ‘Run’ icon on the left navigation bar I see the following:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/startDebug.jpg"
loading="lazy"
alt="run and debug window in VSCode"
>&lt;/p>
&lt;p>Pressing the ‘Run and Debug’ button will run your active file and, if you have breakpoints set up, then it’ll break at those points for you to troubleshoot. This is really useful if you have written a script and it’s not working correctly. Since I’m troubleshooting the call of a function I could write a simple script with the code to call the function, save it and then press ‘Run and Debug’. However there is another option, and that is to launch an interactive debugger. &lt;/p>
&lt;p>Pressing the ‘create a launch.json file’ link opens the command palette with the option to choose your PowerShell debug configuration. Choosing the ‘Interactive Session’ configuration means we can use the integrated console within VSCode to call functions and launch the debugger.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/debugConfig.jpg"
loading="lazy"
alt="Select a PowerShell debug configuration"
>&lt;/p>
&lt;p>This will open a launch.json file that you can edit to add more functionality and customization, but we’ll just save it as is right now.&lt;/p>
&lt;p>{
// Use IntelliSense to learn about possible attributes.
// Hover to view descriptions of existing attributes.
// For more information, visit: &lt;a class="link" href="https://go.microsoft.com/fwlink/?linkid=830387" target="_blank" rel="noopener"
>https://go.microsoft.com/fwlink/?linkid=830387&lt;/a>
&amp;ldquo;version&amp;rdquo;: &amp;ldquo;0.2.0&amp;rdquo;,
&amp;ldquo;configurations&amp;rdquo;: [
{
&amp;ldquo;name&amp;rdquo;: &amp;ldquo;PowerShell: Interactive Session&amp;rdquo;,
&amp;ldquo;type&amp;rdquo;: &amp;ldquo;PowerShell&amp;rdquo;,
&amp;ldquo;request&amp;rdquo;: &amp;ldquo;launch&amp;rdquo;,
&amp;ldquo;cwd&amp;rdquo;: &amp;quot;&amp;quot;
}
]
}&lt;/p>
&lt;p>As soon as you save it the left ‘Run and Debug’ pane will change to look like this. Now we’re ready to run the interactive debugger by pressing the green play button or F5.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DebugInteractive.jpg"
loading="lazy"
alt="Debug and Run window for PowerShell extension"
>&lt;/p>
&lt;p>So now that we’re set up, let’s start troubleshooting.&lt;/p>
&lt;p>Step one is to reproduce this issue. This particular bug was easy to reproduce. The only requirements are that you have Azure Data Studio installed and at least one connection set up, then just running &lt;code>Get-DbaRegServer&lt;/code> caused the error.&lt;/p>
&lt;p>Next we need to add some breakpoints. These need to be the positions in the code where you want to stop execution and take a look at how things are set in the moment. It’s also a great way to see if you entered certain sections of the code that may be guarded by conditional logic.&lt;/p>
&lt;p>Running &lt;code>Get-DbaRegServer&lt;/code> in the integrated console you can see the error, even down to the line from the function where the error is being thrown. In the screenshot below you can see hovering over that line in VSCode allows you to follow the link to open the function and navigate to the exact line.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDbaRegServerError.jpg"
loading="lazy"
alt="Get-DbaRegServer throws an error"
>&lt;/p>
&lt;p>Line 180 of the Get-DbaRegServer is the following:&lt;/p>
&lt;p>$tempserver.ConnectionString = $adsconn.ConnectionString&lt;/p>
&lt;p>We’ll insert a breakpoint here by clicking in the gutter at line 180.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/addBreakpoint.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now pressing F5 the interactive debugger will start, and we can rerun &lt;code>Get-DbaRegServer&lt;/code> in the interactive console. When we do that as soon as the execution gets to line 180 the code will stop, waiting for us to respond.&lt;/p>
&lt;p>You can see below that we are able to find the &lt;code>$adsconn&lt;/code> variable in the variables pane on the left and see that it’s actually an object with three values – which is the issue here – we’re expecting to only have one returned.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2020/04/breakpoint.jpg" target="_blank" rel="noopener"
>&lt;img src="https://i2.wp.com/jesspomfret.com/wp-content/uploads/2020/04/breakpoint.jpg?fit=650%2C246&amp;amp;ssl=1"
loading="lazy"
alt="VSCode code stopped at breakpoint, displaying variables"
>&lt;/a>&lt;/p>
&lt;p>I read back through the &lt;code>Get-DbaRegServer&lt;/code> function to find where the &lt;code>$adsconn&lt;/code> variable was set and found it was from calling the internal function &lt;code>Get-ADSConnection&lt;/code>. I added in another breakpoint within that function to dig in deeper.&lt;/p>
&lt;p>Adding the breakpoint within the second function means that when we call &lt;code>Get-RegServer&lt;/code> and then that calls &lt;code>Get-ADSConnection&lt;/code> the code will wait within the second function and allow you to inspect variables within that function.&lt;/p>
&lt;p>This meant that I was able to determine that there were several connection strings being returned for each server and that we needed to filter down to one.&lt;/p>
&lt;p>Changing line 174 in the &lt;code>Get-DbaRegServer&lt;/code> function to include an additional filter, shown below, meant that only one connection string was returned and solved the problem.&lt;/p>
&lt;p>$adsconn = $adsconnection | Where-Object { $_.server -eq $server.Options[&amp;lsquo;server&amp;rsquo;] -and -not $_.database }&lt;/p>
&lt;p>Hopefully this walkthrough shows a useful way of using the interactive debugger to hunt down bugs.&lt;/p></description></item><item><title>Keeping the demo gods at bay with Pester</title><link>https://jpomfret.github.io/keeping-the-demo-gods-at-bay-with-pester/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/keeping-the-demo-gods-at-bay-with-pester/</guid><description>&lt;p>A short while ago (it’s getting further and further away, but let’s stick with short for now) I was a football/soccer player. As with many athletes, I was pretty superstitious as far as my pregame routine. I always felt better going out onto the pitch if everything had gone smoothly as I got ready.  I put my boots, shin-pads and socks on in a certain order and even taped my socks up in a certain way. The good news is I’ve managed to find a slightly more reliable way to get ready for my presentations – and I’m going to share the secret.&lt;/p>
&lt;p>First you put your right sock on, then your left sock on. Follow that by putting on your right shoe, and then your left shoe… just joking. You use Pester tests!&lt;/p>
&lt;p>If you don’t know what &lt;a class="link" href="https://pester.dev/" target="_blank" rel="noopener"
>Pester&lt;/a> is, it’s a test framework for PowerShell.  In the simplest explanation, using their Domain-Specific Language (DSL) you describe how things should look. If all looks good it returns output in green and if it doesn’t you get red output.  There are a lot of great use cases for Pester, like using it to ensure your code does what it’s supposed to, using it to validate your SQL Server environment (&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a>), or in this example using it to make sure your demos are setup and ready to go.&lt;/p>
&lt;p>When I’m preparing for a presentation I go through the demos over and over again, so it’s easy to accidentally leave things in a state that will cause issues when I go to do my demos in the presentation. If you’re creating a table, for example, during the demo and you already created it practicing and then forgot to drop it, the demo gods will strike and it’ll fail when it matters most! A simple Pester test to check whether the table exists will solve this issue.&lt;/p>
&lt;p>So what do I test?&lt;/p>
&lt;p>Last Wednesday I presented my ‘Life hacks: dbatools Edition’ session for the &lt;a class="link" href="https://www.meetup.com/Southampton-Data-Platform-and-Cloud-Group/" target="_blank" rel="noopener"
>Southampton Data Platform and Cloud meetup&lt;/a> so I’ll talk you through the tests I ran to make sure I was ready to present that session, and it’s a demo heavy one!&lt;/p>
&lt;p>First things first, I test that I can import the dbatools module. I make sure I’m getting the version and the number of commands I expect. dbatools puts out new versions all the time, so I usually update this in the weeks leading up to my presentation as I’m practicing.&lt;/p>
&lt;p>Describe &amp;ldquo;Module is good to go&amp;rdquo; {
Context &amp;ldquo;dbatools imports&amp;rdquo; {
$null = Import-Module dbatools
$module = Get-Module dbatools
It &amp;ldquo;Module was imported&amp;rdquo; {
$module | Should Not BeNullOrEmpty
}
It &amp;ldquo;Module version is 1.0.99&amp;rdquo; {
$module.Version | Should Be &amp;ldquo;1.0.99&amp;rdquo;
}
It &amp;ldquo;Module should import 587 commands&amp;rdquo; {
(get-command -module dbatools | Measure).Count | Should Be 587
}
}
}&lt;/p>
&lt;p>My demo setup involves two containers running on my laptop. Because of that, I’m using the sa credential to connect and I’m setting some PSDefaultParameterValues so I don’t have to include the &lt;code>$credential&lt;/code> in every function call. I can test all that is setup correctly like so.&lt;/p>
&lt;p>Describe &amp;ldquo;Credentials exist&amp;rdquo; {
Context &amp;ldquo;Credential exists&amp;rdquo; {
It &amp;ldquo;Credential is not null&amp;rdquo; {
$credential | Should Not BeNullOrEmpty
}
}
Context &amp;ldquo;username is sa&amp;rdquo; {
It &amp;ldquo;Username is sa&amp;rdquo; {
$credential.UserName | Should Be &amp;ldquo;sa&amp;rdquo;
}
}
Context &amp;ldquo;PSDefaultParameterValues are set&amp;rdquo; {
$params = $PSDefaultParameterValues
It &amp;ldquo;PSDefaultParameterValues contains expected values&amp;rdquo; {
$params.Keys -contains &amp;lsquo;*:SqlCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:SourceSqlCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:DestinationCredential&amp;rsquo; | Should Be True
$params.Keys -contains &amp;lsquo;*:DestinationSqlCredential&amp;rsquo; | Should Be True
}
}
}&lt;/p>
&lt;p>I then have a couple of simple checks to make sure I can connect to both my instances.&lt;/p>
&lt;p>Describe &amp;ldquo;Two instances are available&amp;rdquo; {
Context &amp;ldquo;Two instances are up&amp;rdquo; {
$mssql1 = Connect-DbaInstance -SqlInstance mssql1
$mssql2 = Connect-DbaInstance -SqlInstance mssql2
It &amp;ldquo;mssql1 is available&amp;rdquo; {
$mssql1.Name | Should Not BeNullOrEmpty
$mssql1.Name | Should Be &amp;lsquo;mssql1&amp;rsquo;
}
It &amp;ldquo;mssql2 is available&amp;rdquo; {
$mssql2.Name | Should Not BeNullOrEmpty
$mssql2.Name | Should Be &amp;lsquo;mssql2&amp;rsquo;
}
}
}&lt;/p>
&lt;p>I then make sure that my databases are set up as expected. I am using two databases on my mssql1 SQL Server instance, AdventureWorks2017 and DatabaseAdmin. I make sure each of those exist, are online, and that the compatibility level is set correctly. I also check that the indexes on the Employee table are set up as I expect since I use those in my demos.&lt;/p>
&lt;p>Describe &amp;ldquo;mssql1 databases are good&amp;rdquo; {
Context &amp;ldquo;AdventureWorks2017 is good&amp;rdquo; {
$db = Get-DbaDatabase -SqlInstance mssql1
$adventureWorks = $db | where name -eq &amp;lsquo;AdventureWorks2017&amp;rsquo;
It &amp;ldquo;AdventureWorks2017 is available&amp;rdquo; {
$adventureWorks | Should Not BeNullOrEmpty
}
It &amp;ldquo;AdventureWorks status is normal&amp;rdquo; {
$adventureWorks.Status | Should Be Normal
}
It &amp;ldquo;AdventureWorks Compat is 140&amp;rdquo; {
$adventureWorks.Compatibility | Should Be 140
}
}
Context &amp;ldquo;Indexes are fixed on HumanResources.Employee (bug)&amp;rdquo; {
$empIndexes = (Get-DbaDbTable -SqlInstance mssql1 -Database AdventureWorks2017 -Table Employee).indexes | select name, IsUnique
It &amp;ldquo;There are now just two indexes&amp;rdquo; {
$empIndexes.Count | Should Be 2
}
It &amp;ldquo;There should be no unique indexes&amp;rdquo; {
$empIndexes.IsUnique | Should BeFalse
}
}
Context &amp;ldquo;DatabaseAdmin is good&amp;rdquo; {
$db = Get-DbaDatabase -SqlInstance mssql1
$DatabaseAdmin = $db | where name -eq &amp;lsquo;DatabaseAdmin&amp;rsquo;
It &amp;ldquo;DatabaseAdmin is available&amp;rdquo; {
$DatabaseAdmin | Should Not BeNullOrEmpty
}
It &amp;ldquo;DatabaseAdmin status is normal&amp;rdquo; {
$DatabaseAdmin.Status | Should Be Normal
}
It &amp;ldquo;DatabaseAdmin Compat is 140&amp;rdquo; {
$DatabaseAdmin.Compatibility | Should Be 140
}
}
}&lt;/p>
&lt;p>One of my demos shows the backup history for AdventureWorks, so I test that with Pester before I start to make sure there is history to show. Nothing worse than getting up to show a wonderful set of dbatools functions and nothing being returned because I haven’t actually taken any backups!&lt;/p>
&lt;p>Describe &amp;ldquo;Backups worked&amp;rdquo; {
Context &amp;ldquo;AdventureWorks was backed up&amp;rdquo; {
$instanceSplat = @{
SqlInstance = &amp;lsquo;mssql1&amp;rsquo;
}
It &amp;ldquo;AdventureWorks has backup history&amp;rdquo; {
Get-DbaDbBackupHistory @instanceSplat | Should Not BeNullOrEmpty
}
}
}&lt;/p>
&lt;p>While I was writing my demos I came across issues where my PowerShell environment was set to x86 so I added a test for that to make sure it doesn’t happen again.&lt;/p>
&lt;p>Describe &amp;ldquo;Proc architecture is x64&amp;rdquo; {
Context &amp;ldquo;Proc arch is good&amp;rdquo; {
It &amp;ldquo;env:processor_architecture should be AMD64&amp;rdquo; {
$env:PROCESSOR_ARCHITECTURE | Should Be &amp;ldquo;AMD64&amp;rdquo;
}
}
}&lt;/p>
&lt;p>Finally, I check to see what’s running on my computer. Zoomit, everyone’s favourite screen zoom tool should be running, and we should make sure that Slack and Teams are not.&lt;/p>
&lt;p>Describe &amp;ldquo;Check what&amp;rsquo;s running&amp;rdquo; {
$processes = Get-Process zoomit*, teams, slack -ErrorAction SilentlyContinue
Context &amp;ldquo;ZoomIt is running&amp;rdquo; {
It &amp;ldquo;ZoomIt64 is running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Zoomit64&amp;rsquo;) | Should Not BeNullOrEmpty
}
It &amp;ldquo;Slack is not running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Slack&amp;rsquo;) | Should BeNullOrEmpty
}
It &amp;ldquo;Teams is not running&amp;rdquo; {
($processes | Where-Object ProcessName -eq &amp;lsquo;Teams&amp;rsquo;) | Should BeNullOrEmpty
}
}
}&lt;/p>
&lt;p>Now there are obviously ways that the demo gods can still strike, but using Pester to test your demos is a great way to try and tilt the odds in your favour.&lt;/p>
&lt;p>You can view all the code, including the tests, for this presentation on my &lt;a class="link" href="https://github.com/jpomfret/demos/tree/master/LifeHacks_dbatools" target="_blank" rel="noopener"
>Github&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s what the output looks like:&lt;/p>
&lt;p>Executing all tests in &amp;lsquo;.\Tests\demo.tests.ps1&amp;rsquo;&lt;/p>
&lt;p>Executing script .\Tests\demo.tests.ps1&lt;/p>
&lt;p>Describing Module is good to go&lt;/p>
&lt;pre>&lt;code>Context dbatools imports
\[+\] Module was imported 1.59s
\[+\] Module version is 1.0.99 287ms
\[+\] Module should import 587 commands 162ms
&lt;/code>&lt;/pre>
&lt;p>Describing Credentials exist&lt;/p>
&lt;pre>&lt;code>Context Credential exists
\[+\] Credential is not null 263ms
Context username is sa
\[+\] Username is sa 83ms
Context PSDefaultParameterValues are set
\[+\] PSDefaultParameterValues contains expected values 80ms
&lt;/code>&lt;/pre>
&lt;p>Describing Two instances are available&lt;/p>
&lt;pre>&lt;code>Context Two instances are up
\[+\] mssql1 is available 592ms
\[+\] mssql2 is available 26ms
&lt;/code>&lt;/pre>
&lt;p>Describing mssql1 databases are good&lt;/p>
&lt;pre>&lt;code>Context AdventureWorks2017 is good
\[+\] AdventureWorks2017 is available 863ms
\[+\] AdventureWorks status is normal 32ms
\[+\] AdventureWorks Compat is 140 46ms
Context Indexes are fixed on HumanResources.Employee (bug)
\[+\] There are now just two indexes 1.53s
\[+\] There should be no unique indexes 49ms
Context DatabaseAdmin is good
\[+\] DatabaseAdmin is available 256ms
\[+\] DatabaseAdmin status is normal 15ms
\[+\] DatabaseAdmin Compat is 140 17ms
&lt;/code>&lt;/pre>
&lt;p>Describing Backups worked&lt;/p>
&lt;pre>&lt;code>Context AdventureWorks was backed up
\[+\] AdventureWorks has backup history 627ms
&lt;/code>&lt;/pre>
&lt;p>Describing Proc architecture is x64&lt;/p>
&lt;pre>&lt;code>Context Proc arch is good
\[+\] env:processor\_architecture should be AMD64 125ms
&lt;/code>&lt;/pre>
&lt;p>Describing Check what&amp;rsquo;s running&lt;/p>
&lt;pre>&lt;code>Context ZoomIt is running
\[+\] ZoomIt64 is running 150ms
\[+\] Slack is not running 43ms
\[+\] Teams is not running 17ms
&lt;/code>&lt;/pre>
&lt;p>Tests completed in 6.86s
Tests Passed: 21, Failed: 0, Skipped: 0, Pending: 0, Inconclusive: 0&lt;/p>
&lt;p>Good news, all passed and I&amp;rsquo;m ready to give my demos!&lt;/p></description></item><item><title>Using AutomatedLab to setup a SQL Server Lab</title><link>https://jpomfret.github.io/using-automatedlab-to-setup-a-sql-server-lab/</link><pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/using-automatedlab-to-setup-a-sql-server-lab/</guid><description>&lt;p>I’ve recently found myself in a few situations where I’ve needed a certain lab setup to test something.  Most scenarios these days I find myself firing up some docker containers to run demos or tests against. However, there are a few circumstances where you may need a little more than that, perhaps to test something Windows OS related (my container setup is using Linux) or just to replicate a production environment more closely than you can with containers.  In these situations I turn to PowerShell.&lt;/p>
&lt;p>There is a fantastic PowerShell module called &lt;a class="link" href="https://github.com/AutomatedLab/AutomatedLab" target="_blank" rel="noopener"
>AutomatedLab&lt;/a> that can enable you to easily build out a lab for the  specific scenario you need to test. Even better is the module comes with 70 sample scripts that you can start with and adapt to meet your needs.&lt;/p>
&lt;p>The module gives you the option to work with Hyper-V or VMWare. I will say most of the examples are using Hyper-V, and that is what I’ll be using also.&lt;/p>
&lt;p>For my lab I want a SQL Server 2019 instance joined to a domain, and a separate client machine that I can manage the SQL Server from. On the client I would need to be able to connect to the internet as I want to be able to download PowerShell modules from the gallery easily.&lt;/p>
&lt;h2 id="step-1-get-the-module">&lt;strong>Step 1: Get the module&lt;/strong>&lt;/h2>
&lt;p>There is some setup involved to make sure the pieces needed to build your lab are in the right places.  AutomatedLab gives you two options to ensure you’re set up correctly. &lt;a class="link" href="https://github.com/AutomatedLab/AutomatedLab/wiki/1.-Installation" target="_blank" rel="noopener"
>The documentation on GitHub&lt;/a> already does a great job of outlining these so I’ll just point you in that direction for reference.&lt;/p>
&lt;p>In my lab I used the second method to download from the PowerShell Gallery and then run the configuration command. I ran the following to get my laptop setup:&lt;/p>
&lt;p>Install-Module -Name AutomatedLab -AllowClobber
New-LabSourcesFolder&lt;/p>
&lt;h2 id="step-2-stage-the-isos">&lt;strong>Step 2: Stage the ISOs&lt;/strong>&lt;/h2>
&lt;p>In order to build our lab we will need to store the ISOs needed for installation where AutomatedLab can access them. We used the defaults during our install so that location is &lt;code>C:\LabSources\ISOs&lt;/code>.  You can see below I’ve placed the ISOs for Windows Server 2019, SQL Server 2016 and SQL Server 2019 in this folder.&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/jesspomfret.com/wp-content/uploads/2020/02/isos.jpg?fit=650%2C112&amp;amp;ssl=1"
loading="lazy"
alt="iso folder"
>&lt;/p>
&lt;p>You can test whether AutomatedLab can see the operating system ISOs by using the &lt;code>Get-AvailableOperatingSystems&lt;/code> command. You can see below it has found my ISO folder and therefore lists the available operating systems that I can use. You’ll notice that the Windows Server 2019 ISO allows me to install two editions, Standard or Datacenter, as well as choosing between installing the core version or the more traditional desktop experience. &lt;/p>
&lt;p>It is important to ensure you have the appropriate licensing in place for the OS you install. You can use evaluation editions or enter product keys at a later time.&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/jesspomfret.com/wp-content/uploads/2020/02/AvailableOS.jpg?fit=650%2C79&amp;amp;ssl=1"
loading="lazy"
alt="available isos"
>&lt;/p>
&lt;h2 id="step-3-craft-the-lab-setup-script">&lt;strong>Step 3: Craft the lab setup script&lt;/strong>&lt;/h2>
&lt;p>Now this sounds like a scary step, but fear not! I mentioned before the AutomatedLab team have included over 70 sample scripts, which means it’s likely there is something similar already written. I found that there were two samples that covered all the parts needed for my lab, so I was able to combine them to create the exact scenario needed.&lt;/p>
&lt;p>I’ve added comments throughout the script below so you can easily see the pieces needed:&lt;/p>
&lt;p>&lt;a class="link" href="https://gist.github.com/jpomfret/15dd515bfd421ae13c047f95d65eb333" target="_blank" rel="noopener"
>https://gist.github.com/jpomfret/15dd515bfd421ae13c047f95d65eb333&lt;/a>&lt;/p>
&lt;p>By default, if you don’t specify a the &lt;code>-VmPath&lt;/code> parameter on the &lt;code>New-LabDefinition&lt;/code> function AutomatedLab will test the speed of the available drives and choose the fastest.&lt;/p>
&lt;h2 id="step-4-install-the-lab">&lt;strong>Step 4: Install the lab&lt;/strong>&lt;/h2>
&lt;p>With the script written, open the PowerShell console using ‘Run as Administrator’ and execute the script. &lt;/p>
&lt;p>.\AutomatedLab_SQLServer.ps1&lt;/p>
&lt;p>There will also be some security settings to consider and accept for the first install such as enabling WinRM and adding ‘*’ to your TrustedHosts. Once those are confirmed and the prework is complete, a toast will pop up showing that the lab has started building:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/StartToast.jpg"
loading="lazy"
alt="lab started toast"
>&lt;/p>
&lt;p>This is the perfect time to grab a cup of coffee as there are some steps that can take a while, especially if this is the first install.&lt;/p>
&lt;p>AutomatedLab uses a base disk for installations of the same OS. Creating this base disk can take a while but once it is created it significantly reduces the amount of disk space you need for your lab. If you deploy 4 VMs with the same OS, as I have here, the base disk will be created and then differential disks can be used for each VM.&lt;/p>
&lt;p>If you build a second lab in the same VM folder with the same OS it can again reuse this base disk.&lt;/p>
&lt;h2 id="step-5-use-the-lab">&lt;strong>Step 5: Use the lab&lt;/strong>&lt;/h2>
&lt;p>Once the install is complete, another toast will appear and it’s time to log in and start using your lab.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/completetoast.jpg"
loading="lazy"
alt="lab finished toast"
>&lt;/p>
&lt;p>The summary we displayed using &lt;code>Show-LabDeploymentSummary&lt;/code> will show the machine names, administrator password and some other connection information. We have again used the defaults for these settings but they can be overwritten.&lt;/p>
&lt;h2 id="remove-the-lab">&lt;strong>Remove the lab&lt;/strong>&lt;/h2>
&lt;p>Once you are done with your lab you can quickly and easily run the following to destroy all the pieces created by your lab (except the base OS disk).&lt;/p>
&lt;p>Remove-Lab SQLLab&lt;/p></description></item><item><title>Backups with dbatools &amp; BurntToast</title><link>https://jpomfret.github.io/backups-with-dbatools-burnttoast/</link><pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/backups-with-dbatools-burnttoast/</guid><description>&lt;p>I have just a quick tip for you today using the &lt;a class="link" href="https://github.com/Windos/BurntToast" target="_blank" rel="noopener"
>BurntToast&lt;/a> module to notify us when a backup is complete. As DBAs there is always plenty to do, so we don’t want to have to sit and watch a long running script to catch the moment when it finishes.  Usually what happens to me is I kick off the script, move on to something else and then totally forget about it, perhaps until someone asks if it’s done yet. Oops. Well this tip will help avoid that.&lt;/p>
&lt;p>The &lt;a class="link" href="https://github.com/Windos/BurntToast" target="_blank" rel="noopener"
>BurntToast&lt;/a> module, created by Josh King (&lt;a class="link" href="https://toastit.dev/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/windosnz" target="_blank" rel="noopener"
>t&lt;/a>), allows you to easily add Windows toast notifications to your PowerShell scripts. I’m going to show you how to use BurntToast to keep track of a database backup.&lt;/p>
&lt;p>By this time you should know about my love for &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a>, so today we’re going to take a look at how to take a &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/copy-only-backups-sql-server?view=sql-server-ver15" target="_blank" rel="noopener"
>copy-only&lt;/a> backup.  This is a backup that doesn’t upset the LSN chain of your regular database backups and can be used to just save a specific point in time for perhaps pre-upgrade, or to restore the database somewhere else.&lt;/p>
&lt;p>The following script will take a copy-only backup of the AdventureWorks2019 database on the mssql2 instance. Since I’m not specifying a path it will go to the default backup directory defined for that instance. I’m also saving the results of the command to the &lt;code>$backup&lt;/code> variable.&lt;/p>
&lt;p>The second section below that will run directly after the backup completes will use the results in $backup to notify us using BurntToast:&lt;/p>
&lt;p>## Take a copy only backup, using splatting for readability
$backupSplat = @{
SqlInstance = &amp;ldquo;mssql2&amp;rdquo;
Database = &amp;ldquo;AdventureWorks2019&amp;rdquo;
CopyOnly = $true
}
$backup = Backup-DbaDatabase @backupSplat&lt;/p>
&lt;h2 id="notify-the-backup-is-complete-using-splatting-for-readability">Notify the backup is complete, using splatting for readability&lt;/h2>
&lt;p>toastSplat = @{
Text = (&amp;ldquo;Backup of {0} completed in {1}&amp;rdquo; -f $backup.Database, $backup.Duration.ToString())
AppLogo = &amp;ldquo;C:\temp\dbatools.png&amp;rdquo;
}
New-BurntToastNotification @toastSplat&lt;/p>
&lt;p>That’s it, 2 commands to take a backup and notify us on completion.  I’ve also used the &lt;code>-AppLogo&lt;/code> parameter to add the &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools/blob/development/bin/dbatools.png" target="_blank" rel="noopener"
>dbatools logo&lt;/a>. You can see that this backup only took 1 second to complete (hopefully I didn’t get sidetracked in that time) but if the backup takes a few minutes or longer this is a useful tip to let you know when it’s finished.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/dbatoolsToast.jpg"
loading="lazy"
>&lt;/p>
&lt;p>You could also add some error handling to this script to make it a little more robust by perhaps using try-catch to change the message if the backup was unsuccessful.&lt;/p>
&lt;p>This is just one use case for using BurntToast with dbatools You could use this in any script that you’re writing to keep you notified when it’s done. This will allow you to get on with whatever else you have on your plate and not have to worry about remembering that backup you kicked off a while ago.&lt;/p>
&lt;p>Since writing this post I saw that Josh has created the &lt;a class="link" href="https://github.com/Windos/PoshNotify" target="_blank" rel="noopener"
>PoshNotify&lt;/a> module which allows you to generate popups cross-platform. So if you are not using Windows you can adapt the above script to use this module instead.&lt;/p></description></item><item><title>T-SQL Tuesday #123: Summary of Life Hacks</title><link>https://jpomfret.github.io/t-sql-tuesday-#123-summary-of-life-hacks/</link><pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#123-summary-of-life-hacks/</guid><description>&lt;p>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/p>
&lt;p>There was an amazing response to my &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-123/" target="_blank" rel="noopener"
>TSQL2sday prompt for February&lt;/a> 2020 where I encouraged folks to share their life hacks. So firstly, thanks to everyone who participated.&lt;/p>
&lt;p>A lot of people had more than one life hack so I recommend reading all of the posts linked to below. For this summary post I’ve tried to pick one or two hacks from each post and group them into logical buckets.&lt;/p>
&lt;p>There are several hacks shared that I plan on integrating into my life, and I hope this post will serve as a good reference for us all going forward.&lt;/p>
&lt;p>If I missed any posts please let me know!&lt;/p>
&lt;h2 id="sql-server-management-studio">&lt;strong>SQL Server Management Studio&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://allthezerosmatter.com/2020/02/11/t-sql-tuesday-123-life-hacks-to-make-your-day-easier-custom-shortcuts-in-ssms-ads/" target="_blank" rel="noopener"
>Chris&lt;/a>, a first time TSQL2sday poster (welcome!), talks about adding shortcuts to run common queries, for example selecting the top 100 rows, or getting the number of rows in a table.&lt;/li>
&lt;li>&lt;a class="link" href="https://jasonbrimhall.info/2020/02/12/top-3-database-life-hacks/" target="_blank" rel="noopener"
>Jason&lt;/a> has three life hacks for us, my favourite being the scroll bar map in SSMS. I use that a lot in VSCode, and had no idea you could do that in SSMS as well.&lt;/li>
&lt;li>&lt;a class="link" href="https://sqlstudies.com/2020/02/11/configure-your-tools-t-sql-tuesday-123/" target="_blank" rel="noopener"
>Kenneth’s&lt;/a> hack is to configure your tools and shows some great tips for making SQL Server Management Studio work for you.&lt;/li>
&lt;li>&lt;a class="link" href="https://dallasdbas.com/tsqltuesday-life-hacks/" target="_blank" rel="noopener"
>Kevin&lt;/a> talks us through executing sp_whoisactive as a temporary procedure to leave no trace and using SSMS registered servers.&lt;/li>
&lt;li>&lt;a class="link" href="http://sqlworldwide.com/t-sql-tuesday-123-ssms-tips-to-make-your-day-easier/" target="_blank" rel="noopener"
>Taiob&lt;/a> has some great tips to make using SSMS more effective, including adding your own keyboard shortcuts.&lt;/li>
&lt;li>&lt;a class="link" href="https://timharkin.com/life-hacks-to-make-your-day-easier-t-sql-tuesday-123/" target="_blank" rel="noopener"
>Tim&lt;/a> shares a few hacks including multiline editing in SSMS, and a homemade bread proofing box.&lt;/li>
&lt;/ul>
&lt;h2 id="t-sqlpowershell">T-SQL/PowerShell&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://jimbabwe.co.za/2020/02/12/t-sql-tuesday-123-life-hacking-by-opening-windows/" target="_blank" rel="noopener"
>James&lt;/a> has a hack for using windowing functions in T-SQL to aggregate aggregates and a really well laid out post to explain that concept.&lt;/li>
&lt;li>&lt;a class="link" href="https://lisagb.info/archives/71" target="_blank" rel="noopener"
>Lisa&lt;/a> has a couple of stored procedures to add to your toolbelt. One that recursively finds objects your code calls, and one that recursively finds code that calls your objects.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.sqlstad.nl/rants-and-rambling/t-sql-tuesday-123-life-hacks-that-make-your-life-easier/" target="_blank" rel="noopener"
>Sander’s&lt;/a> life hack is to make the most out of your PowerShell profile, and discusses the contents of his profile.&lt;/li>
&lt;/ul>
&lt;h2 id="keyboardword-shortcuts">Keyboard/word Shortcuts&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.cathrinewilhelmsen.net/2020/02/11/keyboard-shortcuts-moving-text-lines-moving-windows/" target="_blank" rel="noopener"
>Cathrine&lt;/a> has some great keyboard shortcuts to make us more productive, showing us how to move both lines of text and windows around.&lt;/li>
&lt;li>&lt;a class="link" href="https://msurasky.wixsite.com/sqlcorner/post/t-sql-tuesday-life-hacks-to-make-your-day-easier" target="_blank" rel="noopener"
>Martin&lt;/a> levels up from using keyboard shortcuts to keyWORD shortcuts, including some to insert useful code snippets into SSMS for tasks he completes often.&lt;/li>
&lt;/ul>
&lt;h2 id="organisationtime-management">&lt;strong>Organisation/Time management&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://flxsql.com/t-sql-tuesday-123-lifehacks-to-make-your-day-easier/" target="_blank" rel="noopener"
>Andy&lt;/a> has several great life hacks which he mentions are also habits at this point. He includes using cloud storage to sync up your documents and VSCode extensions across computers.&lt;/li>
&lt;li>&lt;a class="link" href="https://sqlblog.org/2020/02/11/t-sql-tuesday-123-life-hacks" target="_blank" rel="noopener"
>Aaron&lt;/a> has a ton of little life hacks built into his daily schedule while he works from home. My favourites are a scheduled hour of ‘disengagement’ time to focus on non-collaborative work and his travel pack of gear.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.brentozar.com/archive/2020/02/tsql2sday-my-life-hack-is-an-hourglass-yes-an-hourglass/" target="_blank" rel="noopener"
>Brent&lt;/a> suggests using a (half) hour glass to assess progress made and keep him on track. He also links to a beautiful wallet damaging hourglass.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.drewsk.tech/2020/02/11/t-sql-tuesday-123/" target="_blank" rel="noopener"
>Drew&lt;/a> highlights the Eisenhower matrix to improve delegation, and then goes on to list several more great productivity tips. Also bonus points for including a February fact!&lt;/li>
&lt;li>&lt;a class="link" href="https://curiousaboutdata.com/2020/02/11/t-sql-tuesday-123-life-hacks-to-make-your-day-easier/amp/?__twitter_impression=true" target="_blank" rel="noopener"
>Mala&lt;/a> has two great tips for us, first to get the benefits of both working at home and in the office. Secondly, a way to squeeze in some learning time.&lt;/li>
&lt;li>&lt;a class="link" href="https://nocolumnname.blog/2020/02/11/t-sql-tuesday-123-life-hacks-to-make-your-day-easier/" target="_blank" rel="noopener"
>Shane’s&lt;/a> life hack is not just to use the Pomodoro technique, which is pretty manual. He’s written a PowerShell script for it complete with toast notifications.&lt;/li>
&lt;li>&lt;a class="link" href="https://tracyboggiano.com/archive/2020/02/t-sql-tuesday-123-life-hacks/" target="_blank" rel="noopener"
>Tracey&lt;/a> has a host of useful life hacks including queuing Ola’s index maintenance scripts and using a real paper planner for goals. Tracey also reveals she is the host for next month’s TSQL2sday!&lt;/li>
&lt;/ul>
&lt;h2 id="toolsapps">Tools/Apps&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://eitanblumin.com/2020/02/11/t-sql-tuesday-123-life-hacks-to-make-your-day-easier/" target="_blank" rel="noopener"
>Eitan&lt;/a> talks about moving from an individual contributor to a manager and now utilising a bunch of “digital assistants” to get his menial tasks done.&lt;/li>
&lt;li>&lt;a class="link" href="https://sqlzelda.wordpress.com/2020/02/11/t-sql-tuesday-123-improve-focus-through-speech" target="_blank" rel="noopener"
>Elizabeth’s&lt;/a> life hack helped her to get her book written (I saw it just came out!). She’s been using dictation software to get the ideas out of her head and onto paper.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.sqlskills.com/blogs/erin/life-hacks/" target="_blank" rel="noopener"
>Erin&lt;/a> has two life hacks for us. First, using the Toggl app to track your time. The second is listening to movies in the background while working to improve focus.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.kevinrchant.com/2020/02/11/t-sql-tuesday-123-favourite-sql-server-life-hack/" target="_blank" rel="noopener"
>Kevin&lt;/a> shares several tools he likes to use including Glenn Berry’s diagnostic scripts especially for finding indexes, dbatools and some other free community tools. Plus wearing jackets with lots of pockets for short flights.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.michaelscalise.com/2020/02/11/t-sql-tuesday-123-life-hacks-to-make-your-day-easier/" target="_blank" rel="noopener"
>Mike&lt;/a>, another first time TSQL2sday poster (welcome to you too!), is using Apple’s ‘Speak Screen’ option to catch up on his backlog of blogs to read.&lt;/li>
&lt;/ul>
&lt;h2 id="personalhealth">Personal/Health&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="http://dataperfpro.com/t-sql-tuesday-123-being-will-parker/" target="_blank" rel="noopener"
>Allen&lt;/a> has a great life hack for dealing with people. Inspired by a role he played, Will Parker, he tries to be friendly and open to everyone.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.bronowski.it/blog/2020/02/t-sql-tuesday-123-life-hacks-to-make-your-day-easier/" target="_blank" rel="noopener"
>Mikey&lt;/a> has a simple but effective hack- get involved with #SQLFamily. He has a good list on where to find people.&lt;/li>
&lt;li>&lt;a class="link" href="https://richbenner.com/2020/02/t-sql-123/" target="_blank" rel="noopener"
>Rich&lt;/a> has a life hack that I can relate to, getting your exercise in, and I’d agree it’s a great life hack both physically and mentally.&lt;/li>
&lt;li>&lt;a class="link" href="http://blogs.lobsterpot.com.au/2020/02/11/shortcuts-good-and-bad/" target="_blank" rel="noopener"
>Rob&lt;/a> talks about a shortcut from his childhood French lessons to handle pronouns, and goes on to talk about the importance of us all doing better with each individual’s pronouns after this past week&amp;rsquo;s SQLFamily discussions.&lt;/li>
&lt;li>&lt;a class="link" href="https://toddkleinhans.wordpress.com/2020/02/11/tsql-tuesday-123-my-life-hack-flowing-in-the-morning/" target="_blank" rel="noopener"
>Todd’s&lt;/a> life hack is to use virtual reality in the morning to get into the flow zone before he starts his day. The videos shared in this post were amazing, and I’ve added the book he references to my to-read list.&lt;/li>
&lt;/ul></description></item><item><title>T-SQL Tuesday #123: Life hacks to make your day easier</title><link>https://jpomfret.github.io/t-sql-tuesday-#123-life-hacks-to-make-your-day-easier/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#123-life-hacks-to-make-your-day-easier/</guid><description>&lt;p>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/p>
&lt;p>It’s time for the February edition of T-SQL Tuesday. I am really grateful to be able to host this edition and look forward to reading everyone’s contributions. In case you are new to T-SQL Tuesday this is the monthly blog party started by Adam Machanic (&lt;a class="link" href="http://dataeducation.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/adammachanic" target="_blank" rel="noopener"
>t&lt;/a>) and now hosted by Steve Jones (&lt;a class="link" href="https://voiceofthedba.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/way0utwest/" target="_blank" rel="noopener"
>t&lt;/a>). It’s a way of encouraging blog posts from the community and helping to share the knowledge.&lt;/p>
&lt;p>So here we are, the first Tuesday of February. I personally always find February to be the month where my motivation is a little low. I live in the northern hemisphere so it can be a pretty dreary winter month where it still feels like there is a long way to spring (I will say this January I moved from Ohio back to England and the distinct lack of piles of snow is helping this cause somewhat). This makes my topic even more relevant as we need a little extra help to be productive and get through the month.&lt;/p>
&lt;p>My topic is looking for your favourite ‘life hack’, something you use to make your day easier. This could be anything from a keyboard shortcut in SSMS that runs ‘sp_whoisactive’, to a technique you use to get and stay organised.  It doesn’t have to be directly related to a technology, just whatever you use to make your life easier.&lt;/p>
&lt;p>Now, I’m personally a huge proponent of using keyboard shortcuts to get things done faster. In the last year or so I’ve started using Visual Studio Code as my editor of choice and the number of little ‘life hacks’ I’ve found has grown incredibly. I’m going to share a couple that I use often to get your ideas flowing.&lt;/p>
&lt;h2 id="multiline-select---ctrl--alt-direction-key">Multiline Select - Ctrl + Alt+ Direction Key&lt;/h2>
&lt;p>This is something I love for formatting queries, among other things. I know you can use T-SQL to generate some queries from the metadata but if you have a list of tables you want to truncate, for example, you can easily accomplish this. Select the start of each line by using Ctrl + Alt + down direction key, add the &lt;code>TRUNCATE TABLE&lt;/code> text and then press end to get to the end of each line, no matter the length, to add the semicolon.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/multilineselect.gif"
loading="lazy"
>&lt;/p>
&lt;p>The other use I have for this hack is to generate names and descriptions of Active Directory groups for tickets to have them created.  At my previous job we created read and admin groups for databases that users could then request access to. Multiline select made this really easy to generate the required information.&lt;/p>
&lt;p>You can use multiline select at the beginning of the row. Start by selecting the first word and copying it (Ctrl+C), then you can type to format your group name. For example, I put &lt;code>SqlDb-&lt;/code> before the database name and then &lt;code>-Read&lt;/code> afterwards.  Pressing enter at the end of the group name will create a second line for all three groups where you can add the description. Notice I can now use paste (Ctrl+V) to add the database name that we copied from each line.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/multilineselect2.gif"
loading="lazy"
>&lt;/p>
&lt;p>This ability to change multiple lines at once is really powerful and once you get the hang of what you can do with it you’ll find so many opportunities.&lt;/p>
&lt;h2 id="change-all-occurrences--ctrl--f2">&lt;strong>Change all occurrences – Ctrl + F2&lt;/strong>&lt;/h2>
&lt;p>A similar hack to my first, VS Code also lets you change multiple occurrences of characters. I say characters because you can select whole words, parts of words, or even punctuation. This is really handy, for example, for formatting a comma separated list on one row into a list with each value on a separate row.&lt;/p>
&lt;p>Carrying on from my previous example, now that we have formatted the group names and description. I can select the word ‘Read’ and replace all with ‘Admin’. Just like that I have all I need to get the group request off to the help desk for creation.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ChangeAllOccurances.gif"
loading="lazy"
>&lt;/p>
&lt;h2 id="command-palette---f1-or-ctrlshiftp">&lt;strong>Command Palette -  F1 or Ctrl+Shift+P&lt;/strong>&lt;/h2>
&lt;p>VS Code also has a really great Command Palette that offers a lot more for you to explore. A few of my favourites are:&lt;br>
- Sort Lines Ascending/Descending – Select some lines in VS Code and easily alphabetise them.&lt;br>
- Git: Undo Last Commit – Rescue that last commit back from your source control. Useful if you realised a second too late you committed to the wrong branch.&lt;br>
- File: Compare Active File With – This clearly highlights differences between two files.&lt;/p>
&lt;h2 id="over-to-you">&lt;strong>Over to you&lt;/strong>&lt;/h2>
&lt;p>I hope my VS Code life hacks have got your ideas flowing, so now it’s over to you. Finally, the important part, the rules. You can &lt;a class="link" href="http://tsqltuesday.azurewebsites.net/rules/" target="_blank" rel="noopener"
>read the full rules here&lt;/a>, but the keys are:&lt;br>
- Write about the topic described above&lt;br>
- Include the T-SQL Tuesday Logo and link back to this post&lt;br>
- Comment on this post so I make sure not to miss your contribution&lt;br>
- Post your blog on February 11th between 0:00 - 23:59 UTC&lt;/p>
&lt;p>If you have an idea for a future T-SQL Tuesday you can &lt;a class="link" href="http://tsqltuesday.azurewebsites.net/contact/" target="_blank" rel="noopener"
>contact Steve Jones&lt;/a>.&lt;/p>
&lt;h2 id="bonus-february-fact">&lt;strong>Bonus February Fact&lt;/strong>&lt;/h2>
&lt;p>Just in case anyone is still reading, while I was looking for a nice way to tie my topic to the month of February I discovered that one of the old English names for this month was Kale-monath, which means “cabbage month.” As if February couldn’t get any worse! This doesn’t really tie my topic and February together, just a useless fact to add to your collection. You’re welcome!&lt;/p></description></item><item><title>T-SQL Tuesday #122 – Imposter Syndrome</title><link>https://jpomfret.github.io/t-sql-tuesday-#122-imposter-syndrome/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#122-imposter-syndrome/</guid><description>&lt;p>&lt;a class="link" href="https://jonshaulis.com/index.php/2020/01/07/t-sql-tuesday-122-imposter-syndrome/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>What a great topic for the start of the year as we’re all thinking about goal setting and personal improvement. Thanks to Jon Shaulis (&lt;a class="link" href="https://jonshaulis.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/JonShaulis" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this first monthly blog party of the decade.&lt;/p>
&lt;p>Imposter syndrome is mentioned so often in our community, and it’s really interesting to me the wide number of people, who I consider experts, that say they still feel this sometimes. I’m really excited to read about how other people combat this feeling and hopefully pick up some tips for when it creeps into my head.&lt;/p>
&lt;p>I’ve only been speaking and blogging for a couple of years now and I’ve found myself most often feeling like an imposter as I’m getting ready for a presentation. The scenario I imagine the most is that an attendee asks a question that I don’t have an answer for, or even worse points out a mistake I made or something I said that isn’t true.&lt;/p>
&lt;p>To combat these feelings I try and prepare as fully as I can for my presentation, including reading and re-reading any Microsoft Docs on the topic. Also if others in the community have written blogs on the topic I try and read those too. To be honest though, I learn best by doing so as I’m getting ready for a presentation I make sure I work through my demos and fully understand what is happening in a lab environment.&lt;/p>
&lt;p>On the other side, I have to accept that there could be questions that I can’t answer, and instead of stuttering and panicking I just say something along the lines of, ‘Great question. I’ve got no idea, but I’ll find out and let you know’. First, this is a great way to generate content for blog posts. Second – you’ll gain the respect from the audience that you didn’t just try and make something up on the spot.&lt;/p>
&lt;p>Hopefully as I speak more and gain confidence in my ability these feelings will subside. However, it’s also possible it’ll always be in the back of my mind. Perhaps it’s a good thing- if I didn’t experience this maybe I wouldn’t be as well prepared for presentations and I’d do the audience a disservice by being overconfident in my ability.&lt;/p></description></item><item><title>T-SQL Tuesday #121 - Gifts received for this year</title><link>https://jpomfret.github.io/t-sql-tuesday-#121-gifts-received-for-this-year/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#121-gifts-received-for-this-year/</guid><description>&lt;p>&lt;a class="link" href="https://curiousaboutdata.com/2019/12/03/tsql-tuesday-121-gifts-received-for-this-year/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>&lt;em>This is a time for material gift giving, for many of us. It might also be a time to consider the many gifts we have received through the year, and perhaps use this opportunity to appreciate people or situations that we were blessed with.&lt;/em>&lt;/p>
&lt;p>When I first saw Mala’s post last week for the last T-SQL Tuesday of the year I immediately started thinking of all the gifts I have received this year and how thankful I was with how 2019 has turned out. So first things first, thanks to Mala (&lt;a class="link" href="https://curiousaboutdata.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/sqlmal/" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this final prompt of 2019.&lt;/p>
&lt;h2 id="the-gift-of-sqlfamily">The gift of #SQLFamily&lt;/h2>
&lt;p>Around this time I found my local user group in Cleveland and started attending meetings. Attending meetings, talking to no one and going home, but still learning a lot from the great speakers and topics that were shared. This really started my journey as I started to interact with the community.&lt;/p>
&lt;p>I was also starting to follow members of the SQL Server community on Twitter. I occasionally used the &lt;a class="link" href="https://twitter.com/hashtag/sqlhelp?src=hashtag_click" target="_blank" rel="noopener"
>#SQLHelp&lt;/a> hashtag to get some guidance, but mostly played the role of lurker. I’d read articles that were shared, learning plenty, but not fully realizing the potential here.&lt;/p>
&lt;p>In 2015 I changed jobs, which meant in 2016 I had the chance to attend my first PASS Summit. What an experience. It was like our local user group on steroids, all day, for 3 whole days.  I was exhausted by the end of the week and that was just from the conference portion as I mostly kept to myself during the evenings.&lt;/p>
&lt;p>As time went on I still attended my local user group almost every month. I started talking to the people there more and more and began to understand this concept of #SQLFamily that I had heard mentioned so many times before.&lt;/p>
&lt;p>My friends at the user group continually provided support, guidance, and encouragement and eventually I started to believe I could give something back to the community. I started this blog in February 2018 and gave my first user group presentation in June 2018. This is when I really started to believe in the magic of the #SQLFamily.&lt;/p>
&lt;p>Since that slightly shaky presentation on Data Compression in June 2018, I’ve presented 3 different sessions a total of 14 times at various user groups, SQL Saturdays and conferences. Although I still feel like I have a long way to go, it was really great to present back to my local user group this last month to show them I’m getting better and their support is working!&lt;/p>
&lt;p>I will also give a special shout out to the Grillen guys. In June 2019 I was lucky enough to present my DSC &amp;amp; SQL Server presentation at DataGrillen. This was my first big conference- I had a few SQL Saturdays under my belt but this felt huge. William (&lt;a class="link" href="http://williamdurkin.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/sql_williamd" target="_blank" rel="noopener"
>t&lt;/a>) and Ben (&lt;a class="link" href="https://www.solisyon.de/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/bweissman" target="_blank" rel="noopener"
>t&lt;/a>) were the most amazing hosts, not only having created an amazing space to present and learn in, but also ensuring both myself and my wife had the best trip possible. I highly recommend checking this conference out if you can. The good news is, at this time, the c&lt;a class="link" href="https://datagrillen.com/" target="_blank" rel="noopener"
>all for speakers for 2020&lt;/a> is still open for 5 more days!&lt;/p>
&lt;p>We are so lucky to live and work in a community that has this concept of #SQLFamily. It took me a while to see it but now I can truly appreciate it and I can honestly say I wouldn’t be where I am today without every one of you!&lt;/p>
&lt;h2 id="the-gift-of-supportive-family--friends">The gift of supportive family &amp;amp; friends&lt;/h2>
&lt;p>The second gift I will talk about is I have the most supportive friends and family. Thanks to the #SQLFamily, I have met and made connections with folks from all over the world.  Thanks to Twitter, I randomly stumbled across this tweet one afternoon:&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/SQLDiplomat/status/1161342206675955713" target="_blank" rel="noopener"
>https://twitter.com/SQLDiplomat/status/1161342206675955713&lt;/a>&lt;/p>
&lt;p>I’m originally from Chippenham in the South West of England (about 1.5 hours from Southampton), and my wife, who’s American, wasn’t opposed to living in England at some point in our lives. Although it wasn’t in the immediate plan we decided to find out more and it turned out this opportunity was too good to miss. I’m really looking forward to 2020 and this new opportunity.&lt;/p>
&lt;p>I’m so excited to move back to England, to be closer to my family, and explore the UK and Europe with my wife. With the excitement comes absolute chaos as we prepare to move the 3,500 miles across the Atlantic. We’re currently working through selling our home, cars, things and packing up our lives in the US. We are very lucky to have the most supportive family on both sides of the pond as well as so many amazing friends that, although are sad to see us move, are being amazing at helping us realize (I guess it’s realise now!) this dream.&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/jesspomfret.com/wp-content/uploads/2019/12/IMG-20191207-WA0002.jpg?fit=650%2C867&amp;amp;ssl=1"
loading="lazy"
>&lt;/p>
&lt;p>Literally carrying me into 2020!&lt;/p>
&lt;p>The final shoutout is to my wife, who to be fair has the rougher deal here. I am moving towards my family, with a job lined up that I’m excited about. She is stepping into a world of unknown, leaving her family and her dream job in the US for an adventure in England. You wouldn’t know this though, she’s the one keeping us organized and on track while I write blog posts to avoid packing ?. Her support and sense of adventure is the gift I’m most thankful for, as I know I couldn’t have accomplished half the things I did this year without her in my corner.&lt;/p>
&lt;p>I’ve had a great 2019. It is ending in no way how I expected, and that’s largely due to the amazing gifts I’ve received from #SQLFamily and my own friends and family. So a big thank you to everyone involved!&lt;/p>
&lt;p>I hope everyone has a safe and enjoyable holiday season. Thanks again Mala for hosting!&lt;/p></description></item><item><title>Creating my first SQL Managed Instance</title><link>https://jpomfret.github.io/creating-my-first-sql-managed-instance/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/creating-my-first-sql-managed-instance/</guid><description>&lt;p>I’ve been thinking about the cloud a lot lately, and I feel it’s an area that I would benefit from learning more about. I’ve attended a couple of presentations on SQL Managed Instances and have read enough to be dangerous (or accidentally spend a lot of money, one of my biggest fears when working in the cloud). However, I always find I learn best and really get to understanding a topic by building something.&lt;/p>
&lt;p>This post will be the first in at least a two part series on SQL Managed Instances (MI). My goal in this post is just to deploy an MI and have it ready to use for my next post.&lt;/p>
&lt;p>I’ve chosen to start with MI as it feels like the easiest route to the cloud. Microsoft advertises an easy ‘Lift &amp;amp; Shift’ experience to move your on-premise applications to the cloud and offers “near 100% compatibility with on-premise”. Along with these you still get the existing benefits from using a SQL Database (PaaS) with reduced administration and management needs.&lt;/p>
&lt;p>You can read more about the specifics of “Azure SQL Database managed instance”, including pricing, in the &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/sql-database/sql-database-managed-instance" target="_blank" rel="noopener"
>Microsoft docs&lt;/a>.&lt;/p>
&lt;h2 id="creating-a-sql-managed-instance">&lt;strong>Creating a SQL managed instance&lt;/strong>&lt;/h2>
&lt;p>Step one is to head to the portal (&lt;a class="link" href="https://portal.azure.com/" target="_blank" rel="noopener"
>https://portal.azure.com&lt;/a>) and get logged in. From there you can search ‘SQL’ and then select ‘SQL managed instances’.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/SQLMI_Create.jpg"
loading="lazy"
>&lt;/p>
&lt;p>On the next pane you’ll see any existing managed instances you have setup. I have none at this point so I’ll select the ‘+Add’ button on the toolbar to start creating one (One thing to note in this blog post, Azure changes often and quickly so these screenshots and steps might not be exact by the time you get to reading this).&lt;/p>
&lt;h3 id="basics">Basics&lt;/h3>
&lt;p>The creation wizard takes you through several tabs. The first ‘Basics’ is where you’ll create a resource group (to group like resources and easily manage them), name your server, size your server, and setup the administrator account (cut off in the below screenshot).  Your server name can’t contain reserved words, and also can only contain lowercase letters, numbers and a hyphen.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/create1-1024x434.jpg"
loading="lazy"
alt="Basics for SQL MI setup"
>&lt;/p>
&lt;p>To size your managed instance you’ll need to choose between the two tier options. Business critical has faster disks (local SSD) for high I/O workloads and built in high availability with Always On. For today we’re going to stick with a general purpose instance.&lt;/p>
&lt;p>An interesting note here is that I chose the smallest managed instance I could, and if I selected the same in different regions the price estimation was slightly different.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/sizing.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="networking">Networking&lt;/h3>
&lt;p>On the networking tab you will create a virtual network for your managed instance to live in. You can also define the connection type (proxy or redirect) and enable a public endpoint. If you enable the public endpoint your managed instance will be available over the internet, which adds some security considerations that should be fully understood before simply enabling the option.&lt;/p>
&lt;h3 id="additional-settings">Additional Settings&lt;/h3>
&lt;p>The third and final settings tab allows you to configure your instance collation and time zone. The default time zone is UTC so you might want to change that to match your usual time zone for servers you build on premise.&lt;/p>
&lt;h3 id="review--create">Review &amp;amp; Create&lt;/h3>
&lt;p>The final tab reviews your settings to ensure they are all valid and allows you to create your managed instance. Depending on whether this also requires changes to the underlying cluster that your managed instance will be deployed on top of, this operation could take a while. Since this is my first managed instance I don’t have an existing cluster so Azure warns me this could take up to 6 hours to complete.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/hourstocomplete.jpg"
loading="lazy"
>&lt;/p>
&lt;h2 id="until-next-time">Until Next Time&lt;/h2>
&lt;p>It’s really quite simple to get a managed instance created. There are not too many decisions to make along the way. I imagine the most important part is to understand the networking aspect to ensure we’re going to be able to securely connect to the database from the application.&lt;/p>
&lt;p>Once my underlying cluster is created and my managed instance is ready to go the next step will be to migrate some databases from my on-premise lab up into the cloud.&lt;/p></description></item><item><title>National Coming Out Day</title><link>https://jpomfret.github.io/national-coming-out-day/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/national-coming-out-day/</guid><description>&lt;p>Today, October 11th is National Coming Out Day in the United States. This (2019) will be the 31st anniversary of its founding by Robert Eichberg and Jean O&amp;rsquo;Leary. The idea was to raise awareness and support for the LGBT community. &lt;a class="link" href="https://www.hrc.org/resources/national-coming-out-day" target="_blank" rel="noopener"
>If you know someone who identifies as LGBT you are more likely to support equal rights&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Most people think they don&amp;rsquo;t know anyone gay or lesbian, and in fact, everybody does. It is imperative that we come out and let people know who we are and disabuse them of their fears and stereotypes.&lt;/em>&lt;/p>
&lt;p>&lt;em>– Robert Eichberg, in 1993&lt;a class="link" href="https://www.nytimes.com/1995/08/15/obituaries/robert-eichberg-50-gay-rights-leader.html" target="_blank" rel="noopener"
>[3]&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>When I first heard about National Coming Out Day I was in college and I didn’t really get it. Why did we need a day to celebrate coming out? Can’t people just do it when they’re ready, or not? Shout out to the &lt;a class="link" href="http://workingwithdevs.com/tsql2sday-119-changing-your-mind/" target="_blank" rel="noopener"
>#tsql2sday&lt;/a> prompt from this month, which was about changing your opinion on something - I’ve changed my mind on this too.&lt;/p>
&lt;h3 id="why-is-national-coming-out-day-important">&lt;strong>Why is National Coming Out Day Important?&lt;/strong>&lt;/h3>
&lt;p>Firstly, for yourself – trying to hide a part of who you are is exhausting. Trying to constantly cover your tracks or not talking about the great weekend you had with your partner or LGBT friends makes you feel guarded and unwilling to fully integrate into conversations.&lt;/p>
&lt;p>A friend once told me about a diversity and inclusion exercise where they go around the group and have to talk about their weekend without revealing their partners gender, which is harder than you might think – and that was just one conversation. Now try doing it all day! &lt;/p>
&lt;p>Secondly, for everyone behind you. This is the magic if you ask me. The more people that are out the more it becomes normal. As we already mentioned, if you know someone who is LGBT, or really of any diverse group, you are much more likely to try to understand and to be cognizant of things that affect them.&lt;/p>
&lt;p>It also creates role models for the next generation. Usually when we mention role models we think of actors/actresses, sports stars and celebrities. When there are LGBT people in the limelight it’s exciting, it’s cool to know &amp;ldquo;hey that person is like me, maybe this will be alright&amp;rdquo;. I’d like to argue the more important role models are the regular people like you and me. Just going about our daily lives, trying to adult.&lt;/p>
&lt;p>This isn’t about my personal coming out story. I am not looking for praise on coming out or being out. This is just about my regular, everyday life. I don’t hide who I am, what you get all the time is me, a slightly socially awkward lesbian IT nerd with a great sense of humour (depending on who you ask, but I like to think so). &lt;/p>
&lt;p>I got married almost a year ago now. We both have jobs we enjoy, we bought a house, we have two cats. It’s all pretty ordinary. – my hope is that by making it normal it’ll be easier for the next group of people coming behind us. Anyone who feels like they don’t fit in for whatever reason, or has feelings they don’t understand. can look around and see people like them, making it.&lt;/p>
&lt;p>One of the posts that came out of #tsql2sday prompt mentioned this exact idea, “Representation matters”. First go queue this link up to read next, it’s excellent. “&lt;a class="link" href="https://www.sqlgene.com/2019/10/08/what-convinced-me-that-diversity-is-important/" target="_blank" rel="noopener"
>What Convinced me that diversity is important&lt;/a>”. In the post Eugene talks about his own type of diversity, and how his life was impacted by seeing successful people like him. This simple act of living your life openly really can make a difference for people.&lt;/p>
&lt;p>Now this is not an encouragement for everyone currently in the closet to just come out and join the party. It’s a big step and it’s not all unicorns and rainbows out here. It’s vital that you are safe and have the support you need. At the end of the day, you do you, because every situation is different.  If you need someone in your corner feel free to reach out to me. I’m definitely no expert but I’ll do my best.&lt;/p>
&lt;p>Finally, if you feel so inclined head over to &lt;a class="link" href="http://amtwo.lgbt" target="_blank" rel="noopener"
>http://amtwo.lgbt&lt;/a> and take a look at my good friend Andy’s post. He’s currently raising money for &lt;a class="link" href="https://www.thetrevorproject.org" target="_blank" rel="noopener"
>The Trevor Project&lt;/a>, an organization whose mission is to end suicide among gay, lesbian, bisexual, transgender, queer &amp;amp; questioning young people. Also, he has the “Unofficial Queer Guide to PASS Summit”, if you’re heading there next month.&lt;/p>
&lt;p>Thanks for reading – and happy National Coming Out Day!&lt;/p></description></item><item><title>T-SQL Tuesday #119 - Changing your mind</title><link>https://jpomfret.github.io/t-sql-tuesday-#119-changing-your-mind/</link><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#119-changing-your-mind/</guid><description>&lt;p>&lt;a class="link" href="http://workingwithdevs.com/tsql2sday-119-changing-your-mind/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It’s time for our monthly T-SQL Tuesday blog party again and we have another interesting prompt to write about. Thanks goes to Alex Yates (&lt;a class="link" href="http://workingwithdevs.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/_AlexYates_" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month.&lt;/p>
&lt;p>I’ve thought about this a lot over the last few days and really wanted to find a technical topic to write about, but over and over I came back to the same thoughts. This last week I attended two conferences and gave three presentations (one topic twice), and I still can’t believe I’ve given a single technical presentation.&lt;/p>
&lt;p>Allen White (&lt;a class="link" href="http://dataperfpro.com" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SQLRunr" target="_blank" rel="noopener"
>t&lt;/a>) has a famous spiel that he gives before every one of his presentations, encouraging his audience that everyone has something they can teach others. I created my PASS profile in 2011, and that was about the time I first heard Allen present. I remember laughing to myself that I would never be able to do that.  Giving presentations during my college years was something I dreaded, why on earth would I volunteer now to do that.&lt;/p>
&lt;p>Well, fast forward to June 2018, I gave &lt;a class="link" href="https://jesspomfret.com/first-user-group-presentation-i-survived/" target="_blank" rel="noopener"
>my first user group presentation&lt;/a>. I stepped way out of my comfort zone for that, but since then I have presented 11 additional times.  I’ve presented at a mixture of SQL Saturdays, online virtual groups and conferences, and every time I’ve got extremely nervous beforehand, and experienced an unbelievable high after.&lt;/p>
&lt;p>This past week I gave my DSC &amp;amp; SQL Server presentation on Friday in Columbus at &lt;a class="link" href="https://dogfoodcon.com/" target="_blank" rel="noopener"
>DogFoodCon&lt;/a> (a great conference with a very different crowd than my usual data platform events) and then I gave two sessions at &lt;a class="link" href="https://sqlsaturday.com/907/eventhome.aspx" target="_blank" rel="noopener"
>SQL Saturday Pittsburgh&lt;/a>.&lt;/p>
&lt;p>At this point I still feel like I’m not really a speaker, every time I walk into the speaker room at an event I expect someone to ask what I’m doing in there. However, I’ve received a lot of great feedback, and more importantly had a lot of great conversations with people about things they learnt in my session. I’m getting there.&lt;/p>
&lt;p>The other amazing part of this adventure is I’ve had so much fun, I’ve met and become friends with some great people and I’ve traveled as far as Germany to explore the world and deliver my sessions.&lt;/p>
&lt;p>I can 100% say I’ve changed my opinion on whether I can become a speaker in this community, and I’ve bought into Allen’s spiel that everyone has something to give back. &lt;/p>
&lt;p>If you are reading this and laughing to yourself that it can’t mean you, it does! Maybe you’re not ready to speak, but start a blog and share some things you’ve done – other people are one month behind you, trying to work out how to do what you just did. If writing isn’t your style, go contribute some code to open source software- &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools" target="_blank" rel="noopener"
>dbatools&lt;/a> and &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a> are on GitHub and always looking for new contributors. Go squash some bugs or add new functionality.&lt;/p>
&lt;p>Thanks again to Alex for the great prompt.&lt;/p></description></item><item><title>T-SQL Tuesday #118 – Your fantasy SQL feature</title><link>https://jpomfret.github.io/t-sql-tuesday-#118-your-fantasy-sql-feature/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#118-your-fantasy-sql-feature/</guid><description>&lt;p>&lt;a class="link" href="https://www.kevinrchant.com/2019/09/03/t-sql-tuesday-118-your-fantasy-sql-feature/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>This month our T-SQL Tuesday host, Kevin Chant (&lt;a class="link" href="https://www.kevinrchant.com" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/kevchant" target="_blank" rel="noopener"
>t&lt;/a>), has challenged us to propose a new fantasy feature for SQL Server. Firstly, thanks to Kevin for hosting this month’s blog party.&lt;/p>
&lt;p>When I first read the invitation I didn’t have any ideas jump straight into my mind. Since then, I’ve come up with two. I’m going to share both since they are for different areas… cheating I know ?.&lt;/p>
&lt;h3 id="likein-operator">LIKEIN Operator&lt;/h3>
&lt;p>First up, I am proposing the introduction of the &lt;code>LIKEIN&lt;/code> operator for T-SQL queries. It would be useful to be able to combine the functionality of the &lt;code>LIKE&lt;/code> and &lt;code>IN&lt;/code> operators. To be able to select multiple strings that include wildcards, without having to use multiple &lt;code>OR&lt;/code> clauses would be a great addition to our T-SQL tool belts.&lt;/p>
&lt;p>My example comes from the AdventureWorks2017 database where we need to select the addresses we have listed within two regions. In the United Kingdom postal codes are split into two parts. For example, the postcode for Wembley Stadium is HA9 0WS (Yes, I’m currently watching the England Vs Bulgaria game). The first half, HA9, is known as the outward code and can be 3 or 4 digits long. This specifies the postcode area and district. The second half of the postcode helps to identify a specific address. &lt;/p>
&lt;p>Using my new &lt;code>LIKEIN&lt;/code> operator this can easily be accomplished:&lt;/p>
&lt;p>select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode
from Person.Address
where PostalCode likein (&amp;lsquo;RG41%&amp;rsquo;, &amp;lsquo;V9B%&amp;rsquo;)&lt;/p>
&lt;p>Since we don’t currently have this functionality I’ll suggest a few other options.&lt;/p>
&lt;p>First, as mentioned above we could use multiple &lt;code>OR&lt;/code> clauses.  The main downside to this option is as soon as we add &lt;code>OR&lt;/code> into our &lt;code>WHERE&lt;/code> clause we lose the ability to do index seeks and instead are forced to scan the entire index.  Hopefully with the implementation of &lt;code>LIKEIN&lt;/code> we would still be able to use index seeks, as long as the string doesn’t start with a wildcard.&lt;/p>
&lt;p>select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode
from Person.Address
where PostalCode like &amp;lsquo;RG41%&amp;rsquo;
or PostalCode like &amp;lsquo;V9B%&amp;rsquo;&lt;/p>
&lt;p>We could also use the &lt;code>substring&lt;/code> and &lt;code>charindex&lt;/code> functions to match just the first part of &lt;code>PostalCode&lt;/code>, before the space with our two desired areas. This also removes our ability to use index seeks. Anytime you add a function onto the right side of the &lt;code>WHERE&lt;/code> clause you are forced to scan the whole index or table.&lt;/p>
&lt;p>select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode
from Person.Address
where substring(PostalCode,0,charindex(&amp;rsquo; &amp;lsquo;,PostalCode,0)) in(&amp;lsquo;RG41&amp;rsquo;,&amp;lsquo;V9B&amp;rsquo;)&lt;/p>
&lt;p>Finally, here’s an option that allows us to effectively use our index. We could use two separate queries and combine them with &lt;code>UNION ALL&lt;/code>. This results in two seeks and then a concatenation operator in the execution plan to combine the results.  Also, since these result sets are not overlapping, &lt;code>UNION ALL&lt;/code> can be used instead of &lt;code>UNION&lt;/code> since we won’t have duplicate results to remove.&lt;/p>
&lt;p>select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode
from Person.Address
where PostalCode like &amp;lsquo;RG41%&amp;rsquo;
UNION ALL
select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode
from Person.Address
where PostalCode like &amp;lsquo;V9B%&amp;rsquo;&lt;/p>
&lt;h3 id="agent-alerts--raise-error-when-message-doesnt-contain-text">Agent Alerts – Raise error when message doesn’t contain text&lt;/h3>
&lt;p>My second new feature is a lot simpler. When configuring SQL Server alerts you have the option to only raise the alert if the message contains certain text. My proposed feature is the opposite- don’t raise the alert if it contains certain text.&lt;/p>
&lt;p>A severity 20 alert, such as the one below contains the client IP address. It would be really nice to exclude any alerts that contained a certain misbehaving client IP, to help keep the noise down.&lt;/p>
&lt;p>Error: 17836, Severity: 20, State: 17.
Length specified in network packet payload did not match number of bytes read;
the connection has been closed. Please contact the vendor of the client library.
[CLIENT: xx.xx.xx.xx]&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/tsql117.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Thanks for reading my fantasy features. I’m looking forward to reading all the other new ideas out there.&lt;/p></description></item><item><title>Desired State Configuration: A few warnings when using PSDscRunAsCredentials</title><link>https://jpomfret.github.io/desired-state-configuration-a-few-warnings-when-using-psdscrunascredentials/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-a-few-warnings-when-using-psdscrunascredentials/</guid><description>&lt;p>When you enact a configuration against a target node by default the process runs as the local system account.  For most of your DSC resources this is fine. However, if you need to access something like a file share, active directory or user registry settings, you might start to run into permission issues.  &lt;/p>
&lt;p>DSC Resources have a built in property, &lt;code>PSDscRunAsCredential&lt;/code>, that when configured changes the account that the resource will be executed under.&lt;/p>
&lt;p>To use this you will use a &lt;code>PSCredential&lt;/code> object in your resources as shown below. Note that this example doesn’t require the use of other credentials and would have had the required permissions as the local system account.&lt;/p>
&lt;p>$Cred = Get-Credential
Configuration DtcServiceRunning {&lt;/p>
&lt;pre>&lt;code>Import-DscResource -ModuleName PSDesiredStateConfiguration
Node 'dscsvr2' {
Service MsDtcRunning {
Name = 'MSDTC'
Ensure = 'Present'
State = 'Running'
PsDscRunAsCredential = ($creds)
}
}
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;h1 id="generates-mof-file">Generates MOF file&lt;/h1>
&lt;p>DtcServiceRunning -Output .\Output\&lt;/p>
&lt;p>When you use credentials in your configurations there are a couple of gotchas- we’ll talk through these next.&lt;/p>
&lt;h3 id="warning-1-plain-text-passwords">Warning 1: Plain Text Passwords&lt;/h3>
&lt;p>Although it feels like you are handling the credential securely, when you run the code above you will get an error explaining that storing passwords in plain text is a bad idea.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/01_PlainTextPasswords.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/01_PlainTextPasswords.jpg"
loading="lazy"
alt="Converting and storing encrypted passwords as plain text is not recommended."
>&lt;/a>&lt;/p>
&lt;p>When you run your configuration, &lt;code>DtcServiceRunning&lt;/code> in my example, a MOF file is generated and with our current setup the passwords will be stored in plain text (you can read more about generating &lt;a class="link" href="https://jesspomfret.com/dsc-mof-files/" target="_blank" rel="noopener"
>MOF files here&lt;/a>). &lt;/p>
&lt;p>The correct way to handle this issue is to generate a certificate and use that to encrypt the MOF file, a topic I will blog about one day soon. To skirt around this issue in test we can add a configuration property that basically forces us to accept we know this is a bad idea, but we’re going for it anyway.&lt;/p>
&lt;p>$configData = @{
AllNodes = @(
@{
NodeName = &amp;ldquo;dscsvr2&amp;rdquo;
PsDscAllowPlainTextPassword = $true
PsDscAllowDomainUser = $true
}
)
}
DtcServiceRunning -Output .\Output\ -ConfigurationData $configData&lt;/p>
&lt;p>Once we run this we’ll see our MOF file has been successfully generated.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/02_moffile.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="warning-2-changing-the-password-used-in-a-mof-file">Warning 2: Changing the password used in a MOF file&lt;/h3>
&lt;p>Once you push your MOF file out to the target node and it is enacted, the MOF remains on the target node in a designated folder and is named &lt;code>current.mof&lt;/code>. The credentials, including the password (hopefully encrypted by a certificate!), are within the file.&lt;/p>
&lt;p>You can run a couple of commands to check your current configuration that compare the desired state, defined in the &lt;code>current.mof&lt;/code> file, to the nodes current state.&lt;/p>
&lt;p>If the password you used for the &lt;code>PsDscRunAsCredentials&lt;/code> property has changed since the MOF file was enacted you’ll get the following errors when you try and run &lt;code>Get-DscConfiguration&lt;/code> or &lt;code>Test-DscConfiguration&lt;/code>.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/04_testFailed.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/04_testFailed.jpg"
loading="lazy"
alt="The user name or password is incorrect."
>&lt;/a>&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/09/05_getfailed.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/05_getfailed.jpg"
loading="lazy"
alt="The user name or password is incorrect."
>&lt;/a>&lt;/p>
&lt;p>The user name or password is incorrect.&lt;/p>
&lt;p>To fix this you must rerun your configuration, which will in turn generate a new MOF file that contains the new password. Once this MOF file has been generated you’ll push it out to your target node.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/05_startDscConfiguration.jpg"
loading="lazy"
alt="Start-DscConfiguration"
>&lt;/p>
&lt;p>Now that the MOF file contains the new password you are able to check in on your current configuration again.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/06_test.jpg"
loading="lazy"
alt="Test-DscConfiguration"
>&lt;/p>
&lt;p>This wasn’t something I thought about before I used &lt;code>PsDscRunAsCredential&lt;/code> on all the resources in my configuration.  A while later I went to check on my target nodes and realized I couldn’t without pushing out a new MOF file. After some discussion in the DSC Slack channel around this, the recommended approach is to only use &lt;code>PsDscRunAsCredential&lt;/code> if necessary, and to be aware of the requirement to generate a new MOF file once you change the password.&lt;/p></description></item><item><title>Disable all Triggers on a Database</title><link>https://jpomfret.github.io/disable-all-triggers-on-a-database/</link><pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/disable-all-triggers-on-a-database/</guid><description>&lt;p>Sometimes it’s best not to ask why. However, if for some reason you have a number of triggers on tables within a database that you would like to temporarily disable, read on.&lt;/p>
&lt;p>I came across a situation recently while automating a process to refresh test environments where this exact scenario came up.  As part of the process several scripts were run to obfuscate production data. While these ran all the UPDATE triggers were firing. Not only were the triggers adding a significant amount of time to the process, they were also updating dates and other values that we’d prefer kept their original values.&lt;/p>
&lt;p>Now, as I mentioned this is not a discussion on whether this is a good database design or not, this is just how to solve this issue.&lt;/p>
&lt;p>In the snippet below I use &lt;code>Connect-DbaInstance&lt;/code> from &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> to create a &lt;code>$svr&lt;/code> object. If you don’t have dbatools installed you could either &lt;a class="link" href="http://dbatools.io/install" target="_blank" rel="noopener"
>install dbatools&lt;/a>, or use &lt;code>New-Object Microsoft.SqlServer.Management.Smo.Server&lt;/code>. The dbatools function is essentially a wrapper around this command that adds a lot of additional checks and options.&lt;/p>
&lt;p>I have also defined an array &lt;code>$triggers&lt;/code> to keep track of the triggers I disable. It’s likely that you’ll want to put the environment back to how it started, so this will make sure you don’t enable any triggers that started off disabled.&lt;/p>
&lt;p>Then we get to the actual work. Using the &lt;code>$svr&lt;/code> object we can loop through all the tables, and then all the triggers on those tables. If a certain trigger is enabled, it is added to the &lt;code>$triggers&lt;/code> array and then disabled using &lt;code>$tr.isenabled&lt;/code>.  As with most (all?) changes made through SMO you then need to call the alter method ,&lt;code>$tr.alter()&lt;/code>, to actually make the change on the server.&lt;/p>
&lt;p>$database = ‘AdventureWorks2017’
$svr = Connect-DbaInstance -SqlInstance server1
$foreach ($tbl in $svr.databases[$database].Tables)
{
foreach ($tr in $($tbl.Triggers | Where-Object Isenabled)) {
$triggers += $tr | Select-Object @{l=&amp;lsquo;SchemaName&amp;rsquo;;e={$tbl.Schema}}, @{l=&amp;lsquo;TableName&amp;rsquo;;e={$tbl.name}}, @{l=&amp;lsquo;TriggerName&amp;rsquo;;e={$_.name}}
$tr.isenabled = $FALSE
$tr.alter()
}
}&lt;/p>
&lt;p>When you are ready to enable the triggers again you can use the following code. This loops through the triggers that we had previously disabled and added to our array and enables them.&lt;/p>
&lt;p>foreach($tr in $triggers) {
$trigger = $svr.Databases[$database].Tables[$tr.TableName,$tr.SchemaName].Triggers[$tr.TriggerName]
$trigger.IsEnabled = $true
$trigger.alter()
}&lt;/p></description></item><item><title>T-SQL Tuesday #117 – When Have You Used MOT Tables?</title><link>https://jpomfret.github.io/t-sql-tuesday-#117-when-have-you-used-mot-tables/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#117-when-have-you-used-mot-tables/</guid><description>&lt;p>This month’s T-SQL Tuesday is all about &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/overview-and-usage-scenarios?view=sql-server-2017" target="_blank" rel="noopener"
>Memory Optimized Tables&lt;/a> (MOT), a topic I will admit I know almost nothing about. When they first came out in 2014 I was excited about the new technology and the options that would bring, however I never found a good reason to implement them.&lt;/p>
&lt;p>&lt;a class="link" href="https://docs.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/overview-and-usage-scenarios?view=sql-server-2017" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Thanks to Steve Jones (&lt;a class="link" href="https://voiceofthedba.com" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/way0utwest" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this month’s edition and after reading about MOT this week I’m looking forward to learning how people are using them.&lt;/p>
&lt;p>I’ve heard many times in the technology world that the third release of a software or new feature is the best time to adopt it.  Reading about MOT, it seems like this theory fits here also. &lt;/p>
&lt;p>When Hekaton, as it was known then, was released with SQL Server 2014 there were a lot of restrictions.  With the subsequent releases, presumably guided by feedback, some of these restrictions have been lifted.  I’ve picked a couple to discuss here that I think could have a big impact on adoption rates.&lt;/p>
&lt;h3 id="schema-changes">Schema Changes&lt;/h3>
&lt;p>In 2014 schema changes were not supported. This meant you couldn’t ALTER a Memory Optimized Table or a natively compiled stored procedure, instead you had to DROP and recreate the object.  This isn’t a huge concern for stored procedures. The main issue here would be losing any object level permissions that had been set.&lt;/p>
&lt;p>With Memory Optimized Tables however, this means you need to make sure you handle the data within that table. There are ways around this obviously. You could copy it off into a staging table, drop and recreate your table and then re-insert the data.  This does however add some complexity to managing your environment, and I could see this putting some people off.&lt;/p>
&lt;p>With the 2016 release ALTER TABLE and ALTER PROCEDURE support was introduced, effectively removing this barrier to use.  A further enhancement, the ability to use sp_rename, was then introduced with the 2017 release.&lt;/p>
&lt;p>Although there are still some restrictions on schema changes, including no extended properties, they are minimal compared to what was available in the 2014 release.&lt;/p>
&lt;h3 id="foreign-keys">Foreign Keys&lt;/h3>
&lt;p>Another huge missing piece when this technology was introduced in 2014 was foreign key support.  My understanding was that this technology could be (reasonably) easily swapped in to help with I/O and contention bottlenecks in your current SQL Server environment. This idea becomes less of a possibility when you realize this lack of support. Most well designed databases use foreign keys to enforce referential integrity, so having to remove those in order to use In-Memory technology seems like an unlikely ask.&lt;/p>
&lt;p>Again, with 2016 we saw this barrier removed. Not only was foreign key support introduced, so too was the ability to use UNIQUE and CHECK constraints on Memory Optimized Tables.  This will make it much easier to take existing SQL Server workloads and move some key tables into memory.&lt;/p>
&lt;p>Although not directly related to foreign keys, 2017 saw the previous restriction of having a maximum of 8 indexes per Memory Optimized Table lifted. This gives you more freedom to add unique indexes and to index foreign key columns.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>To wrap this up, although I haven’t myself used Memory Optimized Tables, it does seem like the barriers to use have been significantly reduced in the last two releases of SQL Server (2016, 2017).  This technology has now had time to mature and grow in features, hopefully now it’ll be easier to find a good use case for implementation.&lt;/p></description></item><item><title>Aggregating Data with PowerShell</title><link>https://jpomfret.github.io/aggregating-data-with-powershell/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/aggregating-data-with-powershell/</guid><description>&lt;p>As a SQL Server DBA, aggregating data is first nature.  I can easily throw together some T-SQL if I need to get the average sales per product group, or perhaps the number of employees hired per month. Yesterday I was writing a little PowerShell when I came across a problem. I needed to get the size of my database data and log files for several databases on a server, when I realized I didn’t know how to group and sum this data in PowerShell.&lt;/p>
&lt;p>&lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> has the &lt;code>Get-DbaDbFile&lt;/code> command, which easily allowed me to collect information about the databases I needed.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Once I had this data, I wanted to group by the type of file and get the total size.  To start I piped the output to &lt;code>Select-Object&lt;/code> to trim down the fields in the result set, then piped this output to the &lt;code>Group-Object&lt;/code>, specifying the field I wanted to group by.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Database, TypeDescription, Size |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Group-Object TypeDescription
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Group.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now that the data is grouped you can aggregate the size property for the files, which is within the &lt;code>Group&lt;/code>.  We’ll add an expression to the final &lt;code>Select-Object&lt;/code> which uses the &lt;code>Measure-Object&lt;/code> to sum the sizes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Database, TypeDescription, Size |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Group-Object TypeDescription |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object Name, @{l=&amp;#39;Size&amp;#39;;e={($_.Group.Size.MegaByte | Measure-Object -Sum).Sum}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Final.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Another interesting note here is that since dbatools returns the Size as a special type, &lt;code>Sqlcollaborative.Dbatools.Utility.Size&lt;/code>, you can specify that you want it to be returned in MBs.  If you have very small databases you might want to change that so you don’t lose accuracy with any rounding.&lt;/p>
&lt;p>The code for the examples in this post was run against a Docker Container I use for demos. You can read more information on getting that set up in my post &lt;a class="link" href="https://jesspomfret.com/data-compression-containers/" target="_blank" rel="noopener"
>Data Compression Demos in Containers&lt;/a>.&lt;/p></description></item><item><title>Desired State Configuration: Troubleshooting in Push Refresh Mode</title><link>https://jpomfret.github.io/desired-state-configuration-troubleshooting-in-push-refresh-mode/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-troubleshooting-in-push-refresh-mode/</guid><description>&lt;p>One of the biggest obstacles people face when using DSC is the troubleshooting and reporting pieces. There are options here to integrate with third party tools to create a more polished enterprise solution, but if you’re going with just straight DSC you might feel it is lacking some in this area.&lt;/p>
&lt;p>We do however have several tools available to troubleshoot issues with configurations or to monitor our nodes to determine whether they are still in the desired state. I’m specifically going to look at the options available if you’re using DSC in the Push refresh mode. If you are using DSC in pull mode with a web server or if you’re using Azure Automation you have some other options available. You can configure the Local Configuration Manager (LCM) to send reports to the pull server. These reports are stored in a database on the server and can be accessed by calling the web service. Perhaps the topic of another blog post.&lt;/p>
&lt;p>The options we’ll look at today are the functions available within the &lt;code>PSDesiredStateConfiguration&lt;/code> module, and the DSC Windows event logs.&lt;/p>
&lt;h3 id="psdesiredstateconfiguration-functions">&lt;strong>PSDesiredStateConfiguration Functions&lt;/strong>&lt;/h3>
&lt;p>We should be fairly familiar with this module as it comes built in with WMF 4.0+ and contains several base resources as well as functions to manage and use DSC.&lt;/p>
&lt;p>In order for us to explore these commands we need to have a target node with an active configuration. I pushed out a simple configuration that will ensure the directory &lt;code>c:\temp&lt;/code> exists on the target node. The configuration was successful and the folder was created.&lt;/p>
&lt;p>First up we have &lt;code>Get-DscConfiguration&lt;/code>. The command based help for this function says it will get “the&lt;br>
current configuration of the nodes.” When I run this against my target node it returns details about the file resource I used to create the &lt;code>C:\temp&lt;/code> directory and notes that &lt;code>Ensure&lt;/code> is &lt;code>Present&lt;/code>. This is as I would expect.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/get_present.jpg"
loading="lazy"
>&lt;/p>
&lt;p>If I manually delete the &lt;code>C:\Temp&lt;/code> folder, the node is no longer in the desired state. When I rerun &lt;code>Get-DscConfiguration&lt;/code> it shows the folder is absent, which is not the behavior I expected from the help. I was expecting to get the current configuration that had been pushed out to the node. It seems that this function returns the resources included in the configuration and their current state. It does not however indicate if this is the desired state.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Get_Absent.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The next function available is &lt;code>Get-DscConfigurationStatus&lt;/code> The description of this function states it will retrieve “detailed information about completed configuration runs on the system.”   If we run that with just the &lt;code>-CimSession&lt;/code> parameter to connect to our node we get some useful information about the last run. &lt;/p>
&lt;p>However, there is a lot of information available from this function that is not within the default columns returned. If we look into the output further we can in fact see whether resources are in the desired state or not.  &lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DscConfigurationStatus_additionalCOls.jpg"
loading="lazy"
>&lt;/p>
&lt;p>During my testing using &lt;code>Get-DscConfigurationStatus&lt;/code> it did not always accurately report when there were resources not in the desired state. Therefore I wouldn’t rely on it for reporting, I would instead look at our third option below.&lt;/p>
&lt;p>The third and final function I’ll highlight from this module is &lt;code>Test-DscConfiguration&lt;/code>. The comment based help for this one states it “tests whether the actual configuration on the nodes matches the desired configuration.”&lt;/p>
&lt;p>Just running this with the target node as the &lt;code>-ComputerName&lt;/code> parameter does not provide much information. It returns `False` telling us it is not in the desired state, but doesn’t explain why.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_basic.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Running with the &lt;code>-Verbose&lt;/code> switch goes into details. It returns the same verbose output you would get from running the configuration in the first place. However, reading through the output of a large configuration file with many resources could get time consuming and it would be easy to miss resources not in their desired state. &lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_verbose.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Finally, we can use the &lt;code>-Detailed&lt;/code> parameter which will return a PowerShell object with exactly the information we’re looking for.  This object gives us more options on how to use this information. For example, we can return whether it is in the desired state and a list of resources not in the desired state is available.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Test_detailed.jpg"
loading="lazy"
>&lt;/p>
&lt;p>This gives us some good information if our configuration was successful, but what happens if we’re troubleshooting a failed configuration? Running both &lt;code>Get-DscConfiguration&lt;/code> and &lt;code>Test-DscConfiguration&lt;/code> state they will run against a pending configuration and both return the error that the network path wasn’t found.  No information is returned on which resource threw this error. If you have a large, complicated configuration it would be nice to receive a little more guidance on where to look next. Hint, it’s the event logs.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/get_test_failed.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="windows-event-logs">Windows Event Logs&lt;/h3>
&lt;p>The next step in the troubleshooting handbook is to head to the windows event logs.  DSC has four event logs, but only two are enabled by default, and it doesn’t seem like much gets written to the Admin log.&lt;/p>
&lt;p>Running the following will show you the enabled logs and the number of records:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WinEvent -ComputerName dscsvr2 -ListLog *dsc*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/wineventlogs.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Looking in the Operational log you can find the error and the related resource:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WinWvent -ComputerName dscsvr2 `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -LogName Microsoft-Windows-DSC/Operational -MaxEvents 10 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object TimeCreated, Message |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Format-List
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/07/eventlogerror.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/eventlogerror.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>The event log also points to a json file that contains more detailed logging, this matches what would be returned if you ran the &lt;code>Start-DscConfiguration&lt;/code> with the &lt;code>-Verbose&lt;/code> and &lt;code>-Wait&lt;/code> switches.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Message : Job {93E0C95E-8F21-11E9-85A2-00155D016620} : Details logging completed for C:\Windows\system32\configuration\ConfigurationStatus\{93E0C95E-8F21-11E9-85A2-00155D016620}-0.details.json.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>If more detail is needed you can enable the debug and analytic logs for DSC and rerun the configuration. The analytic logs will contain a lot more messages that will help you get to the bottom of why your configuration failed.&lt;/p></description></item><item><title>Getting OS and SQL Version information with dbatools</title><link>https://jpomfret.github.io/getting-os-and-sql-version-information-with-dbatools/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/getting-os-and-sql-version-information-with-dbatools/</guid><description>&lt;p>There have been a lot of blog posts and talk around upgrading your servers in the past. However, the chatter always seems to intensify when we start getting close to that dreaded ‘end of support’ date for your older Windows and SQL Server versions.  I hope this isn’t the first place you are discovering this, but July 9th 2019 marks the end of support for both SQL Server 2008 and 2008R2, closely followed on January 14th 2020 with the end of support for Windows Server 2008 and 2008R2.&lt;/p>
&lt;p>With these dates on the horizon it’s a good time to look at our estate and make sure we have a good understanding of the versions we currently support. I’m going to show you how to do that easily with a couple of &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> functions. Then, bonus content, I’ll show you how to present it for your managers with one of my other favourite PowerShell modules &lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a>.&lt;/p>
&lt;p>First things first, I need an object that contains our servers.  At my work we use a central management server to keep track of servers, but you could just as easily pull server names in from a text file, or a database.&lt;/p>
&lt;p>$servers = Get-DbaCmsRegServer -SqlInstance CmsServerName&lt;/p>
&lt;p>Let’s first look at what operating systems we are running. The dbatools function we need for this is &lt;code>Get-DbaOperatingSystem&lt;/code>. I’ll use the name property of my &lt;code>$servers&lt;/code> object to get the OS information for all my servers and save it to a variable.&lt;/p>
&lt;p>$os = Get-DbaOperatingSystem -ComputerName $servers.name&lt;/p>
&lt;p>I chose to save the results to a variable for this since I’m going to examine the results using PowerShell and then also output them to Excel, saving me from having to gather the information from each server multiple times. If I only planned on looking at the results on screen I could instead have just piped the &lt;code>Get-DbaOperationSystem&lt;/code> results straight into &lt;code>Group-Object&lt;/code>.&lt;/p>
&lt;p>Using &lt;code>Group-Object&lt;/code> I can quickly see how many servers I have for each versions of windows, and how many I have going out of support in the near future.&lt;/p>
&lt;p>$os | Group-Object OSVersion |
Sort-Object Name |
Select-Object Name, Count, @{l=&amp;lsquo;Servers&amp;rsquo;;e={$_.Group.ComputerName -Replace &amp;lsquo;.domain.name,&amp;rsquo;&amp;rsquo; -Join &amp;lsquo;,&amp;rsquo;}}&lt;/p>
&lt;p>I have used the -Replace option in my &lt;code>Select-Object&lt;/code> to remove the domain name from the output and instead only return the server name.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/05/os-2.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/os-2.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>We can do the same with &lt;code>Get-DbaProductKey&lt;/code> to get the SQL version information.&lt;/p>
&lt;p>$sql = Get-DbaProductKey -ComputerName $servers.name
$sql | Group-Object Version |
Sort-Object Name |
Select Name, Count, @{l=&amp;lsquo;Servers&amp;rsquo;;e={$_.Group.SqlInstance -join &amp;lsquo;,&amp;rsquo;}}&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/05/sql.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/sql.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>With just 5 lines of code we can review our entire estate and make sure we know what we have nearing the end of support. This is pretty useful information, and also a good thing to export into a pretty spreadsheet and share with your team or management. Enter &lt;a class="link" href="https://github.com/dfinke/ImportExcel" target="_blank" rel="noopener"
>ImportExcel&lt;/a>.&lt;/p>
&lt;p>If you haven’t used this module before, prepare to have your mind blown. Doug Finke has crafted some PowerShell magic to enable you to both import from and export to Excel, using PowerShell, without needing Excel installed even.&lt;/p>
&lt;p>The full code is below. We’ve already done the work of gathering our data so if you are following along skip the first 3 lines below.&lt;/p>
&lt;p>I’ve separated out the properties I want to select and will therefore end up in my spreadsheet. I’ve also used &lt;a class="link" href="https://dbatools.io/splat/" target="_blank" rel="noopener"
>splatting&lt;/a> to make the call to &lt;code>Export-Excel&lt;/code> easier to read.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The important part of this script is the parameters used for the &lt;code>Export-Excel&lt;/code> call so I’ll go through them here:&lt;/p>
&lt;p>[table id=7 /]&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ExcelOutput-1024x669.jpg"
loading="lazy"
>&lt;/p>
&lt;p>There you have it, a simple script to get the current OS and SQL versions you are running with a good looking Excel sheet as the output. Hope you don’t find too many instances out there nearing the end of support.&lt;/p></description></item><item><title>PowerShell Comment Based Help: Examples with Multiple Lines of Code</title><link>https://jpomfret.github.io/powershell-comment-based-help-examples-with-multiple-lines-of-code/</link><pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/powershell-comment-based-help-examples-with-multiple-lines-of-code/</guid><description>&lt;p>One of the reasons I love PowerShell is the comment based help. This allows you to easily get documentation for functions directly within your PowerShell session. By using &lt;code>Get-Help&lt;/code> for a function you can retrieve a description, information on the parameters, and examples of how to use the function.&lt;/p>
&lt;p>Recently I was adding help to a function  and wanted to add two lines of code to my example. Usually the syntax for an example looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Get-Cat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">This will get me a default of one cat.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The first line under &lt;code>.EXAMPLE&lt;/code> will be formatted with a PowerShell prompt in front to show it is code. The second line is a description of the example.&lt;/p>
&lt;p>If I want to add two lines of code, and I used the following, it would only display the first line with a prompt as shown in the screenshot below.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$cats = 4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Get-Cat -NumberOfCats $cats
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Store the number of cats in a variable and then get that number of cats.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/example_first.jpg"
loading="lazy"
>&lt;/p>
&lt;p>You can see above the first example looks good, however in the second example the first two lines should both have a prompt to show they are code. I spent a little while Googling this without much avail. I then figured, somewhere within &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a> there must be an example with two lines of code. Sure enough I found my answer, and it’s pretty straightforward. You just add the prompt to the code yourself and then when the example is displayed it is formatted properly.&lt;/p>
&lt;p>I changed my examples to the following and you can see now they display as expected.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;Get-Cat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">This will get me a default of one cat.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">.EXAMPLE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;$cats = 4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">PS C:\&amp;gt;Get-Cat -NumberOfCats $cats
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Store the number of cats in a variable and then get that number of cats.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/example_second.jpg"
loading="lazy"
>&lt;/p>
&lt;p>I know you’re all dying to see the result of my &lt;code>Get-Cat&lt;/code> function so here you go. If you need to add this to your PowerShell profile etc. so you can quickly brighten any day, the code is on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/02_CommentBasedHelp_Examples.ps1" target="_blank" rel="noopener"
>github&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/cats.jpg"
loading="lazy"
>&lt;/p></description></item><item><title>Execution of Multiple Triggers on one Table</title><link>https://jpomfret.github.io/execution-of-multiple-triggers-on-one-table/</link><pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/execution-of-multiple-triggers-on-one-table/</guid><description>&lt;p>Well it has been a little quiet here recently. I just (or it’s been two weeks now) got back from a 2 week trip to England and France. It was an amazing trip and there are a few pictures on &lt;a class="link" href="https://www.instagram.com/jpomfret/" target="_blank" rel="noopener"
>Instagram&lt;/a> if you are curious about what I got up to.&lt;/p>
&lt;p>This is also going to be a quick post. I asked a question on Twitter last week about what happens when you have multiple triggers on a table. I got the answer (Thanks Aaron!), but figured this would be a good thing to demonstrate.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>I have also been playing with Azure Data Studio and the new notebook feature, so I answered this question with a step-by-step example in a notebook. I also found that you can easily store these notebooks on GitHub so I have uploaded it to my demos repo for you to follow along.&lt;/p>
&lt;p>&lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/Notebooks/TriggerOrder.ipynb" target="_blank" rel="noopener"
>Trigger Order Notebook&lt;/a>&lt;/p>
&lt;p>TL;DR: Triggers execute one after the other. I demonstrated this by creating a table with three insert triggers that each waited 2 seconds and recorded the timestamp.&lt;/p></description></item><item><title>Desired State Configuration: Resources</title><link>https://jpomfret.github.io/desired-state-configuration-resources/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-resources/</guid><description>&lt;p>A critical part of our DSC configuration is made up of resources. These are the building blocks we need to to define our desired state.  There are two kinds of resources that we can use: class based and MOF based (most common). We are going to focus our efforts today on looking at MOF based resources.&lt;/p>
&lt;p>Resources come packaged up as modules and our servers, which use at least WMF 4.0, come with several built-in. We have two main options for additional resources; we can find DSC resource modules in the PowerShell Gallery or we can write our own.&lt;/p>
&lt;h3 id="finding-dsc-resources">Finding DSC Resources&lt;/h3>
&lt;p>To find existing resources we have a few options. We could navigate to the &lt;a class="link" href="https://www.powershellgallery.com/packages?q=Tags%3A%22DSC%22" target="_blank" rel="noopener"
>PowerShell Gallery&lt;/a> website and browse through the modules with the ‘DSC’ tag. We could also use PowerShell by using the `Find-Module` command and the `-Tag` parameter to match our results from the PowerShell Gallery.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-Module -Tag DSC
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-Modules.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Our second option using PowerShell is to use &lt;code>Find-DscResource&lt;/code>. This cmdlet finds specific resources that are contained in modules. Running a count against that right now (4/1/2019) would return 1,413 resources that are available to configure our environment. Using the &lt;code>-Filter&lt;/code> parameter you can search for a keyword throughout the names, descriptions and tags of all these resources.&lt;/p>
&lt;h3 id="installing-resources">Installing Resources&lt;/h3>
&lt;p>If you find a DSC Resource you want to use in your configurations, for example to install SQL Server we will want to use SqlSetup from the SqlServerDsc module, you can install the module as you would a regular PowerShell module. For example, using &lt;code>Install-Module&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Install-Module.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Once the module is installed you can use these resources in your configurations. One thing to remember is the resources need to be available on both your authoring station and your target node.&lt;/p>
&lt;h3 id="five-useful-resources">Five Useful Resources&lt;/h3>
&lt;p>To wrap up I’m going to look at five resources I’ve used in my configurations to give you an idea of what is available.&lt;/p>
&lt;h4 id="file">File&lt;/h4>
&lt;p>The &lt;code>File&lt;/code> resource is one of the built-in resources and is useful for creating files and folders. For my SQL Servers I use it to create separate folders for the data, log, and tempdb files. Another use case would be to create a file and, by using the &lt;code>contents&lt;/code> property, we could even add data to it.&lt;/p>
&lt;h4 id="firewall">Firewall&lt;/h4>
&lt;p>Part of the &lt;code>NetworkingDsc&lt;/code> module, the &lt;code>Firewall&lt;/code> resource can be used to configure firewall rules on your target node. This is useful when installing SQL Server so you can open up access remotely. There is also a &lt;code>SqlWindowsFirewall&lt;/code> resource that will accomplish this task, but has less properties to configure. However, depending on your preferred setup, this may suffice.&lt;/p>
&lt;h4 id="sqlsetup">SqlSetup&lt;/h4>
&lt;p>&lt;code>SqlSetup&lt;/code> is the resource that installs SQL Server. Included in the &lt;code>SqlServerDsc&lt;/code> module there are many properties available to configure the installation just right.&lt;/p>
&lt;h4 id="script">Script&lt;/h4>
&lt;p>One of the most flexible resources is the &lt;code>Script&lt;/code>. This is also a built in resource and gives you the ability to code any PowerShell script into a simple resource. You basically write three mini functions: one that gets the current state, one that tests if it’s in the desired state, and finally one to ‘make it so’.&lt;/p>
&lt;h4 id="sqlagentoperatorhttpsgithubcompowershellsqlserverdscsqlagentoperator">&lt;a class="link" href="https://github.com/PowerShell/SqlServerDsc#sqlagentoperator" target="_blank" rel="noopener"
>SqlAgentOperator&lt;/a>&lt;/h4>
&lt;p>The final resource I’ve picked is part of the &lt;code>SqlServerDsc&lt;/code> module and allows you to create a SQL Agent operator. This is useful if you like to send alerts or notifications from your SQL Servers. The reason I picked this resource is that I wrote it. One of the best parts of DSC is that most of the modules are open source and available on Github. If there is a resource missing that you’d find useful, you are encouraged to write it and submit it to the Microsoft repos. That’s pretty cool if you ask me.&lt;/p></description></item><item><title>Desired State Configuration: Local Configuration Manager</title><link>https://jpomfret.github.io/desired-state-configuration-local-configuration-manager/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-local-configuration-manager/</guid><description>&lt;p>Once we have crafted the perfect configuration and shipped it out to our target nodes, it’s time for the magic to happen. The &lt;a class="link" href="https://jesspomfret.com/dsc-mof-files/" target="_blank" rel="noopener"
>MOF file that we created&lt;/a> by executing our configuration is translated and enacted by the Local Configuration Manager (LCM) on each target node. The LCM is the engine of DSC and plays a vital role in managing our target nodes.&lt;/p>
&lt;p>The LCM on each target node has many settings that can be configured using a meta configuration. This document is written very similarly to our regular DSC configurations and then pushed out to the target node.  I’m going to cover a few of the important LCM settings for use in &lt;code>push&lt;/code> mode. This is where the LCM passively waits for a MOF file to arrive. The other option is &lt;code>pull&lt;/code> mode- this is a little more complicated to set up and in this scenario the LCM is set to actively check in with a pull server for new configurations.&lt;/p>
&lt;h3 id="important-lcm-settings">&lt;strong>Important LCM Settings&lt;/strong>&lt;/h3>
&lt;p>As mentioned we are going to look at a subset of LCM settings. A full list is available at books online “&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/managing-nodes/metaconfig" target="_blank" rel="noopener"
>Configuring the Local Configuration Manager&lt;/a>”.&lt;/p>
&lt;p>[table id=6 /]&lt;/p>
&lt;h3 id="configure-the-lcm">Configure the LCM&lt;/h3>
&lt;p>We are going to change a couple of the LCM settings by writing a meta configuration document, compiling it as a MOF and pushing it to our target node. The LCM on that target node will receive this MOF file and enact it to put the LCM into the desired state. To start with we can check out the available settings by using the &lt;code>Get-DscLocalConfigurationManager&lt;/code> cmdlet.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/get_before.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We are going to change two settings in this example. First, I’m going to change the ConfigurationModeFrequencyMins to 20 minutes, instead of the default of 15.  Secondly, I will change the RebootNodeIfNeeded to true. This means if I push out a configuration that requires a reboot my node will automatically reboot.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[DSCLocalConfigurationManager()]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">configuration LCMConfig
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node dscsvr2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Settings
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ConfigurationModeFrequencyMins = 20
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RebootNodeIfNeeded = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I’ll then execute the &lt;code>LCMConfig&lt;/code> configuration to generate a meta MOF file. You can see this is named with the target node name and then the extension is &lt;code>.meta.mof&lt;/code>. For a regular configuration the file would just be named with the target node name and the extension of just &lt;code>.mof&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">LCMConfig -Output .\output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/CreateMetaMof.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We will then enact this configuration using &lt;code>Set-DscLocalConfigurationManager&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Set-DscLocalConfigurationManager -Path .\output\ -ComputerName dscsvr2 -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/set-DscLcm-1024x239.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Once this is complete we can check our settings using the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2 |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Select-Object ConfigurationModeFrequencyMins, RebootNodeIfNeeded
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/get_after-1024x107.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now our LCM is in our defined desired state and we are ready to push out a configuration to set the desired state of our server.&lt;/p>
&lt;h3 id="applyandautocorrect">ApplyAndAutoCorrect&lt;/h3>
&lt;p>The LCM can also play an important role in keeping our servers in the desired state. If we changed the &lt;code>ConfigurationMode&lt;/code> to &lt;code>ApplyAndAutoCorrect&lt;/code> the LCM would check every 15 minutes (default value for &lt;code>ConfigurationModeFrequencyMins&lt;/code>) to ensure the server was still in the desired state. If it found it was not, the LCM would reenact the current MOF to put the server back to desired state. This is a pretty powerful feature but one that definitely requires some thought. I can imagine a 3rd party vendor wouldn’t be too happy if they set something on installation and my DSC configuration reverted that automatically.&lt;/p></description></item><item><title>Desired State Configuration: MOF Files</title><link>https://jpomfret.github.io/desired-state-configuration-mof-files/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/desired-state-configuration-mof-files/</guid><description>&lt;p>In my &lt;a class="link" href="https://jesspomfret.com/introduction-to-dsc/" target="_blank" rel="noopener"
>last Desired State Configuration (DSC) post&lt;/a> a couple of weeks ago I covered some of the concepts involved with DSC, and I also have a &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-110/" target="_blank" rel="noopener"
>T-SQL Tuesday post&lt;/a> to get you started writing your first configuration. So today we are going to look at the next step in the process: what happens after we’ve written a configuration?&lt;/p>
&lt;p>Here’s a quick recap on how to write a simple configuration. I’ve named this configuration &lt;code>CreateSqlFolder&lt;/code>. I’m targeting the node &lt;code>dscsvr2&lt;/code> and within that node block I’m using the &lt;code>File&lt;/code> resource to define my desired state of having a directory &lt;code>C:\SQL2017\SQLData&lt;/code> exist. With the final line of the script I’m calling the configuration and specifying where the output should land.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration CreateSqlFolder {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDesiredStateConfiguration
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node dscsvr2 {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> File CreateDataDir {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> DestinationPath = &amp;#39;C:\SQL2017\SQLData\&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Type = &amp;#39;Directory&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">CreateSqlFolder -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="generate-a-mof-file">Generate a MOF file&lt;/h3>
&lt;p>When I run this script I see the output in the screenshot below, a MOF file has been created in my output folder. Managed Object Format (MOF) files are used to describe Common Information Model (CIM) classes, these are industry standards which gives us flexibility in working with DSC. In DSC this is important as the MOF file is the artefact that will actually be used to configure our nodes. This MOF will be delivered to our target node and enacted by the Local Configuration Manager (LCM).&lt;/p>
&lt;p>The LCM will be covered in more detail in a later post, but for now know that it can be configured to be in either ‘Push’ mode or ‘Pull’ mode.  Pull mode is more complicated to set up but perhaps more appropriate for managing a large number of servers.  For now, we will look at the ‘Push’ mode where we will deliver the MOF manually to the target node for the LCM to enact.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/CreateSqlFoldersMOF.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Executing configuration to create a MOF file for target node.&lt;/p>
&lt;h3 id="publish-a-mof-file">Publish a MOF File&lt;/h3>
&lt;p>To get the MOF from my authoring station out to the target node I have a couple of options.  First, I can run &lt;code>Start-DscConfiguration&lt;/code>. This will push out the MOF and immediately enact the configuration.  Using the &lt;code>-wait&lt;/code> and &lt;code>-verbose&lt;/code> switches we can see the output returned to our PowerShell console as the configuration is applied.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Start-DscConfiguration -Path .\output\ -ComputerName dscsvr2 -Wait -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/startDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration-1024x242.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>If we want to push out the configuration but not immediately enact it we can use &lt;code>Publish-DscConfiguration&lt;/code>. I again used the &lt;code>-Verbose&lt;/code> switch to return output:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Publish-DscConfiguration -Path .\output\ -ComputerName dscsvr2 -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/publishDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/publishDscConfiguration-1024x122.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>You can see this in this screenshot it says ‘Configuration document successfully saved to pending state’, letting us know this is now ready for the LCM to enact. We can confirm our &lt;code>PendingConfiguration&lt;/code> by running the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscLocalConfigurationManager -CimSession dscsvr2 | Select LCMState
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/getDscConfiguration.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/getDscConfiguration-1024x123.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>To enact the pending configuration we would again use &lt;code>Start-DscConfiguration&lt;/code>, only this time instead of specifying a path we’d add the &lt;code>-UseExisting&lt;/code> switch.&lt;/p>
&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/03/startDscConfiguration_useexisting.png" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration_useexisting-1024x248.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It is important to note that if the LCM settings are currently set to the defaults this configuration will be automatically applied when the next consistency check runs within 15 mins.&lt;/p>
&lt;p>Look for a post coming soon where we’ll look at the LCM in more detail and examine some of the settings we have to manage how it works within DSC.&lt;/p></description></item><item><title>T-SQL Tuesday #112 - Dipping into your Cookie Jar</title><link>https://jpomfret.github.io/t-sql-tuesday-#112-dipping-into-your-cookie-jar/</link><pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#112-dipping-into-your-cookie-jar/</guid><description>&lt;p>&lt;a class="link" href="https://nocolumnname.blog/2019/03/05/t-sql-tuesday-112-dipping-into-your-cookie-jar/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It’s T-SQL Tuesday time again and our host this week, Shane O&amp;rsquo;Neill (&lt;a class="link" href="https://nocolumnname.blog/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SOZDBA" target="_blank" rel="noopener"
>t&lt;/a>), has challenged us to a humble brag. This really is a challenge for most of us as we do awesome things quietly, so thanks to Shane for forcing us to share some ‘cookies’.  This is also the second time in a couple of weeks that &lt;a class="link" href="https://www.amazon.com/Cant-Hurt-Me-Master-Your-ebook/dp/B07H453KGH" target="_blank" rel="noopener"
>David Goggins’ book&lt;/a> has been mentioned, which means I need to move it up on my to-read list.&lt;/p>
&lt;p>Imposter syndrome is something a lot of us struggle with. I am going to share a couple of things that I’m proud of and that I can look back on when things get tough.  As I’ve written before I’m working hard on stepping out of my comfort zone to prepare and deliver technical presentations. Recently I’ve been building a presentation that has caused me to wonder many times, what was I thinking.  Hopefully this recap will remind me that I can do it.&lt;/p>
&lt;h3 id="automating-nonproduction-refreshes">Automating Nonproduction Refreshes&lt;/h3>
&lt;p>This first story is a technical one. I was working on a project a year or so ago that involved complicated changes to business processes and therefore it was decided that development and test environments needed to be as similar to production as possible.  This meant that I was inundated with requests to take backups of production and restore to many nonproduction environments.  This got old fast.&lt;/p>
&lt;p>The process not only involved the backup/restore piece, the data in production was both sensitive and encrypted using TDE.  I started writing PowerShell scripts for each step in the process, restoring certificates and databases, unencrypting and then removing certificates and then masking the sensitive data (which involved calling a T-SQL stored procedure built by my colleague, &lt;a class="link" href="https://twitter.com/awickham" target="_blank" rel="noopener"
>Andrew&lt;/a>).  This was fine to begin with, but as the production data grew the process took longer and longer, which meant that the nonproduction environment was unavailable for longer each time.&lt;/p>
&lt;p>After awhile this process was no longer acceptable because the test environments were down for too long. I met with the team and we came up with a plan.  We ended up agreeing that we would create a process to create ‘restore points’.  These would basically be points in time from production, that were prepared for nonproduction use, off hours in a temporary environment.  I utilized a combination of &lt;a class="link" href="https://developer.ibm.com/urbancode/products/urbancode-deploy/" target="_blank" rel="noopener"
>Urban Code Deploy&lt;/a> (UCD), a product we already had in house, and PowerShell scripts to give the developers the power to first create a restore point, and then to be able to refresh environments from these prepackaged backups.&lt;/p>
&lt;p>The process in UCD to create a restore point took the scripts I had been using manually and packaged them up so the result was a folder of already unencrypted, masked databases that were safe to restore to any of the test environments.  Everyone was happy, I no longer had to do this mundane, boring request and the team could refresh whenever they needed to, only having to wait for the restore to finish.&lt;/p>
&lt;p>There is still plenty of room for improvement in this process. Perhaps we could use some kind of database cloning technology to minimize space requirements and reduce the time to restore. Another option would be running the databases in containers. The developers could then just spin up an environment when they needed to test something. For now, though, the process is making everyone’s life easier, and that was a big win for both myself and the project team.&lt;/p>
&lt;h3 id="speaking-in-front-of-people">Speaking in Front of People!&lt;/h3>
&lt;p>The second humble brag is that I’m speaking in front of people! A year ago I had just agreed to give my first user group presentation and I was terrified.  I had set a goal of giving back to the community by blogging/writing and speaking, but the speaking part was by far the most difficult for me to get my head around. &lt;/p>
&lt;p>Thinking back to college, I dreaded finding out that class projects included final presentations. This hasn’t changed, and now I was voluntarily going to present.  In the last year I’ve put together a pretty decent hour-long presentation on data compression that I’ve delivered not once, but four times.  Each time I’ve grown in confidence and felt more like I could be myself in front of a crowd.  One of my demos that I’m particularly proud of shows how data compression affects the data storage on the page.  I then &lt;a class="link" href="https://jesspomfret.com/data-compression-internals/" target="_blank" rel="noopener"
>wrote about this demo&lt;/a> and it was picked up and featured in &lt;a class="link" href="https://www.brentozar.com" target="_blank" rel="noopener"
>Brent Ozar’s&lt;/a> weekly links.&lt;/p>
&lt;p>I’ve also had my second presentation topic selected for &lt;a class="link" href="https://datagrillen.com" target="_blank" rel="noopener"
>DataGrillen&lt;/a> and &lt;a class="link" href="https://www.sqlsaturday.com/827/eventhome.aspx" target="_blank" rel="noopener"
>SQL Saturday Cincinnati&lt;/a>. I’m working hard to make sure I can deliver an informative and useful session on the possibilities of using PowerShell Desired State Configuration to install and configure SQL Server. During this process it has often felt like I’ve bitten off more than I can chew, both on my topic choice and the fact I submitted it to an international conference with a superstar line up of speakers.&lt;/p>
&lt;p>But here’s to remember the wins, and knowing I’ve already overcome these same doubts and challenges to get my first presentation up and running. This will be a great post to come back to when I need to dip in the cookie jar.&lt;/p></description></item><item><title>Introduction to Desired State Configuration</title><link>https://jpomfret.github.io/introduction-to-desired-state-configuration/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/introduction-to-desired-state-configuration/</guid><description>&lt;p>I’m currently working on a pretty interesting project to explore using PowerShell’s Desired State Configuration (DSC) to manage SQL Servers. DSC uses declarative language to define the desired state of your infrastructure.&lt;/p>
&lt;p>Ensuring that the directory &lt;code>C:\Test&lt;/code> exists is a simple example. A more complicated example would be the complete configuration of a SQL Server. This is my end goal.&lt;/p>
&lt;p>This post is aiming to just introduce DSC and a few of the concepts that come along with it, and give us a good building block for future posts that dive deeper into this topic.&lt;/p>
&lt;p>The infrastructure that surrounds DSC warrants several posts on its own, so for this first scratch of the surface just know that we will write DSC Configuration documents and these documents will be managed and executed on our target nodes.&lt;/p>
&lt;h3 id="declarative-vs-imperative">Declarative Vs Imperative&lt;/h3>
&lt;p>If you are already familiar with PowerShell scripts you write imperative code, or the actual instructions on how to accomplish something. For example if I want to create a folder I’d write:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">New-Item -Path C:\test -ItemType Directory
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>However, when writing DSC configurations you use declarative language, where you describe the desired state without having to instruct exactly how to get there. Using the same example you would add the following resource block to your configuration document to ensure the &lt;code>C:\test&lt;/code> folder exists.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">File CreateTestDir {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> DestinationPath = &amp;#39;C:\test&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Type = &amp;#39;Directory&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="resources">Resources&lt;/h3>
&lt;p>Resources are one of the central building blocks in DSC. Each resource contains the code that takes the declarative syntax you write and makes it happen. In our example above our file resource will translate our desired state into regular PowerShell code, most likely using the same &lt;code>New-Item&lt;/code> cmdlet that we had in our example. This resource is built into Windows so we can’t examine it to prove that.&lt;/p>
&lt;p>There are currently 22 resources available within the built in PSDesiredStateConfiguration module. The table below contains the descriptions of a few, for a full list you can review the &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/reference/resources/windows/builtinresource" target="_blank" rel="noopener"
>Microsoft docs&lt;/a>.&lt;/p>
&lt;p>[table id=2 /]&lt;/p>
&lt;p>On top of these built in resources are hundreds more that have been developed by Microsoft, or by the community. They come packaged just like modules and most can be installed directly from the &lt;a class="link" href="https://www.powershellgallery.com/packages?q=DSC" target="_blank" rel="noopener"
>PowerShell Gallery&lt;/a>), some examples are:&lt;/p>
&lt;p>[table id=5 /]&lt;/p>
&lt;p>As you can see DSC can be used to configure a wide variety of components. We can collect resources from several modules and then combine them into one configuration document to describe our desired state.&lt;/p>
&lt;h3 id="idempotent">Idempotent&lt;/h3>
&lt;p>Another interesting aspect of DSC is that the resources are written to be idempotent. This means that in our file example above if the folder already exists it won’t try and create it again.&lt;/p>
&lt;p>There are two main types of resources, class based and MOF based. We’ll be focusing on MOF based in this post.  Within each resource are three functions: &lt;code>Get-TargetResource&lt;/code>, &lt;code>Set-TargetResource&lt;/code> and &lt;code>Test-TargetResource&lt;/code>.  When you run a configuration that contains our file resource example, the &lt;code>Test-TargetResource&lt;/code> will fire first to see whether we’re already in the desired state. That function returns true or false. If the directory doesn’t exist, the &lt;code>Set-TargetResource&lt;/code> will fire to create the folder.&lt;/p>
&lt;p>On the other hand, if we ran the &lt;code>New-Item&lt;/code> snippet and the directory already existed it would throw errors. To avoid this, we would have to wrap it with extra logic ourselves to test whether the folder exits as expected and if not, go ahead and create it.&lt;/p>
&lt;h3 id="so-why-use-desired-state-configuration">So Why Use Desired State Configuration?&lt;/h3>
&lt;p>DSC is a framework that provides the ability to manage our infrastructure with Configuration as Code.  There are several benefits to managing our infrastructure this way. The two biggest reasons I think DSC will work well in my particular scenario is automation and that my infrastructure will be in source control.&lt;/p>
&lt;p>DSC enables automation for building SQL Servers by creating a configuration document that defines exactly how the server should be built. For example, the document tells you things like where the database data and log files should be stored, how tempdb is configured, and whether database mail is enabled.&lt;/p>
&lt;p>The configuration document can then be combined with configuration data, which contains values specific to this build. For example the instance name and perhaps the edition and version of SQL Server to install would be found in the configuration data.  We can reuse the same configuration document for every server, all we would need to do is provide the appropriate configuration data.&lt;/p>
&lt;p>Using configuration as code for building SQL Servers gives us another great benefit because these documents can be checked into source control.  We now know exactly what the servers should look like, and when we make a change that will be tracked in source control. This creates documentation on your entire build. If you needed to rebuild a server during disaster recovery, for example, you could just push that configuration out to a new server and wait for it to end up in your desired state.&lt;/p>
&lt;h3 id="resources-1">Resources&lt;/h3>
&lt;p>If you want to know more about DSC I have listed a few links below. I also plan on expanding this post into a series covering general DSC concepts as well as the specifics for managing SQL Servers with DSC.&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.amazon.com/PowerShell-Desired-State-Configuration-Depth-ebook/dp/B07CNQD3M9/ref=sr_1_1" target="_blank" rel="noopener"
>Pro PowerShell Desired State Configuration: An In-Depth Guide to Windows PowerShell DSC&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/overview/overview" target="_blank" rel="noopener"
>Windows PowerShell Desired State Configuration&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/configurations/configdata" target="_blank" rel="noopener"
>Using configuration data in DS&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/azure/devops/learn/what-is-infrastructure-as-code" target="_blank" rel="noopener"
>Infrastructure as Code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data Compression Internals</title><link>https://jpomfret.github.io/data-compression-internals/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/data-compression-internals/</guid><description>&lt;p>Last year I gave my first user group presentation on data compression and since then I’ve also given this talk at both SQL Saturday Columbus 2018 and SQL Saturday Cleveland 2019. One of my favourite demos from the presentation is taking a look under the covers to see what SQL Server does with compressed data at the page level. This blog post is going to walk through this demo.  If you’d like to follow along you can pull down my docker image and have your own environment up and running in no time. As long as you already have docker running on your machine you can use the following to get setup and the full demo script is available on &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/DataCompression/01_Page_Internals.sql" target="_blank" rel="noopener"
>GitHub&lt;/a>.&lt;/p>
&lt;p>docker pull jpomfret7/datacompression:demo&lt;/p>
&lt;p>docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d jpomfret7/datacompression:demo&lt;/p>
&lt;p>I’m using an empty database to start this demo so you only really need my containerized environment for the end when we need to load some more sample data.&lt;/p>
&lt;p>Within my empty database, named &lt;code>CompressTest&lt;/code>, I first create a simple table named &lt;code>employee&lt;/code> and insert three rows. A couple of important things to note on this table. Firstly, the datatypes I’ve chosen are all fixed length, and the values inserted leave a lot of empty space. Secondly, there are a few examples of repeating data, both in the name columns and the city.  These conditions make this table a great candidate for both row and page compression.&lt;/p>
&lt;p>USE CompressTest
GO&lt;/p>
&lt;p>CREATE TABLE employee (
employeeId bigint PRIMARY KEY,
firstName char(100),
lastName char(100),
address1 char(250),
city char(50)
)&lt;/p>
&lt;p>INSERT INTO employee
values (1, &amp;lsquo;Alex&amp;rsquo;,&amp;lsquo;Young&amp;rsquo;,&amp;lsquo;2 Sand Run&amp;rsquo;,&amp;lsquo;Akron&amp;rsquo;),
(2, &amp;lsquo;Richard&amp;rsquo;,&amp;lsquo;Young&amp;rsquo;,&amp;lsquo;77 High St.&amp;rsquo;,&amp;lsquo;Akron&amp;rsquo;),
(3, &amp;lsquo;Alexis&amp;rsquo;,&amp;lsquo;Young&amp;rsquo;,&amp;lsquo;1 First Ave.&amp;rsquo;,&amp;lsquo;Richfield&amp;rsquo;)&lt;/p>
&lt;h3 id="compression-level-none">Compression Level: None&lt;/h3>
&lt;p>Once we have our table created we need to use a couple of undocumented, but widely used commands to view the underlying page. Step one is to find the page that contains our data. We&amp;rsquo;ll use &lt;code>DBCC IND&lt;/code> for this and pass in our database and table name.&lt;/p>
&lt;p>DBCC IND (&amp;lsquo;CompressTest&amp;rsquo;, &amp;rsquo;employee&amp;rsquo;, 1);&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_IND.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The &lt;code>employee&lt;/code> table has two types of pages shown here. The &lt;code>PageType&lt;/code> of 1 is our data page and the one we are interested in today. Once we have our &lt;code>PageFID (1)&lt;/code> and &lt;code>PagePID (376)&lt;/code>, we’ll take these values and use them as parameters for &lt;code>DBCC PAGE&lt;/code>.&lt;/p>
&lt;p>We first need to switch on trace flag &lt;code>3604&lt;/code>: this will write the output of our &lt;code>DBCC PAGE&lt;/code> command to the messages tab instead of the event log.&lt;/p>
&lt;p>There are 4 parameters for &lt;code>DBCC PAGE&lt;/code>: we will need to pass in the database name (or id), the file number, the page id and the print option.  Using a print option of 0 will give us just the page header. In these examples I’m going to use option 3 which gives us more details on the rows stored on the page. For more information on using &lt;code>DBCC PAGE&lt;/code> I’d recommend Paul Randal’s post &amp;ldquo;&lt;a class="link" href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2006/06/10/how-to-use-dbcc-page/" target="_blank" rel="noopener"
>How to use DBCC PAGE&lt;/a>&amp;rdquo;.&lt;/p>
&lt;p>We will run the following to inspect our page:&lt;/p>
&lt;p>DBCC TRACEON (3604);
GO
DBCC PAGE(&amp;lsquo;CompressTest&amp;rsquo;,1,376,3)&lt;/p>
&lt;p>There is a lot of information returned. Before you get overwhelmed, we are only going to look at a few data points from this so we can show the changes as we apply compression.  If you are interested in learning more about page internals, another Paul Randal post worth checking out is &amp;ldquo;&lt;a class="link" href="https://www.sqlskills.com/blogs/paul/inside-the-storage-engine-anatomy-of-a-page/%ef%bb%bf" target="_blank" rel="noopener"
>Inside the Storage Engine: Anatomy of a page&lt;/a>&amp;rdquo;.&lt;/p>
&lt;p>From the output below we’ll note the following: the &lt;code>pminlin&lt;/code> (the size of the fixed length data fields) is 512, the &lt;code>m_slotCnt&lt;/code> (the number of records on the page) is 3, and finally the &lt;code>m_freeCnt&lt;/code> (the number of free bytes on the page) is 6545.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_NONE.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Since we used option 3 for &lt;code>DBCC PAGE&lt;/code> we can also scroll down and see the data on our pages. The first record is below and is currently 515 bytes, and you can see on the right there is a lot of unused space.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_RECORD_NONEjpg.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="compression-level-row">Compression Level: Row&lt;/h3>
&lt;p>We’ll now take our employees table and apply &lt;code>ROW&lt;/code> compression to the clustered index. This physically changes how the data is stored on the page.  Any fixed length datatypes will now be stored in variable length fields where the data only uses the minimum number of bytes needed.&lt;/p>
&lt;p>ALTER TABLE employee REBUILD PARTITION = ALL
WITH (DATA_COMPRESSION = ROW)&lt;/p>
&lt;p>When compression is applied the pages are rewritten to disk, we need to use &lt;code>DBCC IND&lt;/code> to retrieve the new page information:&lt;/p>
&lt;p>DBCC IND (&amp;lsquo;CompressTest&amp;rsquo;, &amp;rsquo;employee&amp;rsquo;, 1);&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_IND_ROW.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We then use these values for &lt;code>DBCC PAGE&lt;/code>. The trace flag we set earlier is good for the session, therefore if we’re in the same query window we don’t need to rerun that command.&lt;/p>
&lt;p>DBCC PAGE(&amp;lsquo;CompressTest&amp;rsquo;,1,384,3)&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_ROW.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now that our table is ROW compressed you can see the &lt;code>pminlength&lt;/code> is only 5, this is reduced from 512 when our table wasn’t compressed. You can also note &lt;code>m_slotCnt&lt;/code> is still 3, which is expected, and the amount of free space on the page &lt;code>m_freeCnt&lt;/code> has increased to 7971.&lt;/p>
&lt;p>If we again scroll down to inspect our first row we can see it is now only 35 bytes and the highlighted area on the right clearly shows that the unused space within our row has been removed.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_RECORD_ROW.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="compression-level-page">Compression Level: Page&lt;/h3>
&lt;p>Our final type of compression is &lt;code>PAGE&lt;/code> compression. This compresses the data using three steps:&lt;/p>
&lt;ol>
&lt;li>Row compression: removing any wasted space from fixed length datatypes as we have already seen.&lt;/li>
&lt;li>Prefix compression: the engine will look for repeating data at the start of each column on each page and store that once in the anchor record.  Each row will then store a pointer back to that anchor record which is stored within the compression information section of the page.&lt;/li>
&lt;li>Dictionary compression: similar to prefix compression except the repeating data can be anywhere on the page instead of being restricted to the same column.&lt;/li>
&lt;/ol>
&lt;p>ALTER TABLE employee REBUILD PARTITION = ALL
WITH (DATA_COMPRESSION = PAGE&lt;/p>
&lt;p>Running &lt;code>DBCC IND&lt;/code> again will get us our newly written page:&lt;/p>
&lt;p>DBCC IND (&amp;lsquo;CompressTest&amp;rsquo;, &amp;rsquo;employee&amp;rsquo;, 1);&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_IND_PAGE.jpg"
loading="lazy"
>&lt;/p>
&lt;p>We’ll examine it with &lt;code>DBCC PAGE&lt;/code>:&lt;/p>
&lt;p>DBCC PAGE(&amp;lsquo;CompressTest&amp;rsquo;,1,376,3)&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_PAGE.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The interesting thing here is that nothing has changed, but if I check the DMVs the &lt;code>employee&lt;/code> table shows as &lt;code>PAGE&lt;/code> compressed.&lt;/p>
&lt;p>Use CompressTest&lt;/p>
&lt;p>SELECT
schema_name(obj.SCHEMA_ID) as SchemaName,
obj.name as TableName,
ind.name as IndexName,
ind.type_desc as IndexType,
pas.row_count as NumberOfRows,
pas.used_page_count as UsedPageCount,
(pas.used_page_count * 8)/1024 as SizeUsedMB,
par.data_compression_desc as DataCompression,
(pas.reserved_page_count * 8)/1024 as SizeReservedMB
FROM sys.objects obj
INNER JOIN sys.indexes ind
ON obj.object_id = ind.object_id
INNER JOIN sys.partitions par
ON par.index_id = ind.index_id
AND par.object_id = obj.object_id
INNER JOIN sys.dm_db_partition_stats pas
ON pas.partition_id = par.partition_id
WHERE obj.schema_id &amp;lt;&amp;gt; 4
ORDER BY SizeUsedMB desc&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/dmvs.jpg"
loading="lazy"
>&lt;/p>
&lt;p>SQL Server outsmarted us a little here. I have only inserted three rows into the employee table and we know after looking at the &lt;code>DBCC PAGE&lt;/code> output that there is plenty of free space on this page.  SQL Server will only apply PAGE compression if it needs to as there is a higher CPU cost to use prefix and dictionary compression. If page compression isn’t going to save any pages the engine leaves the table with just &lt;code>ROW&lt;/code> compression applied.&lt;/p>
&lt;p>Running the following will insert 200 more rows into my employee table. I’m getting the data from the &lt;code>vEmployee&lt;/code> view within &lt;code>AdventureWorks2017&lt;/code>.&lt;/p>
&lt;p>INSERT INTO employee (employeeId, firstName,lastName, address1, city)
SELECT TOP 200 BusinessEntityID, FirstName, LastName, AddressLine1, CITY
FROM AdventureWorks2017.HumanResources.vEmployee
WHERE BusinessEntityID &amp;gt; 3&lt;/p>
&lt;p>Now when I run &lt;code>DBCC IND&lt;/code> I can see the employee table uses four pages, two of them being data pages.&lt;/p>
&lt;p>DBCC IND (&amp;lsquo;CompressTest&amp;rsquo;, &amp;rsquo;employee&amp;rsquo;, 1);&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_IND_FINAL.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Finally, we’ll look at &lt;code>DBCC PAGE&lt;/code> to see page compression in action:&lt;/p>
&lt;p>DBCC PAGE(&amp;lsquo;CompressTest&amp;rsquo;,1,376,3)&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/DBCC_PAGE_FINAL.jpg"
loading="lazy"
>&lt;/p>
&lt;p>You can now see there are 102 rows on our page (&lt;code>m_slotCnt&lt;/code>) and our fixed length data types are still 5 (&lt;code>pminlen&lt;/code>). Directly after the page header is the compression information section. You can see on the right that repeating data values have been pulled out and stored here.  There are two possible &lt;code>CI Header Flags&lt;/code> and they are both set here. &lt;code>CI_HAS_ANCHOR_RECORD&lt;/code> shows that prefix compression has been used and &lt;code>CI_HAS_DICTIONARY&lt;/code> shows that dictionary compression has been used.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/compressionInfo_Page-1.jpg"
loading="lazy"
>&lt;/p>
&lt;p>If I scroll down a little further, I’ll come to the first row. The record is now only 24 bytes and you can see that a lot of the data has been replaced. The values &lt;code>Alex&lt;/code>, &lt;code>Young&lt;/code> and &lt;code>Akron&lt;/code> all now reside in the compression information and this record just contains pointers.&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/jesspomfret.com/wp-content/uploads/2019/02/DBCC_PAGE_RECORD_PAGE.jpg?fit=650%2C206&amp;amp;ssl=1"
loading="lazy"
>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>It’s easy to just apply compression to your databases and see massive space savings. This post hopes to shed a little light on what happens to your pages when using row and page compression. I recently gave this talk to the DBA Fundamentals virtual chapter so if you’d like to see the rest the recording is available on &lt;a class="link" href="http://%ef%bb%bfhttps://www.youtube.com/watch?v=F02NqGP2Gyg" target="_blank" rel="noopener"
>YouTube&lt;/a>.&lt;/p></description></item><item><title>T-SQL Tuesday #111 - What is Your "Why"?</title><link>https://jpomfret.github.io/t-sql-tuesday-#111-what-is-your-why/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#111-what-is-your-why/</guid><description>&lt;p>&lt;a class="link" href="https://andyleonard.blog/2019/02/t-sql-tuesday-111-what-is-your-why/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>It’s T-SQL Tuesday again and thanks goes this week to our host Andy Leonard (&lt;a class="link" href="https://andyleonard.blog" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/AndyLeonard" target="_blank" rel="noopener"
>t&lt;/a>). Andy has picked a great topic on why we do the things we do. With our hectic lives it’s good sometimes to sit down and have a think about something like this.  I’ve split my ‘what I do’ up into three buckets that, although are separate, have a lot of common ‘whys’.&lt;/p>
&lt;h3 id="database-administrator">Database Administrator&lt;/h3>
&lt;p>I have been a SQL Server DBA since 2011, and it was kind of an accident.  I graduated from The University of Akron with an undergraduate degree in Information Systems and a Masters in Business Administration, and marched off into the real world to find the perfect job.  I had many interviews for all kinds of IT related positions and although I thought most of them went well and would provide a good first job for me, the folks on the other side of the table didn’t seem to agree. Part of this was I required a visa to work in the US. It was always going to be challenging to convince someone to take a chance on a new graduate and then spend a lot of money and effort to file paperwork.&lt;/p>
&lt;p>I was about a week or two away from having to leave the country as my current visa was expiring, when I got a job offer as a SQL Server DBA.  I had gone on this interview without much hope. I didn’t feel like I was qualified, and although I enjoyed my databases classes in college I had very little experience actually doing this kind of work (I’d worked as a grad assistant CMS web admin for the past two years).&lt;/p>
&lt;p>The first few weeks of this position were rough. I still remember one of the first days, I was being trained to handle simple change requests (adding permissions, changing agent jobs, creating tables etc.), my colleague told me to right click on a particular job and select properties so we could change the schedule.  I had zero idea how to even find the job in management studio. I just looked at him blankly as he had to walk me through expanding ‘SQL Server Agent’ and then ‘Jobs’.&lt;/p>
&lt;p>One thing I’m pretty proud of is I work hard at learning and growing. I like to know how things work and why, and although I was well out of my depth to start with I managed to gain a lot of knowledge in this role.  I got more comfortable handling change requests and eventually got to the point where I could get through a two week on call rotation without having to wake up the senior DBA, even when our reasonably complicated replication setup caught fire in the middle of the night for no reason.&lt;/p>
&lt;p>After four years I moved companies to my current role, still working as a SQL Server DBA.  It’s been over 8 years since that first day, and I still love what I do.  I think the main reason is that the learning never stops.  There is always new challenges to attack, new ideas to test out, and with that comes so many opportunities.&lt;/p>
&lt;h3 id="crossfitter">Crossfitter&lt;/h3>
&lt;p>&lt;a class="link" href="https://www.crossfit.com/" target="_blank" rel="noopener"
>CrossFit&lt;/a>, for those of you that don’t know, is a workout regime consisting of “constantly varied high-intensity functional movements”. It combines cardio, weightlifting and gymnastics into a workout that will leave you led on the floor wondering why it was so hard. In my opinion though the beauty of CrossFit is what happens around the workout. You end up with a random group of people all struggling through the workout of the day (WOD) together, cheering each other on and then reminiscing about how they made it through.  It’s not at all unlike team sports, and that’s why when I found CrossFit it filled a huge void.&lt;/p>
&lt;p>I played soccer (proper football) for the first 23 years of my life. I’m pretty sure as soon as I could walk I had a football at my feet.  When my senior season of college was over in 2009 I stopped playing and I stopped working out. I didn’t stop eating like an athlete though – bring on the weight gain!  Once I started CrossFit I found the same things I’d loved about football - a group of people all working towards the same goals coupled with the challenge of trying to constantly improve at something.&lt;/p>
&lt;p>CrossFit never seems to get any easier and I think that’s why I love it. I recently had a memory pop up on Facebook that 7 years ago I had got my first handstand pushup.  These days I can (not going to say easily) get 50 in a workout, but now I’m working towards being able to walk on my hands.  The challenges never end.  Whenever I reach a goal, there is always the next one. Can I lift more weight, can I do that workout faster, can I catch my wife (no is the answer here…). These challenges, and the amazing community at my gym, are why I’ll keep on Crossfitting.&lt;/p>
&lt;h3 id="community-collaborator">Community Collaborator&lt;/h3>
&lt;p>At some point in 2018 I decided it was time. Time to stop being a lurker and to give something back to the amazing SQL Server and PowerShell communities. I started this blog, actually with a &lt;a class="link" href="https://jesspomfret.com/t-sql-tuesday-99/" target="_blank" rel="noopener"
>T-SQL Tuesday post&lt;/a>, and I signed up to present at my local user group in Cleveland.  I’ve mentioned this before, but this was way out of my comfort zone. The thought of standing in front of people and sharing what I think I know is terrifying. However, it has also turned out to be an amazing experience so far.&lt;/p>
&lt;p>When I started out as a DBA, remember - knowing almost nothing, I leaned heavily on the community. I signed up for newsletters and RSS feeds, trying to read as much as I could.  I also attended our user group meetings and some nearby SQL Saturdays and although I was terrible at the networking portion of the events, I got a lot of great tips from the sessions.&lt;/p>
&lt;p>This past weekend I presented at SQL Saturday Cleveland. This was my second SQL Saturday as a speaker and I still can’t believe how much I loved it.  The whole day I was on a high. I gave my session at 8:30am and it went pretty well. I attended a few great sessions, had lunch with some amazing friends and even recorded a &lt;a class="link" href="https://www.youtube.com/watch?v=9Zk3R7_Pr-U" target="_blank" rel="noopener"
>video with Bert&lt;/a> for his famous YouTube channel.  I got to work on Monday and had a notification from LinkedIn that I had a message. Someone had taken the time to write and let me know they’d enjoyed my session and that it had been useful.  This made my day, and made me realize I’m actually giving something back.  I still have a long way to go but I’m definitely making steps in the right direction.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Throughout the three parts of my life I’ve talked about there have been a few common themes.  I love a good challenge - from standing in front of a room of peers and giving a presentation to trying to walk on my hands, these targets that are just out of reach provide a lot of motivation to me.  What goes hand in hand with this is my desire to keep learning and growing. The DBA landscape is never going to stay still or constant, and with that there are forever going to be opportunities for me to evolve as a data professional.  That’s pretty exciting, and that’s why I do what I do.&lt;/p></description></item><item><title>dbatools with Bert</title><link>https://jpomfret.github.io/dbatools-with-bert/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/dbatools-with-bert/</guid><description>&lt;p>This weekend, while I was having a great time at SQL Saturday Cleveland, I ran into my friend Bert (&lt;a class="link" href="https://bertwagner.com/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/bertwagner" target="_blank" rel="noopener"
>t&lt;/a>). He had some dbatools questions, which I was happy to help him with.  Now that dbatools has over 500 commands, it is both awesome and terrifying.  Bert wanted to know how to automate his database backups and then check he was using the correct recovery model.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="backup-your-databases">Backup your databases&lt;/h3>
&lt;p>Bert’s first question was how to automate his database backups. I showed him the &lt;code>Backup-DbaDatabase&lt;/code> command and explained some of the parameters available.&lt;/p>
&lt;p>First, we backed up all the databases on his instance:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase -SqlInstance localhost\sql2017
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase-2.gif"
loading="lazy"
>&lt;/p>
&lt;p>We then looked at specifying a specific database to backup:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">﻿Backup-DbaDatabase -SqlInstance localhost\sql2017 -Database ApplicationDatabase
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_Database-1.gif"
loading="lazy"
>&lt;/p>
&lt;p>This command will backup to the default backup location for your instance. If you want to override that you can use the &lt;code>BackupDirectory&lt;/code> parameter:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance localhost `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Database ApplicationDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-BackupDirectory C:\backups\﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_BackupDir.gif"
loading="lazy"
>&lt;/p>
&lt;p>The final options we looked at were two switches: &lt;code>CompressBackup,&lt;/code>which will make use of backup compression, and &lt;code>CopyOnly,&lt;/code> which will leave your LSN chain intact by taking a copy only backup.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Backup-DbaDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance localhost `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Database ApplicationDatabase `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-CompressBackup `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-CopyOnly
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Backup-DbaDatabase_Switches.gif"
loading="lazy"
>&lt;/p>
&lt;p>Once we had Bert’s databases all backed up and safe he realized he also needed to make sure the database recovery model was set correctly.&lt;/p>
&lt;h3 id="check-database-recovery-model">Check Database Recovery Model&lt;/h3>
&lt;p>Bert wanted to make sure he was using the Full recovery model for his databases. We went about finding any that were in Simple with the following:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRecoveryModel.gif"
loading="lazy"
>&lt;/p>
&lt;p>We also talked about running this command against multiple instances, either by using a central management server or from a text file:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-SqlInstance $(Get-Content C:\servers.txt) `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-RecoveryModel Simple﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We found some databases in the simple recovery model that we wanted to change. This can easily be accomplished by piping the output of our get command into the set command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Set-DbaDbRecoveryModel -RecoveryMode Full
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DbaDbRecoveryModel_Set.gif"
loading="lazy"
>&lt;/p>
&lt;h3 id="search-dbatools-commands">Search dbatools Commands&lt;/h3>
&lt;p>The final tip I had for Bert was how to use &lt;code>Find-DbaCommand&lt;/code> to help him find the commands he needed to complete his tasks.&lt;/p>
&lt;p>A lot of the commands have tags, which is a good way to find anything relating to compression. For example:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-DbaCommand -Tag Compression﻿
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-DbaCommand_Compression.gif"
loading="lazy"
>&lt;/p>
&lt;p>You can also just specify keywords and the command will search for any reference of these within the inline command based help for all the commands.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Find-DbaCommand triggers
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/Find-DbaCommand_Trigger-1.gif"
loading="lazy"
>&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>There are many more resources to get help with dbatools. Firstly, their website, &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>https://dbatools.io/&lt;/a>, has a lot of great information on how to get started.&lt;/p>
&lt;p>Secondly, the dbatools slack channel is always full of people who can lend a hand. You can get an invite here: &lt;a class="link" href="https://dbatools.io/slack/" target="_blank" rel="noopener"
>https://dbatools.io/slack/&lt;/a>.&lt;/p>
&lt;p>Finally, feel free to get in contact with me if you have any questions or need some help finding the commands you need to get going with dbatools.&lt;/p></description></item><item><title>Data Compression Demos in Containers</title><link>https://jpomfret.github.io/data-compression-demos-in-containers/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/data-compression-demos-in-containers/</guid><description>&lt;p>One of the things I want to spend more time exploring this year is containers, specifically SQL Server running in containers. While I’ve been preparing to give my data compression talk at SQL Saturday Cleveland, which is only two weeks away, and generally procrastinating from all other responsibilities, I decided I should change my demos from running against a VM on my laptop to running against a containerized instance of SQL Server.&lt;/p>
&lt;p>First, a couple of blog shout outs. This idea had been in my mind for a little while after reading &lt;a class="link" href="https://www.cathrinewilhelmsen.net/2018/12/02/sql-server-2019-docker-container/" target="_blank" rel="noopener"
>a great post by Cathrine Wilhelmsen&lt;/a> where she wrote about moving her demo environments to containers. I’ve also spent a decent amount of time reading &lt;a class="link" href="https://dbafromthecold.com/2017/03/15/summary-of-my-container-series/" target="_blank" rel="noopener"
>Andrew Pruski’s excellent container series&lt;/a>. These were the source for the majority of both my knowledge and confidence that I could pull this off.&lt;/p>
&lt;h3 id="demo-environment-setup">Demo environment setup&lt;/h3>
&lt;p>I use three databases in my data compression demos - first is a copy of AdventureWorks. On my VM I actually removed some of the tables I didn’t use to skinny it down a little. The second database I named ‘SalesOrderLarge’ and is just three copies of the SalesOrderDetail and SalesOrderHeader tables that have been enlarged using Jonathan Kehayias’ script. Finally, I have a ‘CompressTest’ database that is just an empty shell that I use to create one table in during the demos to show the internals of compression.&lt;/p>
&lt;h3 id="creating-my-image">Creating my Image&lt;/h3>
&lt;p>The first step in this process was to stop the SQL Server service in my VM and copy out the database files (mdf &amp;amp; ldf) to use in my container. I’ll save these into a folder on my laptop for now and then they’ll be copied into my image as it’s built.&lt;/p>
&lt;p>I created the dockerfile below (following Andrew’s &lt;a class="link" href="https://dbafromthecold.com/2018/12/11/attaching-databases-via-a-dockerfile-update/" target="_blank" rel="noopener"
>example&lt;/a>) that will be used to build an image for my datacompression containers. This image is based off the SQL Server 2019 CTP 2.2 image from Microsoft, then we’ll create a folder and copy in a script and the files for my three databases. The last line runs the script and starts SQL Server.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># start with the SQL 2019 CTP 2.2 image
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">FROM mcr.microsoft.com/mssql/server:2019-CTP2.2-ubuntu
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># create a folder and copy in the attach-db script
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">RUN mkdir /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY attach-db.sh /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># copy in AdventureWorks database files
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/AdventureWorks2017.mdf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/AdventureWorks2017_log.ldf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># copy in CompressTest database files
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/CompressTest.mdf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/CompressTest_log.ldf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># copy in SalesOrderLarge database files
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/SalesOrderLarge.mdf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">COPY DatabaseFiles/SalesOrderLarge_log.ldf /var/opt/sqlserver
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># attach databases and start SQL Server
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ENTRYPOINT /var/opt/sqlserver/attach-db.sh &amp;amp; /opt/mssql/bin/sqlservr
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The attach-db.sh script uses sqlcmd to execute three &lt;code>CREATE DATABASE&lt;/code> commands to finish the setup of my environment and I end up with a folder structure as shown below. You don’t have to put the database files in a separate folder, I only did that for neatness.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/fileSetup.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Everything is setup so I’m ready to build my image. I’ll navigate to the DataCompression folder from my PowerShell console and run the &lt;code>docker build&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">cd C:\Docker\DataCompression\
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker build -t datacompression .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>To check out my new image I’ll use &lt;code>docker images&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/dockerimages-1024x116.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="running-my-demo-container">Running my demo container&lt;/h3>
&lt;p>I used to have a ResetEnvironment script that I would use to make sure my VM was setup for the start of my demos. This allowed me to run through and practice my demos as many times as I wanted (read, a lot, probably too many). With containers I will just start up a new one, run through my demos, then remove it, easy as that.&lt;/p>
&lt;p>Side note - I will run some activity against the AdventureWorks database once I start the container. This is needed to show the compression suggestions since these are partially based off index usage. I’ll probably still use some kind of script to start a container and run the load in preparation for the demos.&lt;/p>
&lt;p>For now, I’ll just use the following to start up a container from my new datacompression image. The important parts of this code are firstly, setting &lt;code>-p 1433:1433&lt;/code>, this maps the containers port 1433 to the host computers port. Secondly, we need to set two environmental variables, &lt;code>ACCEPT_EULA=Y&lt;/code> to accept the end user licensing agreement and &lt;code>SA_PASSWORD=’password’&lt;/code> to create the SA password for the instance. In this case the password needs to match what I have used in my &lt;code>attach.sh&lt;/code> script otherwise when that runs it’ll throw a failed login error and we won’t have any databases created.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d datacompression
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>My plan is to use this setup for my demos at SQL Saturday Cleveland. It’s pretty cool that this works exactly the same in my VM running Windows Server 2016 and SQL Server 2016, or in a linux container running SQL Server 2019.&lt;/p>
&lt;h3 id="try-it-yourself">Try it yourself&lt;/h3>
&lt;p>I&amp;rsquo;m going to write a few follow up posts that make use of this container so if you want to follow along or set up your own environment for tinkering you can pull it down from &lt;a class="link" href="https://cloud.docker.com/repository/registry-1.docker.io/jpomfret7/datacompression" target="_blank" rel="noopener"
>docker hub&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker pull jpomfret7/datacompression:demo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d jpomfret7/datacompression:demo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>T-SQL Tuesday #110 - "Automate All the Things"</title><link>https://jpomfret.github.io/t-sql-tuesday-#110-automate-all-the-things/</link><pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#110-automate-all-the-things/</guid><description>&lt;p>&lt;a class="link" href="https://garrybargsley.com/t-sql-tuesday-110-automate-all-the-things/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Automation is something that interests me greatly, and I think if you have read even one of my previous posts you’ll know that my favorite tool for this kind of work is PowerShell.  This is the perfect topic to kick off T-SQL Tuesday for 2019 so thanks goes to Garry Bargsley for hosting this month.&lt;/p>
&lt;p>One of my goals for 2019 is to improve our server build process, it’s currently reasonably well scripted but there are some definite gaps. I’ve started looking at using PowerShell Desired State Configuration (DSC) to install and configure SQL Server to both meet our needs and increase our speed and efficiency. Although full automation is a stretch goal for this project the DSC technology can certainly scale to accomplish that.&lt;/p>
&lt;h3 id="powershell-desired-state-configuration">PowerShell Desired State Configuration&lt;/h3>
&lt;p>PowerShell DSC is a platform to support the concept of Infrastructure as Code (IaC).  It uses declarative syntax instead of the usual imperative syntax of PowerShell.  This means that you describe your desired state rather than the specific steps needed to get there.  There are two modes for DSC, push and pull, although pull mode offers more features and scalability, we’ll look at writing our configuration and using push mode for this blog post to keep it simple.&lt;/p>
&lt;p>I hope that this blog post will be the first of a series this year as I work to finalize the full process of installing and configuring SQL Server with DSC. For now I will share step 1, ensuring the freshly built Windows OS meets the necessary prerequisites for the SQL Server installation.  Today we’ll look at installing two Windows Features ‘.NET 3.5 Features’ and ‘Active Directory module for Windows PowerShell’ on our target node.&lt;/p>
&lt;p>The first thing we’ll want to do is to update the in-box DSC resources, PSDesiredStateConfiguration. This version comes with Windows PowerShell 4.0 and 5.0 however there have been notable improvements that we will want to take advantage of.  Since the updated module is available in the PowerShell Gallery we can install it to our workstation using the following (note the name change):&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Install-Module PSDSCResources
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Since we are using the push mode we need to make sure any modules we use to write our configurations are available on our target nodes.  I have manually copied the &lt;code>PSDSCResource&lt;/code> module to a path within the &lt;code>$env:PSModulePath&lt;/code> on the target node so it’ll be available when the configuration is enacted there. There are other ways to handle this including setting up a resource module file share.&lt;/p>
&lt;h3 id="writing-our-first-configuration">Writing our First Configuration&lt;/h3>
&lt;p>We’re now ready to write our first configuration, although we are still writing PowerShell the syntax is a little different.  One of my favorite things about PowerShell is using the command based help to discover how to execute new cmdlets and functions, DSC is no different here. We can use &lt;code>Get-DscResource&lt;/code> to list all the resources we have available.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/GetDscResource.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Since we are going to use the WindowsFeature resource we can find out how to use that by passing in the &lt;code>-Syntax&lt;/code> parameter to &lt;code>Get-DscResource&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/WindowsFeatureSyntax.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The only required parameter for the WindowsFeature resource is Name so we’ll include that.  I also like to include the ‘Ensure’ parameter, it defaults to ‘Present’ but I feel it makes it clearer to specifically define that.  I also want to set ‘IncludeAllSubFeature’ to true so those get installed also. &lt;/p>
&lt;p>The below code is all we need to get started with our first configuration. Apart from the resource blocks that we have already mentioned there are a couple of other important parts to note. First is the keyword &lt;code>Configuration&lt;/code> which shows we are writing a DSC Configuration document. In this case I’ve named our configuration ‘SQLServerPreReq’. Secondly, the &lt;code>Node&lt;/code> keyword is important, this defines the target node for our configuration.  It’s important to remember that this is a simple example, it is possible to pass in multiple node names or to parameterize that node name to make our configuration more useful.&lt;/p>
&lt;p>The last line of code calls our SQLServerPreReq configuration and specifies the output path.  When you call a configuration a MOF file is created which is the document that will be sent to the target node and used to both enact the configuration and monitor for configuration drift (a feature of the pull mode that we’ll save for a future post).&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration SQLServerPreReq {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDSCResources
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node &amp;#39;DscSvr2&amp;#39; {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeature dotNet
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = &amp;#39;NET-Framework-Features&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeature ADPowershell
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = &amp;#39;RSAT-AD-PowerShell&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SQLServerPreReq -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Once we have our MOF file the final step is to enact our configuration against our target node.  I’m using the &lt;code>-wait&lt;/code> and &lt;code>-verbose&lt;/code> parameters so that the configuration doesn’t run in the background and we can view the verbose messages on screen as it executes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Start-DscConfiguration -Path .\Output\ -Wait -Verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a class="link" href="https://jesspomfret.com/wp-content/uploads/2019/01/startDscConfiguration1-1.jpg" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/startDscConfiguration1-1.jpg"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Once this runs successfully you can confirm the features are installed using &lt;code>Get-WindowsFeature&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-WindowsFeature -ComputerName dscsvr2 `
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-Name @(&amp;#39;RSAT-AD-PowerShell&amp;#39;,&amp;#39;NET-Framework-Features&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jpomfret.github.io/images/GetWindowsFeature.jpg"
loading="lazy"
>&lt;/p>
&lt;h3 id="using-composite-resources">Using Composite Resources&lt;/h3>
&lt;p>This simple configuration shows how you can install two windows features using PowerShell DSC, however it is rather redundant to have to specify separate resource blocks for each feature.  Since we updated our in-box DSC Resources to the newer PSDSCResources module we are able to use the new WindowsFeatureSet resource which is an example of a composite resource. We can review the syntax again using Get-DscResource:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Get-DscResource -Name WindowsFeatureSet -Syntax
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The main difference is that WindowsFeatureSet takes an array of features to install and then translates this to use multiple WindowsFeature resources when the MOF file is created.  This allows us to keep our configuration document as tidy and concise as possible.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Configuration SQLServerPreReq_v2 {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Import-DscResource -ModuleName PSDSCResources
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Node &amp;#39;DscSvr2&amp;#39; {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> WindowsFeatureSet PreReqFeatures
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Name = @(&amp;#39;NET-Framework-Features&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;RSAT-AD-PowerShell&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Ensure = &amp;#39;Present&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> IncludeAllSubFeature = $true
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SQLServerPreReq_v2 -Output .\Output\
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>That’s step one towards automating my SQL Server builds, I’m looking forward to adding more to this series. Thanks again to Garry for picking the perfect T-SQL Tuesday topic to kick off the year with.&lt;/p></description></item><item><title>2018 Comes to a Close</title><link>https://jpomfret.github.io/2018-comes-to-a-close/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/2018-comes-to-a-close/</guid><description>&lt;p>2018 has been an amazing year for me. I never imagined when I started this blog in February that I would be going into 2019 even more excited to give back to the community. Public speaking is not something that comes easy to me, and in fact I find it pretty terrifying, which is why I’m amazed at how much I enjoyed presenting this past year.  The SQL Server community is definitely a special group of professionals, so thank you to any and all of you who have made this year one of my best yet.&lt;/p>
&lt;p>Now I’m all about goals and targets so in May I wrote a T-SQL Tuesday post about giving back to the community and I set several goals to complete in 2018, let’s review.&lt;/p>
&lt;h2 id="10-blog-posts-on-jesspomfretcom-3-technical">10 blog posts on jesspomfret.com (3 technical)&lt;/h2>
&lt;p>This year end summary makes 12 posts for the year, and 7 were technical posts. My most popular post was ‘&lt;a class="link" href="https://jesspomfret.com/checking-backups-with-dbachecks/" target="_blank" rel="noopener"
>Checking backups with dbachecks&lt;/a>’.  I also contributed a guest post on the dbatools blog covering &lt;a class="link" href="https://jesspomfret.com/checking-backups-with-dbachecks/" target="_blank" rel="noopener"
>migrating application databases.&lt;/a>&lt;/p>
&lt;h2 id="50-prs-to-dbatoolsdbachecks">50 PR’s to dbatools/dbachecks&lt;/h2>
&lt;p>This goal was to encourage contributions to open source projects.  My involvement with dbatools and dbachecks is what got me started down this path of blogging and speaking so I wanted to make sure I didn’t lose sight of that.&lt;/p>
&lt;p>So far in 2018 I have 40 pull requests to dbatools and 12 to dbachecks, so I have met my goal here with a few days to spare.  I also contributed to a few other open source projects this year, I had 3 PRs accepted to the Microsoft Sql-Docs repository, and I have 2 open PRs to the SqlServerDsc module.&lt;/p>
&lt;h2 id="1-user-group--1-other-presentation">1 user group &amp;amp; 1 ‘other’ presentation&lt;/h2>
&lt;p>This goal I surprised myself with, I thought this would be the hardest one to meet and it ended up being the most rewarding.  I presented my &lt;a class="link" href="https://jesspomfret.com/first-user-group-presentation-i-survived/" target="_blank" rel="noopener"
>first ever session&lt;/a> to the Ohio North SQL Server User Group in June, it went ok. My topic was data compression and I found myself sweating, rushing and a little short on content.  With some amazing feedback and support I added some content and delivered it again at SQL Saturday Columbus and it went much better. I then presented it for a third time to the DBA Fundamentals Virtual chapter, which was good although a little strange not being able to see your audience.&lt;/p>
&lt;p>I also presented a PSPowerHour lightening talk on apply data compression with dbatools, I’d say I well exceeded this goal.&lt;/p>
&lt;h2 id="looking-forward-to-2019">Looking forward to 2019&lt;/h2>
&lt;p>My main goal for 2019 is to improve my blog, regular content is going to be the most important part of increasing my traffic so I’m going to aim for 26 blog posts in 2019.  I’m also excited to already have two speaking engagements on the calendar as I aim for 5 presentations this year. My data compression session has been accepted to SQL Saturday Cleveland, and a new session I’m working on ‘Installing SQL Server with PowerShell DSC’ was selected for DataGrillen.&lt;/p>
&lt;p>I hope everyone has a great last few days of 2018 and starts 2019 strong.&lt;/p>
&lt;p>Cheers!&lt;/p></description></item><item><title>PSParameterSets: Mandatory sometimes</title><link>https://jpomfret.github.io/psparametersets-mandatory-sometimes/</link><pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/psparametersets-mandatory-sometimes/</guid><description>&lt;p>I came across a situation this week where I wanted to add the option of running an existing script,  for a specific server/database combination.  The script currently has no parameters and runs against all servers in the environment from a scheduled task.  I wanted to make sure that behavior didn’t change. The other requirement was that if I specified Server, Database should be a mandatory parameter and vice versa.&lt;/p>
&lt;p>The final solution was to add the two parameters to a parameter set and make them both mandatory.  I also had to add a different DefaultParameterSet (thanks to &lt;a class="link" href="http://twitter.com/awickham" target="_blank" rel="noopener"
>Andrew&lt;/a> for this idea), otherwise it defaulted to the defined parameter set, meaning the script always required both Server and Database parameters.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[CmdletBinding(DefaultParametersetname=&amp;#34;Normal&amp;#34;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">param (
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [Parameter(Mandatory = $True, ParameterSetName=&amp;#39;Specific&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [string]$ServerName,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [Parameter(Mandatory = $True, ParameterSetName=&amp;#39;Specific&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [string]$DatabaseName
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">if($PSCmdlet.ParameterSetName -eq &amp;#39;Normal&amp;#39;) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host &amp;#39;Running without params&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">} else {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host (&amp;#34;Server: {0}&amp;#34; -f $ServerName)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Write-Host (&amp;#34;Database: {0}&amp;#34; -f $DatabaseName)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I saved the above code as params.ps1 to ensure my test cases worked. As you can see in my testing below, I can still run params.ps1 without any parameters, this replicates the current behavior of my nightly job. &lt;/p>
&lt;p>I can also now pass in Server and Database parameters, if I specify one the script will prompt for the other since they are both required.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ParamsTest.jpg"
loading="lazy"
>&lt;/p></description></item><item><title>T-SQL Tuesday #108 - Non SQL Server Technologies</title><link>https://jpomfret.github.io/t-sql-tuesday-#108-non-sql-server-technologies/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#108-non-sql-server-technologies/</guid><description>&lt;p>&lt;a class="link" href="https://curiousaboutdata.com/2018/10/29/t-sql-tuesday-108-invitation-non-sql-server-technologies/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues.png"
loading="lazy"
>&lt;/a>It’s T-SQL Tuesday time again, I have struggled in the last month or two to get anything up on my blog. Turns out weddings are pretty time consuming ?! Now that I’m happily married and home from an amazing &lt;a class="link" href="https://www.instagram.com/jpomfret/" target="_blank" rel="noopener"
>honeymoon in Hawaii&lt;/a> it’s back to work on my blog and professional development.  Which makes this T-SQL Tuesday topic a perfect one to get back to, so thanks to Malathi Mahadeven (&lt;a class="link" href="https://curiousaboutdata.com" target="_blank" rel="noopener"
>B&lt;/a>|&lt;a class="link" href="https://twitter.com/sqlmal" target="_blank" rel="noopener"
>T&lt;/a>) for hosting this month.&lt;/p>
&lt;p>I feel like with last week’s PASS Summit (I didn’t attend this year so just watching from afar) it makes it even harder than usual to pick just one thing to learn.  There are so many things right now that I want to read about or fiddle with.&lt;/p>
&lt;p>I’ve decided to pick a main subject, with an auxiliary bonus area attached - kind of cheating, I know.  I’ve been working on a project at work to automate our SQL Server builds with Powershell Desired State Configuration (DSC) so this will be my main goal. I already have a basic understanding of how DSC works and how to install SQL Server with it, I want to improve this knowledge to the point where I can present a session on it.&lt;/p>
&lt;p>The side goal is docker/containers/kubernetes (maybe), I’m wondering if I could use these to test my DSC configurations, maybe not to install SQL Server (I have no idea though) but I imagine I could configure SQL Server running in a container.&lt;/p>
&lt;p>I saw the tweet below last week from the beard, &lt;a class="link" href="https://twitter.com/sqldbawithbeard" target="_blank" rel="noopener"
>Rob Sewell&lt;/a>, that quoted &lt;a class="link" href="https://twitter.com/bobwardms" target="_blank" rel="noopener"
>Bob Ward’s&lt;/a> thoughts on learning directions.  Feels like this is probably solid advice to justify my side goal.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/sqldbawithbeard/status/1061032613979267072" target="_blank" rel="noopener"
>https://twitter.com/sqldbawithbeard/status/1061032613979267072&lt;/a>&lt;/p>
&lt;h2 id="learning-plan">Learning Plan&lt;/h2>
&lt;h4 id="learn-dsc-basics--completed">Learn DSC Basics – completed&lt;/h4>
&lt;p>I’ve already started learning DSC, I was lucky enough to take a PowerShell DSC class a couple of months ago and that combined with reading online documentation and blogs has given me a good base.&lt;/p>
&lt;p>Resources:&lt;/p>
&lt;ul>
&lt;li>Microsoft Docs - &lt;a class="link" href="https://docs.microsoft.com/en-us/powershell/dsc/overview" target="_blank" rel="noopener"
>https://docs.microsoft.com/en-us/powershell/dsc/overview&lt;/a>&lt;/li>
&lt;li>SQLServerDSC Github - &lt;a class="link" href="https://github.com/PowerShell/SqlServerDsc" target="_blank" rel="noopener"
>https://github.com/PowerShell/SqlServerDsc&lt;/a>&lt;/li>
&lt;li>DSC Install of SQL Server &lt;a class="link" href="https://chrislumnah.com/2017/03/07/dsc-install-of-sql-server/" target="_blank" rel="noopener"
>https://chrislumnah.com/2017/03/07/dsc-install-of-sql-server/&lt;/a>&lt;/li>
&lt;li>Making modules available in Push mode &lt;a class="link" href="http://nanalakshmanan.com/blog/Push-Config-Pull-Module/" target="_blank" rel="noopener"
>http://nanalakshmanan.com/blog/Push-Config-Pull-Module/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="learn-more-about-how-dsc-resources-are-written-and-developed">Learn more about how DSC resources are written and developed&lt;/h4>
&lt;p>This is where I currently am, I have the basics up and running to install SQL Server (blog post coming one day) but there are things I’d like to configure that aren’t currently built into the SQLServerDSC module.  Since this is open sourced on github I have the opportunity to learn while doing, I’ve already started working on adding an SQL Agent Operator resource so I can configure an operator during my install.&lt;/p>
&lt;h4 id="dsc-sql-server-and-containers">DSC, SQL Server and Containers?&lt;/h4>
&lt;p>Can I even use DSC to configure SQL Server running in a container? I have no idea, but I plan on finding out.  If this is possible it feels like this would be a really easy way to spin up ‘unconfigured’ SQL Server and test my configurations.  If not – hey maybe I learned a bit about containers along the way, and it feels like those are only getting more mainstream.&lt;/p>
&lt;h4 id="final-goal---present-a-dsc-session">Final goal - Present a DSC session&lt;/h4>
&lt;p>My final goal is to create a &amp;lsquo;Automate your SQL Server Install with DSC&amp;rsquo; session. Presenting on something forces you to learn it in depth, this will be great for myself and hopefully the community. Hopefully it&amp;rsquo;ll make its way to a SQL Saturday next year.  The session would be a crash course on DSC specifically to install and configure SQL Server with the end goal of attendees being able to use this process to automate their own builds. Watch this space, currently in the idea phase.&lt;/p></description></item><item><title>Testing Availability Group Read-Only Routing with dbatools</title><link>https://jpomfret.github.io/testing-availability-group-read-only-routing-with-dbatools/</link><pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/testing-availability-group-read-only-routing-with-dbatools/</guid><description>&lt;p>I recently set up an Availability Group with the intent of using the secondary as a read only replica for reporting.  We have a few AG&amp;rsquo;s in our environment already but currently none are using this feature.&lt;/p>
&lt;p>I&amp;rsquo;m not going to step through setting up the AG or configuring the readable secondary as there are plenty of good posts out there as well as the official &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-only-access-on-an-availability-replica-sql-server?view=sql-server-2017" target="_blank" rel="noopener"
>books online documentation&lt;/a>.&lt;/p>
&lt;p>Once my AG was created I set the &amp;lsquo;Connections in Primary Role&amp;rsquo; to &amp;lsquo;Allow read/write connections&amp;rsquo; and the &amp;lsquo;Readable Secondary&amp;rsquo; to &amp;lsquo;Read-intent only&amp;rsquo; as shown below. On a side note it&amp;rsquo;s important to set these for both instances, if you&amp;rsquo;re running with 01B as the Primary after a failover by setting both you&amp;rsquo;ll get the same behavior, with read only connections being routed to the now secondary, 01A server.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/AGReplicas.jpg"
loading="lazy"
>&lt;/p>
&lt;p>The other part I needed to set up was &lt;a class="link" href="https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-only-routing-for-an-availability-group-sql-server?view=sql-server-2017" target="_blank" rel="noopener"
>read-only routing&lt;/a>, this enables SQL Server to reroute those read only connections to the appropriate replica.  You can also list the read only replicas by priority if you have multiple available or you can group them to enable load-balancing.&lt;/p>
&lt;p>Although this seems to be setup correctly so that connections that specify their application intent of read only will be routed to the secondary node I wanted to prove it. I used the &lt;a class="link" href="https://dbatools.io/functions/connect-dbainstance/" target="_blank" rel="noopener"
>Connect-DbaInstance&lt;/a> function from dbatools to connect to the listener name with the -ApplicationIntent property set to &amp;lsquo;ReadOnly&amp;rsquo;.&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance AGListenerName `
-Database DatabaseInAG `
-ApplicationIntent ReadOnly&lt;/p>
&lt;p>$svr.Query(&amp;lsquo;Select @@ServerName as ServerName&amp;rsquo;)&lt;/p>
&lt;h2 id="servername">ServerName&lt;/h2>
&lt;p>*******01B&lt;/p>
&lt;p>You can see it routed correctly to 01B which is currently the secondary node.  If I don&amp;rsquo;t specify the ApplicationIntent property on the connection it&amp;rsquo;ll be routed to the primary.&lt;/p>
&lt;p>$svr = Connect-DbaInstance -SqlInstance AGListenerName `
-Database DatabaseInAG&lt;/p>
&lt;p>$svr.Query(&amp;lsquo;Select @@ServerName as ServerName&amp;rsquo;)&lt;/p>
&lt;h2 id="servername-1">ServerName&lt;/h2>
&lt;p>*******01A&lt;/p>
&lt;p>This was a quick and easy way to ensure my read only routing was working as expected, and another great use of dbatools.&lt;/p></description></item><item><title>Data Compression + Backup Compression = Double Compression?</title><link>https://jpomfret.github.io/data-compression--backup-compression-double-compression/</link><pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/data-compression--backup-compression-double-compression/</guid><description>&lt;p>I recently gave my &lt;a class="link" href="http://jesspomfret.com/first-user-group-presentation-i-survived/" target="_blank" rel="noopener"
>first usergroup presentation in Cleveland&lt;/a>, closely followed by my first SQL Saturday presentation in Columbus. My chosen topic was row store data compression and I had a few great questions that I plan on answering with blog posts. First up&amp;hellip;&lt;/p>
&lt;h3 id="what-happens-if-i-use-data-compression-and-backup-compression-do-i-get-double-compression">What happens if I use data compression and backup compression, do I get double compression?&lt;/h3>
&lt;p>This is a great question, and without diving too deeply into how backup compression works I&amp;rsquo;m going to do a simple experiment on the WideWorldImporters database.  I&amp;rsquo;ve restored this database to my local SQL Server 2016 instance and I&amp;rsquo;m simply going to back it up several times under different conditions.&lt;/p>
&lt;p>After restoring the database it&amp;rsquo;s about 3GB in size, so our testing will be on a reasonably small database.  It would be interesting to see how the results change as the database size increases, perhaps a future blog post.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/WideWorldImporters-1-300x99.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Now I&amp;rsquo;m not sure how to write a blog post without mentioning &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a>, I&amp;rsquo;m using my favourite PowerShell module to check current database compression (Get-DbaDbCompression), apply data compression (Set-DbaDbCompression) and to create the backups with and without compression (Backup-DbaDatabase).&lt;/p>
&lt;p>The script I used to run through this experiment is available for you to test out on my &lt;a class="link" href="https://github.com/jpomfret/demos/blob/master/BlogExamples/01_DataCompressionPlusBackupCompression.ps1" target="_blank" rel="noopener"
>github&lt;/a> and the results are below:&lt;/p>
&lt;p>[table id=1 /]&lt;/p>
&lt;p>We can clearly see that using backup compression gives us a huge space savings.  On our database where none of the objects are compressed we get a 78% reduction in the size of the backup file. When all our objects are row compressed we get a 70% savings and even when all our objects are page compressed we still get a 60% reduction in size when we apply backup compression.&lt;/p>
&lt;p>Now, if we compare the difference in sizes for the three backups that used backup compression, we do get a small amount of additional space savings by using data compression in combination with backup compression. The backup file is 7% smaller when the database objects are row compressed and 6% smaller when page compression is applied, however, these savings aren&amp;rsquo;t nearly as significant as as just comparing whether backup compression is used or not.&lt;/p>
&lt;p>So to answer the question, we don&amp;rsquo;t get double the compression by using both data and backup compression, but whether we use data compression or not within our database using backup compression will get you a pretty significant space saving when looking at the size of the backup file on disk.&lt;/p></description></item><item><title>T-SQL Tuesday #104 – Code you can't live without</title><link>https://jpomfret.github.io/t-sql-tuesday-#104-code-you-cant-live-without/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#104-code-you-cant-live-without/</guid><description>&lt;p>&lt;a class="link" href="https://bertwagner.com/2018/07/03/code-youd-hate-to-live-without-t-sql-tuesday-104-invitation/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/a>As soon as I saw Bert Wagner (&lt;a class="link" href="https://twitter.com/bertwagner" target="_blank" rel="noopener"
>t&lt;/a>|&lt;a class="link" href="https://bertwagner.com/" target="_blank" rel="noopener"
>b&lt;/a>) post his T-SQL Tuesday topic last week I knew this was going to be a great one. I’m really looking forward to reading about everyone’s favorite code snippets so thanks Bert for hosting and choosing a fantastic subject!&lt;/p>
&lt;p>A lot of the code I can&amp;rsquo;t live without is either downloaded from the community (e.g. &lt;a class="link" href="http://whoisactive.com/" target="_blank" rel="noopener"
>sp_whoisactive&lt;/a>, &lt;a class="link" href="http://karaszi.com/spindexinfo-enhanced-index-information-procedure" target="_blank" rel="noopener"
>sp_indexinfo&lt;/a>, &lt;a class="link" href="https://www.brentozar.com/blitz/" target="_blank" rel="noopener"
>sp_blitz&lt;/a>), or very specific to my workplace so I&amp;rsquo;m going to share some code that I&amp;rsquo;ve been meaning to blog about.&lt;/p>
&lt;p>I’ve been using this at work recently and it also relates to the presentation I gave at the &lt;a class="link" href="http://jesspomfret.com/first-user-group-presentation-i-survived/" target="_blank" rel="noopener"
>ONSSUG June meeting&lt;/a> around data compression. The beginnings of this script originated online as I dug into learning about the DMVs that related to objects and compression and then customized for what I needed.&lt;/p>
&lt;p>If you run the below as is it will provide basic information about all objects in your database, except those in the &amp;lsquo;sys&amp;rsquo; schema, along with their current size and compression level.&lt;/p>
&lt;p>SELECT
schema_name(obj.SCHEMA_ID) as SchemaName,
obj.name as TableName,
ind.name as IndexName,
ind.type_desc as IndexType,
pas.row_count as NumberOfRows,
pas.used_page_count as UsedPageCount,
(pas.used_page_count * 8)/1024 as SizeUsedMB,
par.data_compression_desc as DataCompression
FROM sys.objects obj
INNER JOIN sys.indexes ind
ON obj.object_id = ind.object_id
INNER JOIN sys.partitions par
ON par.index_id = ind.index_id
AND par.object_id = obj.object_id
INNER JOIN sys.dm_db_partition_stats pas
ON pas.partition_id = par.partition_id
WHERE obj.schema_id &amp;lt;&amp;gt; 4 &amp;ndash; exclude objects in &amp;lsquo;sys&amp;rsquo; schema
&amp;ndash;AND schema_name(obj.schema_id) = &amp;lsquo;schemaName&amp;rsquo;
&amp;ndash;AND obj.name = &amp;rsquo;tableName&amp;rsquo;
ORDER BY SizeUsedMB desc&lt;/p>
&lt;p>(This is also available in my &lt;a class="link" href="https://github.com/jpomfret/ScriptsAndTips/blob/master/ObjectSizeAndCompression.sql" target="_blank" rel="noopener"
>GitHub Tips and Scripts Repo&lt;/a>)&lt;/p>
&lt;p>Now this T-SQL is great for a quick look at one database, but what if I want to run this script against every database in my environment? Well I popped over to PowerShell, fired up &lt;a class="link" href="http://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> and ran the following:&lt;/p>
&lt;p>get-command -Module dbatools -Name *compression*&lt;/p>
&lt;p>Bad news, there was no Get-DbaDbCompression, there were commands for compressing objects (Set-DbaDbCompression) and for getting suggested compression setting based on the &lt;a class="link" href="https://blogs.msdn.microsoft.com/blogdoezequiel/2011/01/03/the-sql-swiss-army-knife-6-evaluating-compression-gains/" target="_blank" rel="noopener"
>Tiger Teams best practices&lt;/a> (Test-DbaDbCompression), but nothing to just return the current compression status of the objects.&lt;/p>
&lt;p>What’s more exciting than just using the greatest PowerShell module ever created? Making it better by contributing! So I made sure I had the latest development branch synced up and got to work writing Get-DbaDbCompression.  This has now been merged into the main branch and is therefore available in the Powershell gallery, so if your dbatools module is up to date you can now run the following to get the same information as above from one database:&lt;/p>
&lt;p>Get-DbaDbCompression -SqlInstance serverName -Database databaseName&lt;/p>
&lt;p>Or go crazy and run it against a bunch of servers.&lt;/p>
&lt;p>$servers = Get-DbaRegisteredServer -SqlInstance cmsServer | select -expand servername
$compression = Get-DbaDbCompression -SqlInstance $servers
$compression | Out-GridView&lt;/p>
&lt;p>I hope this post might come in handy for anyone who is curious about data compression in their environments. Both the T-SQL and PowerShell versions provide not just the current compression setting but the size of the object too. Useful if you are about to apply compression and would like a before and after comparison to see how much space you saved.&lt;/p></description></item><item><title>First User Group Presentation - I Survived!</title><link>https://jpomfret.github.io/first-user-group-presentation-i-survived/</link><pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/first-user-group-presentation-i-survived/</guid><description>&lt;p>Well tonight marks three weeks since I gave my first user group presentation and you know what, it’s been a total whirlwind since then so I’ve had little time to reflect.  Myself and the fiancé closed on our first house and my parents flew in to visit. It’s very useful to have people on hand the first couple of weeks so we didn’t feel like completely unqualified homeowners. I spent some time right after the presentation to start breathing again and to jot down some thoughts, but this is the first chance I’ve had to report back.&lt;/p>
&lt;p>TL;DR I didn’t die, the SQL Server community is fantastic and I have amazing supportive friends.&lt;/p>
&lt;p>&lt;a class="link" href="https://twitter.com/Pittfurg/status/1004125722082934784" target="_blank" rel="noopener"
>https://twitter.com/Pittfurg/status/1004125722082934784&lt;/a>&lt;/p>
&lt;p>&lt;strong>Why Present?&lt;/strong>&lt;/p>
&lt;p>The answer to this is twofold. Firstly this year I’ve challenged myself to get more involved in the SQL Server community. For several years now I’ve attended user group meetings, SQL Saturday’s and even made it to the PASS Summit a couple of times, but I’ve never contributed anything. It’s been all take.   Last year I got involved with dbatools and that started my quest to return the favour.  I’m still currently wrestling with the ideas of “who wants to listen to what I have to say” and “do I really have anything to contribute anyway,” but I’m doing my best to keep one foot in front of the other and see what happens.&lt;/p>
&lt;p>Secondly, whenever I review my strengths and weaknesses at annual review time, communication, or more precisely public speaking is always something that I consider a weakness.  I’m not sure of a better way to improve in this area than to put myself out there and practice, so here’s to some professional development. I’m certain that gaining some knowledge, experience and confidence in this area will help me in many areas of my life.&lt;/p>
&lt;p>&lt;strong>What to Present?&lt;/strong>&lt;/p>
&lt;p>I ended up presenting on SQL Server data compression.  I talked about the types of data compression, and how the internals work before focusing on what you can compress, how to compress and how to decide what should be compressed.  This topic stemmed mainly from an issue at work where data compression was implemented with a large performance benefit.  This issue at work also encouraged me to spend some time looking at dbatools and compression. There were existing commands for Set-DbaDbCompression and Test-DbaDbCompression that I added some improvements to, and then I added Get-DbaDbCompression.&lt;/p>
&lt;p>The culmination of both the issue at work and working on dbatools commands for compression left me feeling like this was a great topic to share.  Since 2016 SP1 data compression is now a standard level feature, opening up the possibilities to a lot more people.&lt;/p>
&lt;p>&lt;strong>Improvements&lt;/strong>&lt;/p>
&lt;p>Overall I think the presentation went well, I delivered most of what I had planned on saying and my demo’s did a decent job of explaining the process for deciding what to compress and then applying compression through T-SQL, SSMS and PowerShell.  I was lucky to have some great friends in the audience (&lt;a class="link" href="https://twitter.com/awickham" target="_blank" rel="noopener"
>Andrew&lt;/a>, &lt;a class="link" href="https://twitter.com/Pittfurg" target="_blank" rel="noopener"
>Drew&lt;/a> and &lt;a class="link" href="https://twitter.com/erinstellato" target="_blank" rel="noopener"
>Erin&lt;/a>) who asked great questions which helped me to drive home certain points.&lt;/p>
&lt;p>My timing was definitely a bit off. I’d prepared for what I thought would be 45-60 minutes of content and it was a bit shorter than that.  I plan on adding some additional content and delivering slightly slower the next time I give this talk to fix this problem.&lt;/p>
&lt;p>I got some great feedback both from the speaker evaluations and from &lt;a class="link" href="https://www.sqlskills.com/blogs/erin/" target="_blank" rel="noopener"
>Erin Stellato&lt;/a> who went above and beyond my request for any tips and feedback she may have.  I’ll make sure to incorporate some real life stories on where compression has had an impact as well as adding some more demos.&lt;/p>
&lt;p>&lt;strong>What’s next?&lt;/strong>&lt;/p>
&lt;p>That’s right, there is a next! I took a chance and submitted my session to &lt;a class="link" href="http://www.sqlsaturday.com/736/eventhome.aspx" target="_blank" rel="noopener"
>SQL Saturday Columbus&lt;/a> and was lucky enough to be selected.  I’ve been to quite a few SQL Saturdays in Cleveland, Columbus, Pittsburgh and even Minneapolis, but this will be my first as a speaker.  If you’re in the area on July 28th and want to learn more about data compression I’ll be on at 8:30am in Room 6.&lt;/p>
&lt;p>I’m also going to work on some blog posts around data compression and the dbatools commands so watch out for that also.&lt;/p></description></item><item><title>Extended Events - Hidden Treasure</title><link>https://jpomfret.github.io/extended-events-hidden-treasure/</link><pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/extended-events-hidden-treasure/</guid><description>&lt;p>I was troubleshooting an issue last week which led to me firing up extended events to look at records being written to the transaction log, I typed into the search bar ‘Transaction’ hoping to find something that would do the trick and didn’t quite find what I was looking for.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/XEvents_Transaction.jpg"
loading="lazy"
>&lt;/p>
&lt;p>After a few more failed attempts I headed to the internet and found a &lt;a class="link" href="https://www.sqlskills.com/blogs/paul/t-sql-tuesday-67-monitoring-log-activity-with-extended-events/" target="_blank" rel="noopener"
>post by Paul Randal&lt;/a> describing exactly what I needed for this situation, using the [sqlserver].[transaction_log] event. Hold on, that’s exactly what I searched for.  I ran the T-SQL within his blog post, the event was successfully created and gave me the information I was looking for.&lt;/p>
&lt;p>I then noticed someone asked in the comments whether it was a bug that the transaction_log event doesn’t show up in the XEvents GUI and Paul had replied:&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/PaulRandal.jpg"
loading="lazy"
>&lt;/p>
&lt;p>It took me a second to find it but by default there is a filter on the ‘Channel’ column that doesn’t include ‘Debug’. Selecting that gives you a whole host of new XEvents to investigate (and use carefully, for example the transaction_log event can generate a lot of activity).&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/XEvents_ChannelTransaction.jpg"
loading="lazy"
>&lt;/p>
&lt;p>I’m not sure how often I’ll need these ‘Debug’ events, but it sure is nice to know they exist. I feel like there should be some notation in the GUI that there is a filter being applied, similar to the icon in Excel when you have something filtered.&lt;/p></description></item><item><title>T-SQL Tuesday #102 - Giving Back</title><link>https://jpomfret.github.io/t-sql-tuesday-#102-giving-back/</link><pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#102-giving-back/</guid><description>&lt;p>&lt;a class="link" href="https://scribnasium.com/2018/05/giving-back-t-sql-tuesday-102-invite/" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/a>First of all thanks to Riley Major (&lt;a class="link" href="https://scribnasium.com" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/RileyMajor" target="_blank" rel="noopener"
>t&lt;/a>) for hosting this months T-SQL Tuesday.  This is a great topic for me as I have mentioned before my goal this year is to work on getting my blog going and starting to give back to this amazing community. I am always impressed by the content people produce and share, whether it be blog posts, presentations or just snippets on Twitter and I have learnt so much in my career so far from these sources.  Honestly I often feel like I don&amp;rsquo;t have anything worth sharing and that what I know is obvious and everyone would know it.  Then I think back to that first Ohio North SQL Server User Group presentation I went to where Allen White (&lt;a class="link" href="http://dataperfpro.com/blog/" target="_blank" rel="noopener"
>b&lt;/a>|&lt;a class="link" href="https://twitter.com/SQLRunr" target="_blank" rel="noopener"
>t&lt;/a>) gave his now famous introduction, everyone has something to teach, so here goes.&lt;/p>
&lt;p>I am all about goals and since we&amp;rsquo;re already almost six months into 2018 I&amp;rsquo;m going to set some goals down on paper that will force me out of my comfort zone. I have been working towards these haphazardly so far, so I hope that writing these out will hold me accountable.&lt;/p>
&lt;h2 id="blog-posts">Blog Posts&lt;/h2>
&lt;p>I started this blog back in February of this year and so far have managed four posts, including this one and one guest post on the &lt;a class="link" href="https://dbatools.io/migrating-application-dbs/" target="_blank" rel="noopener"
>dbatools blog&lt;/a>.  My goal for the year is to have 10 posts on this blog, so I have 6 left to write.  My aim is to also write some more technical posts. I have been leaning on the T-SQL Tuesday topics so far, so I&amp;rsquo;ll aim for 3 of the 6 to be more technical in nature.&lt;/p>
&lt;h2 id="github">Github&lt;/h2>
&lt;p>So far this year I&amp;rsquo;ve had 22 pull requests (PRs) merged into the &lt;a class="link" href="https://github.com/sqlcollaborative/dbatools" target="_blank" rel="noopener"
>dbatools&lt;/a> module, and 8 PRs merged into the &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks&lt;/a> module. Contributing to these modules has been so much fun and has really improved both my PowerShell and Pester skills. I&amp;rsquo;ll aim to get this up to 50 PR&amp;rsquo;s by the end of 2018, so 20 to go here.&lt;/p>
&lt;h2 id="presenting">Presenting&lt;/h2>
&lt;p>Now this third section is the one that produces the most anxiety. I have thought about this for a while now and it&amp;rsquo;s time.  My goal here is to deliver one presentation to my local user group (it&amp;rsquo;s actually scheduled for June 5th, 28 days away but who&amp;rsquo;s counting&amp;hellip;), and one to another audience.  I imagine it&amp;rsquo;ll be the same presentation, I&amp;rsquo;ve been working hard on one for the user group on data compression, but I would love to deliver this to a SQL Saturday or some other local conference. I was hoping I could present to the user group and then submit to a local SQL Saturday but my best chance of accomplishing this by the end of 2018 is to submit to SQL Saturday Columbus which I need to do before May 29th.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>What&amp;rsquo;s better than a checklist? I hope 2018 is the start of many years of giving back and perhaps my 10th (or more) post of the year will be an update on my progress- watch this space.&lt;/p>
&lt;ul>
&lt;li>10 blog posts on jesspomfret.com
&lt;ul>
&lt;li>3 technical posts&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>50 PR&amp;rsquo;s to dbatools/dbachecks&lt;/li>
&lt;li>1 user group presentation&lt;/li>
&lt;li>1 &amp;lsquo;other&amp;rsquo; presentation&lt;/li>
&lt;/ul></description></item><item><title>T-SQL Tuesday #101 - The Multitool of my DBA toolbox</title><link>https://jpomfret.github.io/t-sql-tuesday-#101-the-multitool-of-my-dba-toolbox/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#101-the-multitool-of-my-dba-toolbox/</guid><description>&lt;p>&lt;a class="link" href="http://t-sql.dk/?p=1947" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/tsqltues-300x300.png"
loading="lazy"
>&lt;/a>Thanks to &lt;a class="link" href="http://t-sql.dk/" target="_blank" rel="noopener"
>Jens Vestergaard&lt;/a> for hosting T-SQL Tuesday #101.  When I saw the topic for this month’s T-SQL Tuesday, I knew instantly which tool I would write about. Although there are many great tools out there that make my job as a DBA easier (and I’m excited to read the summary for this month to see what everyone else leans on), there is one that has fundamentally changed far more than just my work day. First of all I love PowerShell; the ability to make my daily tasks both repeatable and automated is something that has always appealed to me. Then I found &lt;a class="link" href="http://dbatools.io" target="_blank" rel="noopener"
>dbatools&lt;/a>, which combines everything I love about PowerShell into an ever-evolving open source module.&lt;/p>
&lt;p>Once you &lt;a class="link" href="https://dbatools.io/install" target="_blank" rel="noopener"
>install the module&lt;/a> you can run the following to list all the available commands in your toolbox. It’s a good idea to keep your copy of the module updated and check this often as people are always adding new commands.&lt;/p>
&lt;p>Get-Command -Module dbatools -CommandType Function | Out-GridView&lt;/p>
&lt;p>Having hundreds of commands can be a little overwhelming. In no particular order, these are the top five that I use most or that save me the most time.&lt;/p>
&lt;h3 id="test-dbasqlbuild">Test-DbaSqlBuild&lt;/h3>
&lt;p>When I found this command I couldn’t have been more excited. My day-to-day job requires the care and watering of over 100 SQL Server instances of varying versions.  Using this command you can get the current build of all your instances and then compare that to the most recent available.  There are also parameters for how far you want to be from the latest version. Setting the -latest switch means just that, your server will only be seen as compliant if it’s on the latest release, passing in -1CU means that it can be no more than 1 cumulative update behind.&lt;/p>
&lt;p>This snippet takes the registered instances from our central management servers and pipes them into Test-DbaSqlBuild to determine if they are on the latest version. It creates an easy list of what needs patched.&lt;/p>
&lt;p>Get-DbaRegisteredServer -SqlInstance nonProdServers, prodServers |
Test-DbaSqlBuild -Latest&lt;/p>
&lt;h3 id="get-dbadiskspace">Get-DbaDiskSpace&lt;/h3>
&lt;p>This is a great command to have handy. Pass in one or many server names and it returns the current size, amount of space available and blocksize for all drives and mount points on that server. There are also switches to look for any drives that have SQL files on, or to check the filesystem fragmentation levels.  One great use of this command is to pass in a list of servers and filter for drives under 5% free space. This is a great proactive check of where action may be needed soon.&lt;/p>
&lt;p>Get-DbaDiskSpace -ComputerName $servers | Where-Object {$_.PercentFree -lt 5}&lt;/p>
&lt;h3 id="copy-dbadatabase">Copy-DbaDatabase&lt;/h3>
&lt;p>This command is used to move databases from one instance to another. You can use either a backup/restore method or by detaching and re-attaching the files. Check out my post on the &lt;a class="link" href="https://dbatools.io/migrating-application-dbs/" target="_blank" rel="noopener"
>dbatools blog&lt;/a> for a more detailed look on how I used this to save a lot of time on a recent project at work.&lt;/p>
&lt;p>Copy-DbaDatabase -Source SourceServer -Destination DestinationServer `
-Database MigratingDatabase -BackupRestore -NetworkShare \\fileshare\&lt;/p>
&lt;h3 id="repair-dbaoprhaneduser">Repair-DbaOprhanedUser&lt;/h3>
&lt;p>This command is pure wizardry and &lt;a class="link" href="https://voiceofthedba.com/2018/02/16/dbatools-and-orphaned-users/" target="_blank" rel="noopener"
>Steve Jones has a more extensive post&lt;/a> on this command, but it does exactly what it says. It syncs up the SIDs for SQL logins that you’ve migrated from one instance to another. This is not a difficult task, however this one line fixes any orphaned logins on your whole instance. Extremely useful if you are migrating a lot of databases to new servers.&lt;/p>
&lt;p>Repair-DbaOrphanUser -SqlInstance serverName&lt;/p>
&lt;h3 id="write-dbadatatable">Write-DbaDataTable&lt;/h3>
&lt;p>The final command for my top five takes an object in PowerShell and uses SQL bulk copy to insert it into a table in a SQL Server database. Using the -AutoCreateTable switch will do just that, if the table doesn’t exist it will be created. One thing to watch is this will be a heap so if you’re going to use this table going forward building the table ahead of time with appropriate indexes and keys is probably advisable. However, this one line can be very useful for quickly throwing results into a table to save or analyze further.&lt;/p>
&lt;p>$results | Write-DbaDataTable -SqlInstance serverName -Database databaseName -Table tableName -AutoCreateTable&lt;/p>
&lt;p> &lt;/p>
&lt;p>The Second reason dbatools is my favorite tool is all the other things I’ve gained and learnt from this module.  It’s been almost one year since my first pull request to dbatools, and at that point I had a decent handle on PowerShell but git was a foreign language. Guided by Chrissy LeMaire (&lt;a class="link" href="http://twitter.com/cl" target="_blank" rel="noopener"
>t&lt;/a>) and some other folks from the &lt;a class="link" href="http://dbatools.io/slack" target="_blank" rel="noopener"
>slack channel&lt;/a> I got the repo forked, created my own branch and then submitted a PR to get my contributions merged in.  Since then I’ve contributed multiple more PRs, everything from small fixes to the command based help, to writing a brand new command (Get-DbaDbCompression will be released soon!).&lt;/p>
&lt;p>This tool not only gives you hundreds of commands to make your job easier, it encourages you to branch out and get involved in a truly special community. You will meet some brilliant people to bounce ideas off, learn new skills like github, integration tests or even continuous integration and development, all while giving back to the amazing community that surrounds the SQL Server ecosystem.  This blog is the start of my attempt to give back while furthering my understanding of certain topics. In June I’ll be stepping even further outside of my comfort zone by presenting at my local user group on data compression, and of course that’ll feature some dbatools related demos.&lt;/p></description></item><item><title>Checking backups with dbachecks</title><link>https://jpomfret.github.io/checking-backups-with-dbachecks/</link><pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/checking-backups-with-dbachecks/</guid><description>&lt;p>Folks, there is something fantastic coming from the creators of dbatools!&lt;/p>
&lt;p>Chrissy LeMaire (&lt;a class="link" href="https://blog.netnerds.net/" target="_blank" rel="noopener"
>blog&lt;/a>|&lt;a class="link" href="https://twitter.com/cl" target="_blank" rel="noopener"
>twitter&lt;/a>) and Rob Sewell (&lt;a class="link" href="https://sqldbawithabeard.com/" target="_blank" rel="noopener"
>blog&lt;/a>|&lt;a class="link" href="https://twitter.com/sqldbawithbeard" target="_blank" rel="noopener"
>twitter&lt;/a>) just announced something big at SQLBits 2018, a new PowerShell module that combines &lt;a class="link" href="https://dbatools.io/" target="_blank" rel="noopener"
>dbatools&lt;/a> with &lt;a class="link" href="https://github.com/pester/Pester" target="_blank" rel="noopener"
>Pester&lt;/a> to ensure your environment is &amp;ldquo;as expected&amp;rdquo;. I&amp;rsquo;ve been lucky enough to get to know both Chrissy and Rob by contributing to dbatools and when they introduced me to this new module I instantly saw a lot of potential.&lt;/p>
&lt;p>I&amp;rsquo;m going to start off with just a small way to gain some value from dbachecks, ensuring your backups are completing with the frequency you expect.&lt;/p>
&lt;p>The module is hosted on &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>GitHub&lt;/a> which means you can fork and contribute to it just as you would with any other open source project. There is also a lot of useful information out there including the &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks#dbachecks" target="_blank" rel="noopener"
>readme&lt;/a>.  Reviewing this readme is an important first step as there are a couple of prerequisites and some potential caveats when you go to update the module.&lt;/p>
&lt;p>You can download a copy of the module from the PowerShell Gallery (if this doesn&amp;rsquo;t work for you due to corporate firewalls, PowerShell version etc. head back to the readme for more ways to get the module):&lt;/p>
&lt;p>Install-Module dbachecks&lt;/p>
&lt;p>First off let&amp;rsquo;s take a look at &lt;em>Get-DbcCheck&lt;/em> to look for checks we may want to implement: &lt;img src="https://jpomfret.github.io/images/Get-DbCCheck.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Each check has one unique tag which basically names the check and then a number of other tags that can also be used to call a collection of checks.&lt;/p>
&lt;p>For this example we are going to use several checks to ensure that we meet the following requirements:&lt;/p>
&lt;ul>
&lt;li>Full backup once a week - using &lt;em>LastFullBackup&lt;/em>&lt;/li>
&lt;li>Differential backup once a day - using &lt;em>LastDiffBackup&lt;/em>&lt;/li>
&lt;li>Log backup every hour - using &lt;em>LastLogBackup&lt;/em>&lt;/li>
&lt;/ul>
&lt;p>Since each of the three checks we want to run also have the &lt;em>LastBackup&lt;/em> tag we can use that to call the collection of checks at once.&lt;/p>
&lt;p>There are many ways to point dbachecks at your instances, for this simple example we&amp;rsquo;ll just pass in one server name to check.&lt;/p>
&lt;p>$server = &amp;ldquo;ServerName&amp;rdquo;
Invoke-DbcCheck -SqlInstance $server -Check LastBackup&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ChecksFailing.jpg"
loading="lazy"
>&lt;/p>
&lt;p>As you can clearly see from the test results there is a lot of red, meaning I&amp;rsquo;m not meeting backup requirements. However looking closer at the context we can see that the check is not configured for my specific needs &amp;ldquo;StackOverflow full backups on Server should be less than 1 days&amp;rdquo;, but I only require a full backup within 7 days.&lt;/p>
&lt;p>The checks are set up in a way that make them extremely flexible. You can configure them to meet your needs exactly. We can use &lt;em>Get-DbcConfig&lt;/em> to review the backup configurations.  Here you can see we&amp;rsquo;re looking for full backups every 1 day (policy.backup.fullmaxdays), differentials every 25 hours (policy.backup.diffmaxhours) and log backups every 15 minutes (policy.backup.logmaxminutes).&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/Get-DbcConfig.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Let&amp;rsquo;s change these configuration properties to match our requirements of a full backup within the last 7 days and a log backup in the last 60 minutes.&lt;/p>
&lt;p>Set-DbcConfig -Name policy.backup.fullmaxdays -Value 7
Set-DbcConfig -Name policy.backup.logmaxminutes -Value 60&lt;/p>
&lt;p>Now that the configuration is setup correctly we can rerun and confirm our environment backups are in the green.&lt;/p>
&lt;p>&lt;img src="https://jpomfret.github.io/images/ChecksSuccess.jpg"
loading="lazy"
>&lt;/p>
&lt;p>Validating your backups are running is just one small example of how you can utilize dbachecks to keep your environment in line.  In fact this is just the tip of the iceberg, there are 80 checks as of writing this post as well as multiple ways to display the results (including a pretty impressive PowerBi dashboard - &lt;a class="link" href="http://claudioessilva.eu/2018/02/22/dbachecks-using-power-bi-dashboards-to-analyse-results/" target="_blank" rel="noopener"
>Cláudio Silva has a great post on that&lt;/a>).&lt;/p>
&lt;p>I hope this has peaked your interest in dbachecks, I suggest heading over to the &lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks#dbachecks" target="_blank" rel="noopener"
>readme&lt;/a> to learn more or download a copy and get checking right away!&lt;/p>
&lt;p>Useful links:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/sqlcollaborative/dbachecks" target="_blank" rel="noopener"
>dbachecks on github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://dbachecks.io/blog" target="_blank" rel="noopener"
>dbachecks blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://dbachecks.io/introducing" target="_blank" rel="noopener"
>Introducing dbachecks&lt;/a> - including links to a plethora of blog posts by other contributors&lt;/li>
&lt;/ul></description></item><item><title>T-SQL Tuesday #99 - Door #1</title><link>https://jpomfret.github.io/t-sql-tuesday-#99-door-%231/</link><pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate><guid>https://jpomfret.github.io/t-sql-tuesday-#99-door-%231/</guid><description>&lt;p>&lt;a class="link" href="https://sqlblog.org/2018/02/06/t-sql-tuesday-99" target="_blank" rel="noopener"
>&lt;img src="https://jpomfret.github.io/images/mjtuesday.png"
loading="lazy"
alt="MJTuesday"
>&lt;/a>First off, welcome to my first T-SQL Tuesday which seems like the perfect first blog post to introduce myself and my non-SQL Server life.  Starting this blog and becoming more involved with the SQL Server community (read speaking) is my goal for 2018 so here goes nothing. Thanks to &lt;a class="link" href="http://twitter.com/AaronBertrand" target="_blank" rel="noopener"
>Aaron Bertrand&lt;/a> for hosting and picking a great topic.&lt;/p>
&lt;p>I was born in Oxford, England before my family moved to the small market town of &lt;a class="link" href="https://en.wikipedia.org/wiki/chippenham" target="_blank" rel="noopener"
>Chippenham, Wiltshire&lt;/a> where I was raised.  Leading to my main passion being proper football. We can argue about why this is the proper football in a later post perhaps (hint: you use your feet to kick a ball, no hands or egg-shaped objects here).&lt;/p>
&lt;p>My earliest memories involve playing and watching football. When I was in primary school (equivalent of elementary school in the USA) my mum fought with the headmistress until I was allowed to play on the boy’s team.  The boys weren’t particularly happy until they realized I could hold my own.&lt;/p>
&lt;p>I then played for Chippenham Town Ladies for many seasons while I was in Secondary School and earned appearances for the Wiltshire County team.  Next I joined the Bristol Academy of Sport at Filton College for the 2003/2004 season and played for Bristol Rovers Ladies.  This set up my big move across the pond as I was recruited to join &lt;a class="link" href="http://gozips.com/" target="_blank" rel="noopener"
>The University of Akron Zips&lt;/a> in 2005.  After four fantastic years of playing football every day and traveling around the United States, I graduated in 2009 with a degree in Information Systems and eBusiness, followed by an MBA.  Then in 2011 accidentally became a DBA for a direct marketing company in Akron.&lt;/p>
&lt;p>I have since moved employers but still reside in Northeast Ohio as a SQL Server DBA.  My days of playing football are behind me, but I now spend more time than I probably should watching the English Premier League and worrying about my &lt;a class="link" href="https://fantasy.premierleague.com/a/team/991342" target="_blank" rel="noopener"
>fantasy (proper) football&lt;/a> team. I also enjoy watching US sports, hiking, Crossfiting and travelling with my partner, Kelcie.&lt;/p>
&lt;p>[gallery type=&amp;ldquo;rectangular&amp;rdquo; link=&amp;ldquo;none&amp;rdquo; size=&amp;ldquo;medium&amp;rdquo; columns=&amp;ldquo;2&amp;rdquo; ids=&amp;ldquo;52,53,54&amp;rdquo;]&lt;/p></description></item></channel></rss>