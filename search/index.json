[{"content":"","date":"2022-09-13T15:05:44Z","permalink":"https://jpomfret.github.io/p/examplepost/","title":"Examplepost"},{"content":"Active Directory groups are used all over our IT estates. They can be used to simplify managing SQL Server access (Discover SQL Server Permissions hidden via AD Group Membership) as well as for other applications. One of my favourite commands from the ActiveDirectory PowerShell module is Get-AdUser, specifically when used in the following snippet:\n1 Get-ADUser UserName -Properties MemberOf | Select-Object -ExpandProperty MemberOf This snippet will list all the groups the user is in. Super useful for troubleshooting permissions issues or if you’re onboarding a new employee and want to see what groups their peers are in. But what happens if your environment consists of multiple domains, and you have a query about a user in another domain?\nWell good news - here’s the answer!\nFirst we need to know a little about the other domain, specifically the name of a domain controller in that domain. We can find that out by running the following in a console:\n1 2 3 4 5 6 7 PS\u0026gt; nltest /dclist:otherdomain.com Get list of DCs in domain \u0026#39;otherdomain.com\u0026#39; from \u0026#39;\\\\\\\\DC1.otherdomain.com\u0026#39;. DC1.otherdomain.com \\[PDC\\] \\[DS\\] Site: London DC2.otherdomain.com \\[DS\\] Site: London DC3.otherdomain.com \\[DS\\] Site: London The command completed successfully The results are shown above, and in this example you can see there are three domain controllers. We can pick any for the next step. Once we have the domain controller we just need to add the -Server parameter to our original snippet:\n1 2 Get-ADUser AnotherUserName -Server DC1.otherdomain.com -Properties MemberOf | Select-Object -ExpandProperty MemberOf This will now return all the groups that AnotherUserName@otherdomain.com is in.\nFor full transparency, I used this tip a lot at a previous job but today when I needed it I couldn’t remember how to get the name of the AD controller. So this blog is for future Jess, because let’s be honest, I’ll be back here soon.\n","date":"2022-07-07T00:00:00Z","image":"https://jpomfret.github.io/cover.jpg","permalink":"https://jpomfret.github.io/p/run-activedirectory-powershell-commands-against-another-domain/","title":"Run ActiveDirectory PowerShell commands against another domain"},{"content":"Log shipping is a SQL Server feature used for disaster-recovery where the transaction log backups are ‘shipped’ from your production instance to a secondary instance. This enables you to cutover to this secondary server in the event of a disaster where you lose your primary instance. Log shipping is not a new feature but is still quite popular.\nRecently I was tasked with setting up Log Shipping for a reasonably large production database. I didn’t want to initialize the secondary database with a new full backup as I was already taking full and log backups of the database. In this case we have the option of initialising the database by restoring the full \u0026amp; log backups up to the latest point in time and then configuring log shipping.\nWhich backups to restore? In order for us to stage the database on the secondary at the point in time where we can configure log shipping we need to get the last full backup and any log backups taken since then. If we were using differential backups as part of our strategy we would need the last full, the latest differential, and any log backups since then.\nThis could be a lot of backup files to find, put in the right order, and then restore (with no recovery) onto the secondary server. dbatools makes this so easy! We can use Get-DbaDbBackupHistory with the -Last switch to get the latest backup chain. Then by piping that to Restore-DbaDatabase we can automatically restore each piece of the puzzle. First, the full backups and then any differentials or log backups we need to get us to the point in time we are now.\nGet-DbaDbBackupHistory -SqlInstance mssql1 -Database productionDb -Last | Restore-DbaDatabase -SqlInstance mssql2 -NoRecovery -UseDestinationDefaultDirectories\nDepending on how long the restores take you might have new log backups to apply to the secondary database, like those that have been taken on the primary since we ran the last command. Again, we can use dbatools to help us with this. I will execute the same call to Get-DbaDbBackupHistory to get the last backup chain, but instead of piping it straight to Restore-DbaDatabase I will use Out-GridView with the -PassThru switch to effectively create a GUI window where I can select the backups I want to restore (any since the last log backup we applied), and then pass them on down the pipeline to be restored by Restore-DbaDatabase.\n1 2 3 Get-DbaDbBackupHistory -SqlInstance mssql1 -Database productionDb -Last | Out-GridView -PassThru | Restore-DbaDatabase -SqlInstance mssql2 -NoRecovery -UseDestinationDefaultDirectories -Continue Check on what we restored Once the restores are complete we can view what was restored using Get-DbaDbRestoreHistory.\n1 Get-DbaDbRestoreHistory -SqlInstance mssql2 -Database productionDb -Last What’s next? At this point our secondary database has been initalised and we’re ready to set up log shipping. You can use the GUI in SSMS for this, or I’d recommend taking a look at dbatools offering Invoke-DbaDbLogShipping.\n","date":"2022-05-24T00:00:00Z","image":"https://jpomfret.github.io/cover.jpg","permalink":"https://jpomfret.github.io/p/log-ship-staged/","title":"Log Shipping – Pre-stage database backups with dbatools"},{"content":"I found myself needing to clear out a large amount of data from a table this week as part of a clean up job. In order to avoid the transaction log catching fire from a long running, massive delete, I wrote the following T-SQL to chunk through the rows that needed to be deleted in batches. The question is though, what’s the optimal batch size?\nIt’s worth noting that an index on the date column was required for this to be as efficient as possible. The table I was deleting from had 4 billion rows and I didn’t want to scan that each time!\nDECLARE @Before DATE = DATEADD(DAY,-30,GETDATE()), @BatchSize INT = 50000\nWHILE (1=1) BEGIN\nDELETE TOP (@BatchSize) t FROM dbo.bigTable t WHERE Date \u0026lt; @Before -- if we deleted less than a full batch we're done IF @@rowcount \u0026lt; @BatchSize BREAK; -- add a delay between batches WAITFOR DELAY '00:00:01' END\nTime for a science experiment. The easiest way to determine the optimal batch size is to run some tests. I decided to test deleting in batches of 10k, 25k, 50k and 100k and measure the delete durations with extended events.\nMy goal was two part - to delete as many rows as possible in a two-hour maintenance window, but also to reduce the amount of time locks were held on the target table.\nThe following T-SQL creates an extended events session that captures one event, sqlserver.sql_statement_completed, and filters on both my username and the statement like ‘delete top%’. I didn’t choose a target as I just chose to watch the live data, but you could easily add an event_file if you want to persist the data.\nCREATE EVENT SESSION [DeleteExperiment] ON SERVER ADD EVENT sqlserver.sql_statement_completed(SET collect_statement=(1) ACTION(sqlserver.nt_username) WHERE ([sqlserver].[equal_i_sql_unicode_string]([sqlserver].[session_nt_user],N\u0026rsquo;JessUserName\u0026rsquo;) AND [sqlserver].[like_i_sql_unicode_string]([statement],N\u0026rsquo;delete top%\u0026rsquo;) ) ) GO\nOnce the extended events session had been successfully created and was running, I opened the ‘Watch Live Data’ pane and started running my deletes in another window. I left each experiment running for a while to make sure I got a decent sample size for each batch size.\nOnce I’d cycled through the different batch sizes, I used the grouping and aggregation features in the Extended events wizard, shown on the toolbar below:\nI grouped by ‘last_row_count’ which is my batch size, and then calculated the average duration for each group. You can see in the screenshot below the values for each.\nThe duration unit is microseconds for the sp_statement_completed event so after some squinting and calculations the results are as follows:\nBatch Size Average Duration (Seconds) 10,000 0.46 25,000 1.74 50,000 3.31 100,000 7.57 For now, I’ve decided to go with batches of 50,000 for the deletes. Depending on how things go I might change this to 25,000 as in my mind both those batch sizes met my criteria.\nHope this blog post has given you some ideas for testing out different scenarios with Extended Events.\n","date":"2021-12-07T00:00:00Z","permalink":"https://jpomfret.github.io/p/using-extended-events-to-determine-the-best-batch-size-for-deletes/","title":"Using Extended Events to determine the best batch size for deletes"},{"content":"One of the benefits available to us when using SQL Server Availability Groups is that we can offload read activity to a secondary replica. This can be useful if we need to run reports against our OLTP databases. Instead of this taking up valuable resources on the primary instance we can make use of the otherwise idle secondary replica.\nNote: This could affect your licensing standpoint, so ensure you’re in compliance on that front.\nLast week, I was working on a project to analyse indexes on a database that was part of an availability group. The main goal was to find unused indexes that could be removed, but I was also interested in gaining an overall understanding of how the system was indexed.\nUnused indexes not only take up disk space, but they also add overhead to write operations and require maintenance which can add additional load on your system. We can also use this analysis to look for a high number of lookups which could indicate we need to adjust indexes slightly.\nNote: dbatools does have a command called Find-DbaDbUnusedIndex to just look for unused indexes – however since I wanted to collect overall usage as well it wasn’t appropriate in this situation.\ndbatools has a command Get-DbaHelpIndex which returns detailed information on our indexes which we can then use to complete the necessary analysis. To run this against a single database we could use the following code:\nGet-DbaHelpIndex -SqlInstsance mssql1 -Database AdventureWorks | Out-GridView\nIn the above example I’ve used Out-GridView to popup the results in a nice, easy to view GUI. I love using this output option to get a feel for the results. You can also filter and sort to help do some initial analysis to help get an understanding of your data.\nThis is perfect – except I mentioned this database was in an AG. Oh, and it is set up to take advantage of using that read-only replica to run reporting against. That means the whole picture of the index usage is spread across two instances. We might find a totally unused index on our primary replica, a great candidate to be dropped, unless it’s heavily used by reports on the secondary.\nRemember, the secondary replica is just a read-only copy – so the indexes needed on the secondary must be created on the primary.\nIn this situation we need to combine the index stats for both replicas into one easy to use result set – for this we can make use of PowerShell’s PSCustomObject to join these two result sets. In the code below I’ve set up a few variables at the top, and then run Get-DbaHelpIndex against both instances. We then set up a variable to catch the results in $export and use foreach-object to loop through the results for the primary instance. As we loop through, we’re looking for the matching index on the secondary replica before adding properties from both sides to the PSCustomObject.\nFinally, we lean on the ImportExcel module to export the results to an excel spreadsheet – if you haven’t checked this module out yet I highly recommend it.\nhttps://gist.github.com/jpomfret/a9afa22c4d1129fecc4ea3e6cde1b51c\nLooking at our results spreadsheet we can now easily review the index usage across both replicas and make sure that any indexes we identify as unused, truly are unused.\n","date":"2021-11-16T00:00:00Z","permalink":"https://jpomfret.github.io/p/collating-index-usage-stats-across-availability-group-replicas/","title":"Collating index usage stats across Availability Group replicas"},{"content":"\nWell folks, it’s Wednesday here in the UK, which means I’m a day late to get my blog post in for T-SQL Tuesday. However, if I was in Hawaii it would be still Tuesday so let\u0026rsquo;s go for it\u0026hellip;\nI used a handy short script this morning and I figured it was worth a quick, late entry! Hopefully John Mccormack (b|t), will forgive me for stretching the deadline!\nFirst of all, shout out to John for hosting the monthly blog party, he has got a great prompt and I’m really excited to see the wrap-up post as I’m sure it’ll be full of great little code snippets.\nT-SQL Tuesday this month is going back to basics and its all about code. I’d like to know “What are your go to handy short scripts”?\nThis morning I was working on pulling together some information which included whether certain accounts were in the local administrator’s group on some remote servers. I had the perfect snippet saved in my code repo so I was quickly able to answer that question – and then I realised I should share that with you all.\nThe following PowerShell snippet uses the net localgroup command line tool to retrieve the results and parse them so we just get the account names. The final line includes the -ComputerName parameter so you can easily run it against remote machines.\nInvoke-Command -ScriptBlock { net localgroup administrators | Where-Object { $_ -AND $_ -notmatch \u0026ldquo;command completed successfully\u0026rdquo; } | Select -skip 4 } -ComputerName mssql1\nHope this comes in handy, and sorry again John for sneaking in late.\n","date":"2021-10-13T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#143-short-code-examples/","title":"T-SQL Tuesday #143: Short code examples"},{"content":"\nIt’s T-SQL Tuesday time and I’ve finally managed to put some words into a blog post again. It’s been a few months since I’ve posted anything on my blog, things have been busy, and my motivation has been a little lacking. However, this topic has inspired me to try and pull something together. Thanks to TJay(b|t) for hosting this month and for choosing a great topic.\nTJay has asked us to share our personal journey with managing work/life balance. It’s an interesting prompt, especially at this point in a global pandemic. The last 18 months have caused a massive shift in my work/life balance, and at this point I’d have to say overall it’s been a positive change.\nLike many of us, in March 2020 I was told to work from home full time for who knows how long. Previously I had been working from home one day a week and commuting into the office the other four. My commute wasn’t particularly long, but it did take about 45 mins each way for me to walk to the station and take the train into Southampton where my office is located. Gaining that commute time back has given me a lot more flexibility as far as my working hours – which is in turn where I’ve been able to improve my work/life balance.\nI’m lucky I work at a company, and on a team, that has transitioned smoothly into the work from home lifestyle. The ability to work more flexibly is the biggest benefit that I see when working from home and it does allow for a more manageable work/life balance in my opinion.\nI’ve talked about my love of CrossFit before, and it’s something I missed a lot while gyms were closed during the lockdowns over the last year. Now that gyms are back open again I’ve got back into the habit of going 5-6 days per week. Pre-working from home I would go to the gym in the evenings, checking in for either the 5pm or 530pm classes. Afterwards there was time for dinner, a little TV and not much else in the evenings.\nNow that I work from home, I’ve been able to change my schedule to go to the lunch time class. It’s created a great break both mentally and physically in the middle of my workdays, allowing me to get away from my desk and spend an hour at the gym with my wife and friends. It really breaks up the day and since I’m not commuting, I don’t end my day any later than I used to.\nI will say that it was a struggle at first to end my workdays on time. I found myself slipping into working later than I would if I had to leave the office to commute home. This was especially hard when we were in lockdown and there was nothing really to do. Luckily my wife tended to keep me on track here. We’d go out and walk together after work which meant I couldn’t just sit at my desk all evening. That helped a lot, and I feel like now I’ve got into a pretty good routine of ending my days at a reasonable time.\nAlthough 2020/2021 has been a hard year I think overall at this point the changes have had a positive impact on my work/life balance. I’ll leave you with my keys to success:\nA little fitness – I always feel better when I exercise, even if it’s just a walk and some fresh air so I ensure I make time for this. A little support – I’m lucky that my wife always keeps me on track, but having someone in your corner be it friends or family, can make a huge difference. Thanks again to TJay for hosting, and I look forward to reading everyone else\u0026rsquo;s responses.\n","date":"2021-08-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#141-work/life-balance/","title":"T-SQL Tuesday #141: Work/Life balance"},{"content":"Well folks, after starting the year off on a strong foot it’s been a while since I’ve published any blog posts. Hope you didn’t miss me too much, but I’m back now and I’ve got a useful dbatools snippet for you today. Last week at the day job I had a situation where I needed to find all stored procedures that referenced a certain string, across any database on a specific server. This is a pretty trivial task in SSMS when you’re just talking about one database. For example, if we’re looking for any reference to ‘Person’ perhaps we could run this T-SQL within the context of the database:\nselect o.name, sc.text, o.type from sys.objects o inner join sys.syscomments sc on sc.id = o.object_id where text like \u0026lsquo;%Person%\u0026rsquo; and o.type = \u0026lsquo;P\u0026rsquo; \u0026ndash; filtered for just stored procedures\nYou can see I’ve found one procedure in my TestDb that references the ‘Person’ table, so it has been returned.\nHowever, if I want to search all databases on the server I now need to start thinking about a cursor, or using something like sp_MSforeachdb to iterate over the databases. A quick warning here- sp_MSforeachdb is an undocumented procedure and there are some known issues with it.\nThe natural next step here when we’re thinking about handling multiple databases is to switch to PowerShell and use dbatools.\nFind the dbatools command for the job When we’re looking for the command we need within dbatools to fulfil our needs I cannot recommend Find-DbaCommand highly enough. This command will search all other commands for the pattern you pass in. Today we know we want to find references in stored procedures so let’s see if there is a command that will help.\nFind-DbaCommand *stored*proc*\nLooks like Get-DbaDbStoredProcedure is what we need here. Since this is our first time using this particular command I always have a quick look through the help content. I highly recommend running Get-Help Get-DbaDbStoredProcedure -ShowWindow, this will open a separate window from your console and allow you to keep that open to refer back to if needed. The last section of the help gives us several examples on how to use this command- let’s run a simple one against a test database to see what we get. I’m also going to pipe the output to Select-Object so I can just sample the first 2 results.\nGet-DbaDbStoredProcedure -SqlInstance \u0026rsquo;localhost,2500\u0026rsquo; -Database testDb | Select-Object -First 2\nThis is handy, but this output doesn’t look like it’s going to help answer the question and find references of a string within the stored procedure code. There’s more than meets the eyes though.\nSQL Server Management Objects (SMO) dbatools deals mostly with SQL Server Management Objects (SMO), which means that what you see in the output for commands is not always all there is available. SMO is a hierarchy of objects which can be easily traversed from the output of the commands. You can tell that we’re dealing with SMO instead of standard PowerShell objects by using Get-Member and looking at the TypeName.\nGet-DbaDbStoredProcedure -SqlInstance \u0026rsquo;localhost,2500\u0026rsquo; -Database testDb | Get-Member\nGet-Member is also really useful for looking to see what’s available from the object that is returned. In the screenshot above you can see multiple methods that can be used. If you run this in your console you’ll also get a list of all available properties. That’s a hint for what we need for our scenario.\nFind our string within all Stored Procedures in any database Now that we know the Get-DbaDbStoredProcedure command is going to return SMO StoredProcedure objects we can look at some of the properties not returned by default. We already saw one option for this- using Get-Member will list all the properties available to us. Another option is to select all the properties for the first result.\nGet-DbaDbStoredProcedure -SqlInstance \u0026rsquo;localhost,2500\u0026rsquo; -Database testDb | Select-Object -First 1 *\nIn the output you can see there are a lot of properties available that weren’t returned by default, this includes TextBody which is what we need to search for our reference string. All we need to do now is pipe the output of the command through to Where-Object to find what we need:\nGet-DbaDbStoredProcedure -SqlInstance \u0026rsquo;localhost,2500\u0026rsquo; -ExcludeSystemSp | Where-Object TextBody -like \u0026lsquo;*Person*\u0026rsquo;\nYou’ll notice two more changes to the code above. I dropped the Database parameter, opening the search up to the whole server. I also added ExcludeSystemSp, which means I’m only interested in user defined stored procedures. It is important to note if you have a lot of stored procedures this command could take a little while to run.\nPowerShell also supports other comparison operators by default, including `match` which can be used to find regex patterns within your procedures. This opens up a lot more possibilities when looking for more complicated patterns within your database.\nSo many more options… Today we were only looking for results on one SQL Server instance, but since dbatools makes handling multiple SQL instances easy we could also widen the search and instead search our entire estate for references to a certain string.\nWe also only looked at Stored Procedures today, but if you do a little research with Find-DbaCommand, Get-Help and Get-Member you’ll soon find what you need to search through functions, views and more.\nHappy searching!\n","date":"2021-05-25T00:00:00Z","permalink":"https://jpomfret.github.io/p/searching-stored-procedures-for-a-pattern-made-easy-with-dbatools/","title":"Searching Stored Procedures for a pattern made easy with dbatools"},{"content":"This is now the third post in a series on Azure tags. You can read the other two posts here to get up to speed with where we’ve been, however that isn’t required for this post .\nKeeping track of Azure resources with tags – Part 1 Keeping track of Azure resources with tags – Part 2 In part one I discussed how useful Azure tags can be, and specifically about how adding a ‘dateCreated’ tag can help you keep track of your resources, and how to find resources with certain tags using PowerShell. Part 2 and 3 are based around the fact that adding the ‘dateCreated’ tag is a great idea, but relying on a human to remember to add it is less than ideal. In part 2 we looked at using Azure Policy to automatically add the tag. Today’s post will cover another option using Azure Functions.\nAzure Functions gives us a way of running serverless code, written in a number of different languages, triggered by specific events or timings. Looking through the documentation there are many use cases from processing files to analysing IoT workstreams. Our use case is to run a PowerShell script that tags any resources that are missing the ‘dateCreated’.\nStep 1 - Create a function Azure Functions live within a function app, so the first thing we have to do is create this logical container. At this level we’ll decide on a ‘Function App name’, I’ve called mine ‘resourceTagJp’, and choosing ‘Code’ for the publish option we can then choose PowerShell as our language of choice. There are some other options for selecting a storage account and configuring ‘Application Insights’, but for now I’ve left those all as the defaults.\nOnce the ‘Function App’ is created we are ready to create our function. On the left hand pane choose ‘Functions’ and then ‘Add’. This will open a pane for you to choose how to develop the function, either in the portal or on your local machine in VSCode, for example, and the template to base your function off of.\nOne of the simplest options is to choose ‘Timer Trigger’, which as expected will execute the function code based on a schedule. The schedule is set using a cron expression. For it to run once an hour at the top of the hour we’ll use the following:\n0 0 * * * *\nOnce the function is created we’ll choose ‘Code + Test’ on the left hand pane of the portal to actually add the function code. The code for my function is going to be pretty simple, but if you are writing more complicated functions there is a VSCode extension that can be used to develop and test functions locally before publishing them to Azure.\nWe have three files within our function:\nReadme.md – for documentation in markdown Function.json – the config file, currently contains our timer binding information Run.ps1 – the main function code, in PowerShell as that’s what we chose The code for the function is below and makes use of the Update-AzTag cmdlet to add the ‘dateCreated’ tag. In this example, since I’m using PowerShell, I can easily format the date to be exactly how I want it to be displayed. If you read part 2 in this series, that was a downfall of using Azure Policy, I only had one datetime format option. The Update-AzTag also has a -Merge parameter which ensures any tags already on the resource aren’t overwritten by this function.\n# Input bindings are passed in via param block. param($Timer)\nSelect the subscription $null = Select-AzSubscription -SubscriptionId (Get-AzSubscription -SubscriptionName \u0026lsquo;MSDN Platforms\u0026rsquo;)\ntag any resources that are missing tags $res = Get-AzResource -ResourceGroupName functionTest | Where { $_.tags.keys -notcontains \u0026lsquo;dateCreated\u0026rsquo; } $res.Foreach{ Update-AzTag -ResourceId $psitem.ResourceId -Tag @{\u0026lsquo;dateCreated\u0026rsquo; = (Get-Date -Format \u0026ldquo;yyyy-MM-dd\u0026rdquo;)} -Operation Merge }\nThat’s all it is to create our function – however, it doesn’t currently have the authorisation to view or update resources.\nStep 2 - Add a managed identity To provision access to allow our function to work we can make use of Azure Managed Identities. These are similar to Managed Service Accounts in that there is no need to set or rotate passwords. This means we can configure our function with a managed identity and then forget about it- we know it’ll remain secure and the password/secret will be rotated often.\nThere are two options for managed identities: system-assigned or user-assigned. The system-assigned identity is tied directly to your application – if we delete the function the identity will also be deleted, and that’s fine for this purpose.\nConfiguring the system-assigned identity is pretty straightforward. On our function pane under identity, change the status to ‘On’. After a couple of minutes the identity will be deployed and you’ll see the option to assign ‘Azure role assignments’.\nClicking on ‘Azure role assignments’ will open a pane where you can assign whatever permissions your function will need to run. This can be scoped at the subscription or resource group level. For this example my function is just tagging anything within the ‘functionTest’ resource group so I can set the permissions to that scope. I have chosen the ‘contributor’ role as that gives us enough permissions to view and tag resources.\nStep 3 - Test our function We have created our function and set up the managed identity to enable the function to access our resources, now it’s time to make sure it’s working as expected. From the ‘Code + Test’ page there is a ‘Test/Run’ at the top that brings out the pane on the right. In that pane pressing run will simulate the time trigger being met and our function executing.\nIn the console you can see exactly what the function does and any output you’ve configured – in this example you can see a storage account was tagged.\nWe can also test this function by creating an untagged resource in the ‘functionTest’ resource group and waiting for the timer to be triggered.\nHowever we test our function we can see the tag is now on our storage account and we no longer have to rely on a human to remember the tag when they create resources.\nSummary This has been as much a learning experience for me as it hopefully has been for you. My journey into Azure is still pretty new but I’m enjoying the adventure.\nTo wrap this up, having a tagging strategy is important and there are multiple ways to ensure that tagging strategy is followed. Both Azure Policy and Azure Functions give us a good option for automatically tagging resources that are missing tags. If you’re using Terraform to deploy Azure resources John Martin has written about adding the a tag for date created to all resources as they are deployed, which is definitely worth a read.\n","date":"2021-03-31T00:00:00Z","permalink":"https://jpomfret.github.io/p/keeping-track-of-azure-resources-with-tags-part-3/","title":"Keeping track of Azure resources with tags – Part 3"},{"content":"Last week, in Part 1, we talked about how to easily keep track of our resources with tags. There are many strategies for tagging your resources but I specifically focused on adding a ‘dateCreated’ tag so we could see when resources were created – since this isn’t available by default. During that post we identified the biggest issue we had was that we were relying on a human to remember to add the ‘dateCreated’ tag for every resource they created. I’ve got two ideas on how to fix that – today we’ll look at the first option, using Azure Policy.\nAzure Policy is a way of comparing your Azure estate to defined requirements. You can either use predefined definitions (of which there are many) or create your own specific rules. These definitions can be assigned to certain scopes (subscriptions, resource groups). Azure Policy then reports on whether you’re in the expected state and in some cases can alter resources to ensure you are.\nStep 1 – Define a policy In our example, all resources should have a ‘dateCreated’ tag, and if Azure Policy finds the tag is missing, it should add that tag with the current date.\nThere are a few steps to set up our policy for ensuring the ‘dateCreated’ tag exists on all resources. First we need to write a policy definition in JSON. Don’t panic yet though – there are a lot of examples in the Azure/azure-policy GitHub repo that we can start with. By browsing the repo you can find the ‘add-tag’ policy which is the perfect base for us to build off of. When viewing that GitHub page there is a ‘Deploy to Azure’ button- clicking that (presuming you’re logged into the portal) will take you straight to the ‘New Policy Definition’ wizard where we can modify our policy to meet our needs and save it. You’ll need to choose a ‘Definition Location’ (which is the subscription this policy should reside in), name your policy, and edit the description if needed. The GitHub template has already specified this will go in the ‘Tags’ category,but you can change that if you’re keeping custom policies in a new category. Then we get to the policy rule, it’s JSON time.\nSince we imported the sample from GitHub the JSON is almost exactly what we need. The first section defines the rules and the second section defines parameters. In this case we’re going to remove the parameter section and change the tag name and values expected to be static. This means that the policy will only ever be used for adding the specific ‘dateCreated’ tag. The reason for this is we’re going to add some logic to the tag value so it contains the current date. (It is possible that this can be achieved with parameters, but I couldn’t get it to work. Please let me know in the comments if you know differently). The JSON below is pretty simple. There are two main sections: the condition to be met and then the operation to carry out if the conditions are met. In the conditions section we’re looking to see if the dateCreated tag exists, if it doesn’t we’ll move onto the second section of the JSON. This defines what to do about it, and in this case it’s pretty simple, we’ll modify the target and add the dateCreated tag. The tag value is dynamic and uses a resource manager template function to get the current date. There are a few restrictions on using functions within policy definitions, one being that we can’t overload the uctNow function with a format parameter so we’ll only be able to get a date in ISO 8601 (yyyyMMddTHHmmssZ) format.\n{ \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;Indexed\u0026rdquo;, \u0026ldquo;policyRule\u0026rdquo;: { \u0026ldquo;if\u0026rdquo;: { \u0026ldquo;field\u0026rdquo;: \u0026ldquo;tags[\u0026lsquo;dateCreated\u0026rsquo;]\u0026rdquo;, \u0026ldquo;exists\u0026rdquo;: \u0026ldquo;false\u0026rdquo; }, \u0026ldquo;then\u0026rdquo;: { \u0026ldquo;effect\u0026rdquo;: \u0026ldquo;modify\u0026rdquo;, \u0026ldquo;details\u0026rdquo;: { \u0026ldquo;roleDefinitionIds\u0026rdquo;: [ \u0026ldquo;/providers/microsoft.authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c\u0026rdquo; ], \u0026ldquo;operations\u0026rdquo;: [ { \u0026ldquo;operation\u0026rdquo;: \u0026ldquo;add\u0026rdquo;, \u0026ldquo;field\u0026rdquo;: \u0026ldquo;tags[\u0026lsquo;dateCreated\u0026rsquo;]\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;[utcNow()]\u0026rdquo; } ] } } } }\nThe final decision to make for our new policy definition is the ‘Role definition’. Since our policy has remediation actions, the operation to add tags if needed, a managed identity will be created for the policy. The ‘role definition’ is the permissions that will be granted to that managed identity. The default is ‘Contributor’ which assigns ‘full access to manage all resources’. This can be changed based on what your policy needs access to.\nStep 2 – Assign the policy We’ve now defined our policy, but the second part of this process is to assign that policy a scope. This outlines what this policy applies to. The scope can be a subscription or resource group, and can be more finely tuned by excluding specific resources that you don’t want to apply the policy too. Then it’s as easy as selecting the policy we defined, setting an ‘Assignment name’, which could be a combination of policy name and assigned score, and adding an optional description.\nStep 3 – Test the policy The easiest way to test our policy is to create a resource without the ‘dateCreated’ tag and see what happens. We have scoped our policy assignment to the ‘policyTest’ subscription so I’ll run the following PowerShell to create a new storage account that’s missing my required tag.\nNew-AzStorageAccount -ResourceGroupName policyTest -AccountName mystorageaccountjp77 -Location uksouth -SkuName Standard_GRS\nYou can see there is no -Tags parameter specified, so this storage account was created without any tags. If we now run Get-AzStorageAccount we can see it has the \u0026lsquo;dateCreated\u0026rsquo; tag.\nGet-AzStorageAccount -ResourceGroupName policyTest | Select-Object StorageAccountName, Tags\nYou can also see the tag in the portal view of our storage account.\nFinally, if we check out the Azure Policy we can see that we are in compliance. All resources in the ‘policyTest’ subscription have the required ‘dateCreated’ tag. We can also see the specific resources that are in compliance, in our case just the storage account we created.\nSummary This is just one option for automatically assigning the ‘dateCreated’ tag on all new resources. In this case we scoped the policy to a specific resource group but have assigned it at the subscription level to cover all resources. Note – this is specifically to tag resources. If you want to also tag resource groups the policy definition will need to be altered slightly.\nOne downside of this method is I haven’t found a way to control the format of the date in the tag value. This isn’t a big concern but does lack a little flexibility, especially if we wanted to add the date in a different time zone.\nNext week we’ll look at adding the same functionality using Azure Functions to auto tag new resources.\n","date":"2021-03-23T00:00:00Z","permalink":"https://jpomfret.github.io/p/keeping-track-of-azure-resources-with-tags-part-2/","title":"Keeping track of Azure resources with tags – Part 2"},{"content":"I’ve been working on some Azure exams recently, and I personally learn best by fiddling with things. The Microsoft learn content is excellent, and I’d highly recommend that for any of the Azure exams I’ve taken so far. However, I also like to build things myself and experiment a little with all the available options.\nOne of the vital parts of this learning and experimenting needs to be cleaning up after myself. We all know the risks of leaving things running in Azure- it’s likely to drain your training budget pretty quickly. To be fair, this is also a good lesson for real world scenarios. Getting used to turning off or scaling down resources based on need is a good way to reduce your Azure spend.\nThis brings me to one morning last week. I logged in to the portal and got a pop up that my credit was down to under $5, which is not what I was expecting. I started looking around and wondering what I’d left running – it isn’t always easy to spot though.\nLuckily, John Martin (b|t) has instilled in me the importance of adding a tag for creation date on all resources, as it’s not tracked automatically. This means we can easily see what we last deployed and what we might have forgotten about.\nIn Azure, tags are just key value pairs that can be applied to resources and subscriptions to add metadata. You can use them to organise resources by environment, cost centre, business criticality, and anything else that might be important to your individual situation. There is a limit of 50 tags per resource. If you’re getting close to having 50 tags per resource you might need to rethink your tagging strategy to reduce the complexity.\nYou can view tags through the portal either through the dedicated ‘Tags’ pane, on each individual resource, or on the ‘Cost Management’ area. Here you can view your Azure spend broken down by tags, which can be very useful. You can also view tags using either the Azure CLI or PowerShell. I usually opt for PowerShell, so let’s have a look at how we can view resources with certain tags using the Az module. If you don’t already have the module installed you can run Install-Module az to get started. More details on prerequisites and options available can be found in the Install Azure PowerShell with PowerShellGet docs.\nFind resources with a certain tag I already mentioned I add a ‘dateCreated’ tag to all resources, but when I’m playing around in Azure and working through training courses I also add a ‘training’ tag and set the value to ‘true’. This is an easy way for me to find all the resources I’ve created while training and clean them up.\nWe can easily list these resources in PowerShell using the following one liner:\nGet-AzResource -TagName training -TagValue \u0026rsquo;true\u0026rsquo; | Select-Object Name, ResourceGroupName,ResourceType, Tags\nIf we’re ready to clean them up we can pipe the results from Get-AzResource straight into Remove-AzResource, ensuring we haven’t got anything left running and costing us credit.\nGet-AzResource -TagName training -TagValue \u0026rsquo;true\u0026rsquo; | Remove-AzResource\nResources created in the last x days Another useful snippet since we’re adding ‘dateCreated’ tags to all our resources is to get any resources that have been created in the last few days. Since I have formatted my dates as yyyy-MM-dd in my tags I can easily convert them into dates with PowerShell and then filter based on them.\nGet-AzResource -TagName dateCreated | Select-Object Name, ResourceType, @{l=\u0026lsquo;dateCreated\u0026rsquo;;e={get-date($_.Tags[\u0026lsquo;dateCreated\u0026rsquo;])}} | Where-Object dateCreated -gt (get-date).AddDays(-7)\nResources without a tag The final tag related snippets I have for today is to make sure all our resources have the \u0026lsquo;dateCreated\u0026rsquo; tag. Since currently I’m manually adding these tags there is a chance I forget or get lazy and some resources make it through without the tags.\nWe can find these tags by interrogating the key values of the tags:\nGet-AzResource | where {$_.tags.Keys -notcontains \u0026lsquo;dateCreated\u0026rsquo;} | Select-Object Name, ResourceType, Tags\nOnce we know which resources are missing tags we can easily update them using Update-AzTag, the operation parameter that controls what should happen if there are existing tags. Merge will ensure we don’t overwrite the current tags.\n$resources = Get-AzResource -ResourceGroupName missingtag | Where-Object {$_.tags.Keys -notcontains \u0026lsquo;dateCreated\u0026rsquo;}\nUpdate-AzTag -ResourceId $resources.ResourceId -Tag @{\u0026lsquo;dateCreated\u0026rsquo; = (Get-Date -Format \u0026ldquo;yyyy-MM-dd\u0026rdquo;)} -Operation Merge\nThe main problem with this whole idea is we are relying on whoever creates the resources to both remember to create the tag and put the values of the tag in a standard format. Next week we’ll look at a couple of ways to automate this process.\n","date":"2021-03-16T00:00:00Z","permalink":"https://jpomfret.github.io/p/keeping-track-of-azure-resources-with-tags-part-1/","title":"Keeping track of Azure resources with tags – Part 1"},{"content":"\nShout out to Brent Ozar (b|t) for hosting this month\u0026rsquo;s TSQL2sday. It’s time again for this monthly blog party and he wants to know all about our favourite or least favourite data types. To start with I was having a hard time thinking of a favourite data type. I know I have favourite words (merge and plethora, in case you’re wondering), but it seems a bit wrong to pick favourites here – I mean lots of them are great in their own right. Then it came to me- my favourite data type is the right one for the job at hand. Feels like I’m skirting the question a little here, but bear with me.\nLet’s talk about accuracy and precision, and how much of it you actually need. The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).\nFirst things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:\nLet’s talk about accuracy and precision, and how much of it you actually need. The rest of this post is going to focus on datetime datatypes, but these thoughts could easily apply elsewhere (for example tinyint vs int vs bigint).\nFirst things first, let’s review our options when it comes to storing dates in a SQL Server table. These are the 6 current datatype options for datetime data in SQL Server:\nDatatype Accuracy Storage Size Notes Date One day 3 bytes Doesn\u0026rsquo;t include time SmallDateTime One minute 4 bytes DateTime Rounded to increments of .000, .003, or .007 seconds 8 bytes DateTime2 100 nanoseconds 6 bytes for precision less than 3. 7 bytes for precision 3 or 4. All other precision require 8 bytes. Time 100 nanoseconds (1 millisecond in Informatica) 5 bytes Doesn\u0026rsquo;t include date DateTimeOffset 100 nanoseconds 10 bytes Includes time zone awareness This table was created from information on Microsoft Docs, and there is plenty more information if you’re interested: Data types (Transact-SQL) - SQL Server | Microsoft Docs.\nThe Setup For this example, let’s say I have a table that gets loaded by a daily batch job. The column we will discuss needs to store just the date, no time and no need for time zone awareness. This leaves us with 4 options to investigate.\nIn order to demonstrate the importance of choosing the right datatype I’m going to create 4 really simple tables. They will each have one column, with a default value of today\u0026rsquo;s date, and no time information. These tables are totally unrealistic, but it isolates the storage required for a single column and will allow us to focus in on the differences our decision here can cause in a small test case.\ncreate table dataTypeDate ( batchDate date default (convert(date,getdate())) )\ncreate table dataTypeSmallDateTime ( batchSmallDateTime smalldatetime default (convert(date,getdate())) )\ncreate table dataTypeDateTime ( batchDateTime datetime default (convert(date,getdate())) )\ncreate table dataTypeDateTime2 ( batchDateTime2 datetime2 default (convert(date,getdate())) )\nOnce we have four tables we can run the following to load our batch for the day. To simulate this we’re going to insert the default values, 500,000 times. You can see here I’m using the GO 500000 syntax to run each insert half a million times.\ninsert into dataTypeDate default values; GO 500000\ninsert into dataTypeSmallDateTime default values; GO 500000\ninsert into dataTypeDateTime default values; GO 500000\ninsert into dataTypeDateTime2 default values; GO 500000\nThe Comparison First, let’s take a look at the first row in each table so we can see the different accuracies of our columns:\nSELECT TOP 1 batchDate FROM dataTypeDate SELECT TOP 1 batchSmallDateTime FROM dataTypeSmallDateTime SELECT TOP 1 batchDateTime FROM dataTypeDateTime SELECT TOP 1 batchDateTime2 FROM dataTypeDateTime2\nWe have the exact same data in these columns, just a date, no time. However, because of the different datatypes we used when we defined our tables, you can clearly see the different accuracies that were available to us. The requirement for the datatype to be able to meet stricter accuracies is where the additional storage comes in. As you saw in the table above, storing just a date, which is all we need in this situation, will cost us 3 bytes. Any other datatype we choose will add unnecessary storage. While we’re talking bytes, and it doesn’t seem like a big deal for one row with one column, it adds up quickly when we’re talking millions of rows with multiple columns that are inappropriate typed.\nThe following query will display some information about our four tables, including the number of 8k pages that each table is using.\nSELECT schema_name(obj.SCHEMA_ID) as SchemaName, obj.name as TableName, ind.type_desc as IndexType, pas.row_count as NumberOfRows, pas.used_page_count as UsedPageCount, (pas.used_page_count * 8)/1024 as SizeUsedMB, par.data_compression_desc as DataCompression, (pas.reserved_page_count * 8)/1024 as SizeReservedMB FROM sys.objects obj INNER JOIN sys.indexes ind ON obj.object_id = ind.object_id INNER JOIN sys.partitions par ON par.index_id = ind.index_id AND par.object_id = obj.object_id INNER JOIN sys.dm_db_partition_stats pas ON pas.partition_id = par.partition_id WHERE obj.schema_id \u0026lt;\u0026gt; 4 ORDER BY UsedPageCount desc\nWe can see that there are 500,000 rows in each of our tables and even with only one column there is a sizable difference in the number of pages needed if we chose DateTime or DateTime2 over just the Date type. It’s about a 25% savings- multiply that out by multiple columns across multiple tables and we’re going to start seeing a pretty sizable difference in our storage needs.\nThis is still a pretty small dataset, but it does clearly show that there is a significant difference in the amount of storage needed if we choose an unnecessarily accurate datatype for our date values.\nStorage is cheap – why do I care? Storage might not be our biggest concern, although enterprise grade storage is not as cheap as the USB drives at the supermarket checkout, but there are several other reasons why this wasted space is a big deal. Here’s a few:\nBuffer Cache – When SQL Server needs to interact with our data it first reads it into memory. Wasted space on disk then becomes wasted space in memory. That means we can store less data in the buffer cache and will have to flush out pages more quickly than if they were optimised.\nBackup\\restore – The bigger your database the longer it’s going to take to perform backup and restore activities.\nTransaction log activity – The bigger the record in SQL Server the more space it’ll need when any operations are written to the transaction log. This means you’ll need more disk space for your transaction log and your t-log backups will be larger. More wasted space.\nSummary So in a truly diplomatic fashion, I rate all the datatypes equally. We’ve only looked at types specific to datetimes in this post, but each and every datatype is suitable for storing certain data. The most important point is that we make solid decisions on both the type of data we want to store and the accuracy/precision needed to store that data.\nThanks for reading, and thanks again to Brent for hosting.\n","date":"2021-03-09T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#136-blog-about-your-favorite-data-type-or-least-favorite/","title":"T-SQL Tuesday #136: Blog About Your Favorite Data Type (Or Least Favorite)"},{"content":"Another week and another useful dbatools snippet for you today. Last week at work I was given a folder of 1,500 scripts – each containing a create table statement. Can you imagine having to open each file in Management Studio to be able to execute it? Thank goodness we have PowerShell and dbatools on our side.\nThe code for this example is pretty short, but there are a couple of things to point out. First, I used Connect-DbaInstance to create a server object to use to run the queries. This means that we’re efficiently reusing the connection rather than opening a new one for each file we want to execute. Second, I’m using the foreach method which takes each script file returned from the Get-ChildItem call, and executes Invoke-DbaQuery. With this we can use the -File parameter to pass in the sql file and that’s really all we need. This will loop through each file running the sql scripts.\n$SqlInstance = \u0026lsquo;mssql1\u0026rsquo; $destinationDatabase = \u0026lsquo;AdventureWorks2021\u0026rsquo; $folderPath = \u0026lsquo;.\\output\\AdventureWorks2017\u0026rsquo;\nCreate a connection to the server that we will reuse - can use SqlCredential for alternative creds $sqlInst = Connect-DbaInstance -SqlInstance $SqlInstance\n(Get-ChildItem $folderPath).Foreach{ Invoke-DbaQuery -SqlInstance $sqlInst -Database $destinationDatabase -File $psitem.FullName }\nThat’s really all we need for this blog post, but in order to set this up for a demo I did use a few other dbatools commands. I’ve posted the script above, along with the setup scripts on my GitHub. This includes creating a new database, scripting out all the tables into individual script files, and ensuring all the schemas and other dependencies were ready in the new database.\nThanks for reading, and hope this is a useful snippet. It sure saved me a lot of time this week.\n","date":"2021-03-01T00:00:00Z","permalink":"https://jpomfret.github.io/p/quickly-execute-a-folder-of-sql-scripts-against-a-sql-server/","title":"Quickly Execute a Folder of SQL Scripts against a SQL Server"},{"content":"I was working in my lab environment this weekend, playing with some SQL Servers that I had built with PowerShell DSC a while ago. I had installed SQL Server with mostly defaults, including not changing the engine and agent service accounts. For the blog post I thought I was going to write next, I wanted to change these to be active directory accounts – it did not go smoothly, and I figured this might be useful to document for future Jess, or anyone else who might stumble across this problem.\nCreate the Issue: Change SQL Server Service Accounts First off, I created two new Active Directory users that I’ll use for my service accounts. The below code will create a prompt for each account for the password to be entered.\n$engSvcAccount = \u0026lsquo;svc-dscsvr1-eng2\u0026rsquo; $agSvcAccount = \u0026lsquo;svc-dscsvr1-ag2\u0026rsquo;\n$EngSvcAccount = @{ Name = $engSvcAccount UserPrincipalName = $engSvcAccount AccountPassword = (Get-Credential -Credential EnterPassword).Password PasswordNeverExpires = $true Enabled = $true } New-AdUser @EngSvcAccount\n$AgentSvcAccount = @{ Name = $agSvcAccount UserPrincipalName = $agSvcAccount AccountPassword = (Get-Credential -Credential EnterPassword).Password PasswordNeverExpires = $true Enabled = $true } New-AdUser @AgentSvcAccount\nWe can view the current SQL services with Get-DbaService. This is useful to see what account they are currently running under, as well as the service names.\nGet-DbaService -ComputerName dscsvr1 | Format-Table\nThere is also a command for updating service accounts in dbatools. I will note, sometimes I have issues with the command being able to update the accounts and I’m not sure why. It worked perfectly in this scenario though running the following.\nThis again creates a prompt to enter the service account password, before setting the service \u0026lsquo;StartName\u0026rsquo;.\nUpdate-DbaServiceAccount -ComputerName dscsvr1 -ServiceName MSSQLSERVER -ServiceCredential (Get-Credential -Credential \u0026ldquo;Pomfret\\$engSvcAccount\u0026rdquo; ) Update-DbaServiceAccount -ComputerName dscsvr1 -ServiceName SQLSERVERAGENT -ServiceCredential (Get-Credential -Credential \u0026ldquo;Pomfret\\$agSvcAccount\u0026rdquo; )\nIf I rerun Get-DbaService I can see all looks good. StartName shows my new accounts and the services for both engine and agent are running.\nGet-DbaService -ComputerName dscsvr1 | Format-Table\nThe Issue At this point I was still planning on writing a blog post on a totally different topic. I ran the following to determine what databases I already had on dscsvr1:\nGet-DbaDatabase -SqlInstance dscsvr1 -ExcludeSystem | Format-Table\nBut instead of getting a quick answer to my question, I just got the following error:\nWARNING: [15:19:49][Get-DbaDatabase] Error occurred while establishing connection to dscsvr1 | The target principal name is incorrect. Cannot generate SSPI context.\nI checked a few things as I started troubleshooting:\nAre the services running – we already checked this with Get-DbaService and they are Was it firewall related – inbound rules were in place, and I was able to previously connect Was it certificate related – I’m not forcing encryption, and there are no certificates set up Was it Service Principal Name (SPN) related – bingo I have seen this happen before when changing service accounts for SQL services. I’m not an Active Directory expert, and I’m certainly not a Kerberos expert – in fact I’m as surprised as you that Kerberos has actually appeared on this blog. What I do know is that due to permissions, the SPNs needed were not able to be registered for the new service accounts. The easiest way to investigate SPN issues is with dbatools, saving us again!\nTest-DbaSpn works out exactly what SPNs are needed for our SQL instances and determines if they are in place.\nTest-DbaSpn -ComputerName dscsvr1 | Format-Table\nThis shows we should have two SPNs set for the default instance (MSSQLSERVER) on DscSvr1. The ‘IsSet’ column shows they aren’t set – and this is why we can’t connect to our instance remotely.\nLet\u0026rsquo;s Fix It The fix for this issue seems simple- register the required SPNs. dbatools again tries to make this as easy as possible for us. We can take the output from Test-DbaSpn and pipe it straight into Set-DbaSpn and dbatools will take care of the rest – won’t it?\nTest-DbaSpn -ComputerName dscsvr1 | Set-DbaSpn\nAs you can see from the warning message, dbatools wasn’t able to set our required SPNs either. It complains about \u0026lsquo;A constraint violation occurred\u0026rsquo;.\nThe reason is each SPN can only be registered once, and these SPNs were created for the previous service accounts and never cleaned up due to a lack of permissions.\nWe now need to use the setspn command line tool that is built into Windows and available when you have AD windows features installed, but the output from the dbatools command is still very useful for building the inputs for setspn.\nFirst we’ll try and register the required SPNs manually. For this we’ll use the -a parameter on setspn, the format being:\nsetspn -a \u0026laquo;SPN\u0026raquo; \u0026laquo;ServiceAccount\u0026raquo;\nSo we’ll run the following, getting the SPN from the ‘RequiredSPN’ column of the Test-DbaSpn output. You’ll notice there are two required SPNs, one without a port specified and one with 1433 – we’ll want to fix both.\nsetspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\\svc-dscsvr1-eng2\nYou can see in the output, the problem is highlighted – ‘Duplicate SPN found’. The useful part of this output is on the second line. I’ve highlighted the current owner of the SPN – we need this to be able to resolve the problem. Not surprising, it is the computer account since I was previously running SQL Server as the default NT SERVICE\\MSSQLSERVER account.\nNow we know what the duplicate is we can remove it, again using setspn, but this time with the -d parameter. The format is:\nsetspn -d \u0026laquo;SPN\u0026raquo; \u0026laquo;ServiceAccount\u0026raquo;\nWe’ll run the following two commands to clear up both old SPNs:\nsetspn -d MSSQLSvc/DscSvr1.pomfret.com DSCSVR1 setspn -d MSSQLSvc/DscSvr1.pomfret.com:1433 DSCSVR1\nFinally we can add the required SPNs. We can either use dbatools with the code we tried earlier or setspn.\nsetspn -a MSSQLSvc/DscSvr1.pomfret.com Pomfret\\svc-dscsvr1-eng2 setspn -a MSSQLSvc/DscSvr1.pomfret.com:1433 Pomfret\\svc-dscsvr1-eng2\nYou can see the output now states ‘Updated object’ which means we were successful. If we try and view the databases again now we should see the output we were expecting.\nSummary As I mentioned, this was not at all what I was expecting to write about – but I hope it’ll be useful if you ever find yourself in this situation while trying to change service accounts for SQL Server.\nI was in the end able to resolve the permissions problems for my service accounts by following this great blog post \u0026lsquo;SQL Server - Could not register the Service Principal Name\u0026rsquo;. Once I applied these permissions when I changed service accounts they were able to delete and recreate the required SPNs.\nAnother great blog post for more reading on SPNs is this post by Drew Furgiuele on how to use the dbatools SPN commands.\n","date":"2021-02-16T00:00:00Z","permalink":"https://jpomfret.github.io/p/troubleshooting-spn-troubles-cannot-generate-sspi-context/","title":"Troubleshooting SPN Troubles - Cannot generate SSPI context"},{"content":"\nIt’s time for February’s monthly blog party. This month is hosted by Mikey Bronowski (b|t) and he’s asking us to write about our ‘tools of the trade’. He’s looking for those tools that make our lives easier, ones we couldn’t imagine going without. Thanks for hosting Mikey, can’t wait to read everyone’s contributions and add some tools to my toolbelt.\nI’m going to split this into a couple of sections. I’m sure you can all guess what’s up first though…\nPowerShell If I could only choose one tool for my toolbelt it would be PowerShell, which is actually probably cheating because there are so many options to import modules and add functionality. I’m going to highlight five modules I use a lot below.\ndbatools – If you’ve read much of my blog before, or seen me present, it should be no surprise that dbatools is number one. I use dbatools every day, whether it’s to check diskspace, update database owners, or a plethora of other uses. In fact I previously wrote a post ‘The Multitool of my DBA toolbox’ that highlights five great use cases. dbachecks – A close friend of dbatools, dbachecks combines Pester and dbatools to create an amazing infrastructure testing module. This is perfect for creating a morning checks dashboard, or quickly checking certain parts of your estate. For example, in my post ‘dbachecks meets ImportExcel’ we check up on backups and database status before exporting to create an Excel report. Pester – Originally designed for unit/integration testing, I personally use this framework to test anything you can write in PowerShell. It quickly provides a clear and easy to read answer for whether everything is as expected. I’ve written about it previously to ‘Pester test your Cluster Role Owners’. ImportExcel – This module lets you work with Excel objects, without having Excel installed. Easily read data from spreadsheets into PowerShell, or export data to create detailed reports with a few lines of code. Our host for this T-SQL Tuesday has written a great series on this module, if you’re looking for inspiration. importexcel Archives - Mikey Bronowski - Blog PSFramework – Finally, I want to highlight PSFramework. Portions of this module are used within both dbatools and dbachecks. It provides great options for both setting configuration options that can be then used in your modules as well as for creating great logging. I’ve switched to using Write-PSFMessage instead of Write-Host\\Verbose\\Output as it provides a lot more flexibility as well as writing to a physical log file. I also recently wrote about PowerShell’s interactive search functionality, and after a poll on Twitter was pretty shocked by how few people knew about it. I recommend checking it out, as it is a really handy built in feature.\nhttps://twitter.com/jpomfret/status/1357014555638042624\nMicrosoft Excel Since I’ve written a lot about PowerShell previously, I wanted to highlight some other tools that I depend on. I’ve always been a fan of Excel, my personal life is full of spreadsheets – most decisions end with a spreadsheet (lucky for me, my wife is also a big fan of Excel!). I often find myself copying data into Excel to keep track of work, or to quickly analyse data. It’s also a great way of sharing data with a clear structure. I’m also a big fan of shortcuts – so here’s a few I use often.\nCtrl+; - Insert today’s date into the current cell – really useful, and avoids you having to remember we’re now in 2021! Ctrl+l – Select a cell within a dataset, press Ctrl+l (lowercase L), press enter. Your data is transformed into a table. Ctrl+D – Fill down, this will copy the contents of the cell above into your current cell. Also smart enough to handle continuation of formulas. Ctrl+R – Fill right, same as above but it’ll copy the contents of the cell to your left into your current cell. Ctrl+Up/Down arrow – This will move your cursor to either the first value in the current column, or the last. I use this a lot for navigating around worksheets/tables. F2 – This edits a cell\u0026rsquo;s contents. It puts your cursor at the end of the value, but you can now use your arrow keys to move about in the cell. It also stops you accidentally overwriting what was already in the cell. My Bike My final tool is my bike. Not technical at all, but a tool I use to keep fit and have some fun. I love cycling, and in the current times it’s my best option for fitness (I’m in England – we’re deep into lockdown 3 and gyms are closed). Honestly, I have a really hard time working out at home. I enjoy going to the gym, seeing some friendly faces and having someone tell me what to do for an hour. It’s not the same at home, and my mood is instantly affected by not being active.\nHowever, I’m happy to go out for a ride, and living in the South of England the weather is reasonably kind all year round. Previously, living in Ohio there weren’t many options for winter bike riding, unless you had fat tyres and loved the snow! I’m also lucky to be close to the South Downs (pictured below), as well as plenty of country lanes to explore.\nSummary Thanks for reading and hope you’ve enjoyed digging through my toolbox. Thanks again to Mikey for hosting. I always enjoy participating in these T-SQL Tuesday’s, partly because it gives me a prompt to write about, partly because it’s fun to see what everyone else wrote about.\nStay safe folks.\n","date":"2021-02-09T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#135-the-outstanding-tools-of-the-trade-that-make-your-job-awesome/","title":"T-SQL Tuesday #135: The outstanding tools of the trade that make your job awesome"},{"content":"I was listening to a podcast last week about PowerShell, when one of the hosts mentioned having to ‘up arrow’ back through your history to find a command you wanted to rerun. This made me realise that I should write this quick post on using PSReadLine’s interactive search function. This tip is a serious time saver and I rely on it heavily.\nThe great news is that if you are using Windows PowerShell on Windows 10 or if you’re using PowerShell 6+, PSReadLine is already installed and you can immediately start using this tip. If you don’t have the module though, it’s easy enough to install from the PowerShell Gallery: Install-Module PSReadLine\nThe main goal of this module is to enhance the command line experience for users. There is a lot of great stuff in this module, some of which you’re probably already using, without even realising it’s coming from PSReadLine. A fun experiment for this is to open a new console, run Remove-Module PSReadline and then see what’s missing. The two biggest things I notice (on top of the interactive search) is there’s no history available from previous sessions, and no colour coding to show the differences between variables, parameters and input.\nThe screenshot above is using Windows Terminal and PowerShell 7, but the same thing happens using older versions of PowerShell. Shout out to Chrissy LeMaire for the beautiful Windows Terminal theme.\nAs I mentioned earlier, when you’re running PowerShell in a console it tracks the commands you run, building up a history of all the things you’ve executed. This is really useful if you want to slightly change the command you just ran, perhaps fixing a typo, or piping the output to another command. The problem comes when you know you ran something recently, and you start ‘up arrowing’ furiously through the history trying to find what you’re looking for.\nYou can also view the history of your current session by executing history.\nPSReadLine makes looking for a specific command in the history haystack even easier with ‘Bash/zsh style interactive history search’. I have created a gif below to demonstrate this, but it’s as simple as pressing ‘Ctrl+R’ from the console. That will create a second line under your prompt where you can start typing your search terms.\nAs you type PSReadLine is going back through your history to find the most recent command that matches what you’ve typed so far. As you continue typing it hones in on the exact command you’re looking for. To look further back in your history, press ‘Ctrl+R’ again to find the next time you used that search term in a command.\nIf you press `Ctrl+R` one too many times, you can press `Ctrl+S` to search forward, basically taking you forward in time one search result.\nHope you find this as useful as I do. As I mentioned I rely heavily on interactive search to rerun commands I’ve used before.\n","date":"2021-02-02T00:00:00Z","permalink":"https://jpomfret.github.io/p/easily-search-powershell-command-history-with-psreadline/","title":"Easily Search PowerShell Command History With PSReadLine"},{"content":"When granting permissions to SQL Server resources we have a few options. One option is to grant permissions to Active Directory groups instead of individual users. This has several benefits, for example, improved security over using SQL logins, and the ability to create a separation of duties when controlling database access.\nHowever, it does add an extra step when trying to determine who has what access to your SQL Servers. It can also make troubleshooting permission issues more challenging. This post is going to aim to simplify this by combining dbatools and ActiveDirectory PowerShell modules to provide a clear solution.\nSetup This is obviously not needed in our actual environments, this is just how I prepared my lab so I could demonstrate how we can solve this problem. Feel free to skip ahead to the solution if you already have plenty of AD groups to investigate.\nThe full code sample is available in my GitHub demos repo.\nFirst, I created some AD users and groups to use in my lab. This is easily achieved with the AD PowerShell module using New-AdUser and New-AdGroup. Then, using Add-AdGroupMember, I added two users into the newly created group.\n# setup - create some AD users\\groups uing the ActiveDirectory module\ncreate several new ad users (\u0026lsquo;pomfretJ\u0026rsquo;,\u0026lsquo;smithA\u0026rsquo;, \u0026lsquo;jonesP\u0026rsquo;,\u0026lsquo;barnesR\u0026rsquo;).foreach{New-AdUser $_}\nview newly created users $date = (get-date).AddHours(-1) get-aduser -filter {created -gt $date} | select name\ncreate a new Ad group $newAdGroup = @{ Name = \u0026lsquo;AdventureWorksReadOnly\u0026rsquo; GroupCategory = \u0026lsquo;Security\u0026rsquo; GroupScope = \u0026lsquo;Global\u0026rsquo; Path = \u0026lsquo;CN=Users,DC=pomfret,DC=com\u0026rsquo; } New-ADGroup @newAdGroup\nadd users to group $addMemberGroup = @{ Identity = \u0026lsquo;AdventureWorksReadOnly\u0026rsquo; Members = \u0026lsquo;pomfretj\u0026rsquo;, \u0026lsquo;jonesP\u0026rsquo; } Add-ADGroupMember @addMemberGroup\nThe second part of the setup was to add an AD group and an AD user to the SQL Server and grant some permissions using dbatools.\n# setup - grant permissions to ad users\\groups using dbatools\nadd ad group and grant permissions (db_datareader to AdventureWorks) New-DbaLogin -SqlInstance dscsvr1 -Login \u0026lsquo;Pomfret\\AdventureWorksReadOnly\u0026rsquo; New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login \u0026lsquo;Pomfret\\AdventureWorksReadOnly\u0026rsquo; Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_datareader -User \u0026lsquo;Pomfret\\AdventureWorksReadOnly\u0026rsquo; -Confirm:$false\nadd ad user to sql server and provide permissions (db_owner to AdventureWorks) New-DbaLogin -SqlInstance dscsvr1 -Login \u0026lsquo;Pomfret\\smithA\u0026rsquo; New-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -Login \u0026lsquo;Pomfret\\smithA\u0026rsquo; Add-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 -Role db_owner -User \u0026lsquo;Pomfret\\smithA\u0026rsquo; -Confirm:$false\nViewing database access Now that my lab environment is set up, let’s take a look at database users that have access to the AdventureWorks2017 database. This is an easy task thanks to dbatools, we can just use Get-DbaDbUser. Shown below, you can clearly see there is a WindowsUser \u0026lsquo;smithA\u0026rsquo; that has access, as well as a WindowsGroup \u0026lsquo;AdventureWorksReadOnly\u0026rsquo;.\n# Find users that have permissions through group membership Get-DbaDbUser -SqlInstance dscsvr1 -Database AdventureWorks2017 -ExcludeSystemUser | Select-Object SqlInstance, Database, Login, LoginType, HasDbAccess\nWe can also use Get-DbaDbRoleMember to see exactly which database roles these users have been granted. Get-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 | Select-Object SqlInstance, Database, role, Login\nThe issue is the same for both these examples, we don’t know which users are inheriting the permissions granted to the \u0026lsquo;AdventureWorksReadOnly\u0026rsquo; group. This is where we need to combine these two modules to get the answers we need.\nThere are several ways you could combine the output of two functions. For this example I’m going to use a calculated property.\nIf I run the exact same code as before to get a list of role members from the dbatools function Get-DbaDbRoleMember, I can add a calculated property in the Select-Object to lookup the members of that specific group from active directory. In the example below you can see the \u0026lsquo;AdventureWorksReadOnly\u0026rsquo; group has two members, and we now know that both \u0026lsquo;pomfretJ\u0026rsquo; and \u0026lsquo;jonesP\u0026rsquo; have read access to the AdventureWorks2017 database. You can also still see the WindowsUser, \u0026lsquo;smithA\u0026rsquo;, has db_owner permissions. Since that lookup didn’t return any results (obviously, since it’s a user not a group), the GroupMembers property remains empty.\nGet-DbaDbRoleMember -SqlInstance dscsvr1 -Database AdventureWorks2017 | Select-Object SqlInstance, Database, Role, LoginType, Login, @{l=\u0026lsquo;GroupMembers\u0026rsquo;;e={ (Get-AdGroupMember -Identity ($_.Login).Split(\u0026rsquo;\\\u0026rsquo;)[1]).Name }}\nYou can also use this same code to determine specific user access, for example, by adding a Where-Object to see just the permissions granted to \u0026lsquo;pomfretJ\u0026rsquo;.\nSummary This should give you an easy option for determining specific user access that is hidden behind AD groups, and I think reduces one of the negatives of using AD groups in this situation. It also shows us that we can combine multiple functions into one to get all the information we need with one easy line of code.\nI would also encourage you to explore the other permission related dbatools functions available, including Get-DbaServerRole and Get-DbaPermission. These can also be used in combination with Get-AdGroupMember to enhance the results.\n","date":"2021-01-26T00:00:00Z","permalink":"https://jpomfret.github.io/p/discover-sql-server-permissions-hidden-via-ad-group-membership/","title":"Discover SQL Server Permissions hidden via AD Group Membership"},{"content":"Have you ever wanted to quickly backup/restore a database to the same instance to do some side by side testing? Perhaps to make some index changes or code changes, without actually changing the live copy of the database? Ideally you’d already have another environment for this sort of work, but even then sometimes it’s handy to have a quick option.\nLet’s first take a look at the databases on my SQL Server- we can use a GUI tool for that (SSMS, ADS) or we can use dbatools.\nGet-DbaDatabase -SqlInstance mssql1 | Select-Object SqlInstance, Name, Status, Size\nWe’re working hard on the AdventureWorks2017 database, perhaps getting it ready for an upgrade – since it’s now 3+ years out of date.\ndbatools has so many functions, and I know I’ve mentioned it before, but Find-DbaCommand is a great way of looking for what we need. I want to know what the default backup path is set to, and since I’m just backing up and restoring to the same server, we already know that the instance has the required permissions here. If only there was an easy button for this…\nFind-DbaCommand *default*path*backup*\nEven just reading the synopsis, I can see that Get-DbaDefaultPath will give me exactly what I need. I recommend the next step is running Get-Help Get-DbaDefaultPath -ShowWindow, that’ll create a popup that provides all the information you need about the function.\nThe only required parameter is a SqlInstance, and you can see the backup property returns gives us the path we need for our copy.\nGet-DbaDefaultPath -SqlInstance mssql1\nThat’s all the groundwork done- we have our instance, database, and a location to backup/restore from. We’re going to want to check we have enough disk space available on both the instance and that backup path, then we’re ready to go.\nUsing Copy-DbaDatabase I’ve already spoken and blogged a lot about the power of this command (related links at the end of this post), but today’s tip is centred around a less than well-known parameter. Hidden deep in the comment based help (another great reason to read all of Get-Help Copy-DbaDatabase -ShowWindow) you’ll find the ‘Prefix’ parameter. This will allow us to easily add a prefix to both the database and the associated files, meaning we won’t have any issues restoring the database to the same server.\n-Prefix All copied database names and physical files will be prefixed with this string\nThis option is mutually exclusive of NewName\nRequired? false Position? named Default value Accept pipeline input? False Accept wildcard characters? false\nHere I’ve set a SqlInstance variable so I can reuse the same value multiple times in my code. Then created a hash table ‘$copySplat’ with the necessary parameters so we can utilise splatting (a way to improve code readability) to pass the whole set into Copy-DbaDatabase. Two parameters I want to highlight- I’ve set ‘Prefix’, meaning the database and files for the restored database will start with ‘Test’. I’ve also set SharedPath and used the code we already wrote to get the default backup path.\n$sqlInstance = \u0026lsquo;mssql1\u0026rsquo;\n$copySplat = @{ Source = $sqlInstance Destination = $sqlInstance Database = \u0026lsquo;AdventureWorks2017\u0026rsquo; BackupRestore = $true SharedPath = (Get-DbaDefaultPath -SqlInstance $sqlInstance).Backup Prefix = \u0026lsquo;Test\u0026rsquo; } Copy-DbaDatabase @copySplat\nThe output below shows the migration was successful, and there were no warnings or errors (those would appear in the notes column).\nFinally, let’s confirm it worked by rerunning our Get-DbaDatabase command again:\nExtra proof, it’s now accessible through Azure Data Studio (ADS) and we’re ready to start our testing. One note, if you are on the same server it’s important to confirm any code you run isn’t referencing the original database name.\nAdditional content As I mentioned I have already spoken and written about the power of Copy-DbaDatabase, one of my favourite commands. If you’d like to read more, I’ve written a post on the dbatools blog, migrating application databases with dbatools.\nI’ve also recorded a short ‘Life hack’ video, easy database migrations with dbatools that I’ve published on my YouTube channel.\n","date":"2021-01-19T00:00:00Z","permalink":"https://jpomfret.github.io/p/easily-create-a-copy-of-your-database-for-testing/","title":"Easily Create A Copy Of Your Database For Testing"},{"content":"\nIt’s the first T-SQL Tuesday of 2021, and James Mcgillivray (b|t) is our host for January’s edition of this monthly blog party. This month’s topic is around how we escape with vacation/holiday plans. They gave us a few options on what to blog about and I’m going to take you on a virtual trip to Hawaii.\nWhat is the best vacation/holiday you’ve ever had? The best holiday I’ve ever had would have to be my honeymoon to Hawaii in 2018. We spent 10 days exploring two islands and it was fantastic. This is a great prompt as it has encouraged me to look back at my photos from that time and record some memories. I have a pretty horrible memory, but that trip I will never forget. I hope that as you follow along with this post it will take you on a virtual holiday, welcome to ‘Team Jelcie’s Hawaii Honeymoon’.\nFirst stop Maui We got married on Saturday 27th of October 2018 in Ohio, not far from where we were living. We were lucky to be surrounded by friends and family from both sides of the Atlantic, and apart from the constant rain all day, we had an amazing time. The following Monday we boarded a plane for Maui, Hawaii. It’s actually a longer flight time from Ohio to Maui than it is from Ohio to England. Until we started planning this trip I had no idea how far off the coast of the continental USA it actually is.\nOne of the benefits of this long flight time, jet lag had us waking up really early the first few days. We brewed some coffee in the room and took it down to the beach. We were treated to the most amazing sunrises. The longer we stood there the more impressive the colours.\nWe’d planned our honeymoon using a travel agent, so we pre-booked several excursions for our time away. The first being a sunset sail, which didn’t disappoint, since the sunsets were almost as beautiful as the sunrises.\nThe second excursion was one of the top memories of the trip, a snorkelling excursion to Molokini, a ‘crescent-shaped, partially submerged volcanic crater’. I have never seen such clear water- you could see the bottom of the ocean and it was 40m deep in parts. Also so many fish, every shape and size and colour. It was fantastic. We also hiked, dropped into a Crossfit gym, relaxed and ate plenty of delicious food, before it was time to board a flight to the next stop of our trip.\nSecond island – Kauai A short flight and we were on our second island. At the airport I surprised Kelc by renting a Jeep for us to drive, her dream car, and it made our options of exploring Kauai a lot easier. The first picture I have, apart from the Jeep, was this Poke bowl. We found this stand inside a grocery store, and it was probably the most delicious thing I’ve ever eaten – this was not the only time we ate this within our few days here.\nThis island felt totally different from Maui. Maui wasn’t that commercialised, but Kauai was even less so. We definitely felt like we were enjoying island life in all its glory.\nWe love to hike and we were not disappointed. One of the days we took the Jeep up to Waimea Canyon and hiked several miles into these hidden waterfalls. There was a lot of elevation gain and at some points there were helicopters flying in the canyon beneath us.\nWe were also lucky to see some Hawaiian Monk Seals. I love seals, so it was amazing to see them rock up on to the beach to rest in the sun for the day. They are protected so when they do beach themselves volunteers run out and cordon them off so they are protected for the day. When the sun sets they roll and wiggle down the beach and back into the ocean, prompting the volunteers to collect their signs and wait to see where they appear the next day. Our third excursion was an ATV trip through a historic sugar plantation. This was great- we got filthy and had a lot of fun touring around beautiful scenery that’s featured in many movies, including Jurassic Park, although we saw no dinosaurs.\nSummary Overall we both thoroughly enjoyed this holiday, a great mix of relaxation and adventure. I think we would both say that Maui was our favourite of the two islands, but Kauai offered a great second perspective of what Hawaii is all about.\nI really enjoyed looking back over my Hawaii trip, and I hope you enjoyed this virtual holiday in one of the most beautiful spots in the world.\nWhat’s next? If I was to write about my next dream holiday, it would be the Maldives. Back in June, when we had almost nothing to look forward to, we booked a 10 day holiday in the Maldives for January 16th, 2021. It felt like ages away, and surely things would be better by then. Well, if you’re reading this when it was posted, that’s next week, and it’s already been postponed.\nWe’re now a couple of weeks into our third lockdown in the UK and we’re hoping we’ll get to take the trip in April now, fingers crossed.\n","date":"2021-01-12T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#134-give-me-a-break/","title":"T-SQL Tuesday #134 - Give me a break"},{"content":"Well 2020 was certainly an interesting year. I’m pretty sure none of us could have predicted how the world would look now and it won’t be regarded as the best year ever. However, there were definitely still some bright points I’d like to make note of before setting some targets for 2021.\nLooking back on 2020 One of my biggest achievements of the year was being awarded MVP for Cloud and Datacenter Management in February. I’m really proud of this and my ability to positively contribute to our data and PowerShell communities. Along with that I managed to publish 25 blog posts and added two videos to my YouTube channel. I also gave 10 presentations, only one in person, and also sat on two diversity and inclusion panels. I’ve also continued to contribute to dbatools, dbachecks and other open source software.\nOutside of the tech world, it was a year of cancelled plans and lockdowns. I will say this gave me and my wife plenty of time to explore our local footpaths. I recorded over 460 miles of walking last year. Something we usually wouldn’t have had time for. I also cycled just over 700 miles after getting a new bike in June.\nLooking forward to 2021 Now, I’m not going to say 2021 is going to be back to normal, who knows if we’ll ever go back to that normal. However, I’m hopeful that things will be better than 2020, and I hope sooner rather than later we can get back to travelling and actually meeting up with people in person!\nI’m going to set some targets to aim for that won’t depend on what 2021 looks like.\nPublish 26 blog posts – this would average to one every fortnight. I also aim to publish at a regular cadence. Create 10 YouTube videos – my short ‘life hack’ videos that I’ve published have got positive reviews. I aim to get more of those recorded and available on YouTube Speaking – I don’t have a specific number target here, but I’d like to think I can beat last year\u0026rsquo;s total of 10 presentations. Perhaps even some at in person events?!? Open source contributions – Again, no specific numbers here but just a goal to continue contributing. I also have some ideas for this area that need a little love and time, but I hope to work on those this year as well. Cycle 1000 miles – this would be more than I’ve ever cycled in a year, but without a commute in 2020 I found a lot more time for cycling before/after work. Duolingo streak – another non tech goal. I’ve been using Duolingo to work on my French and hope to keep the streak alive throughout 2021. All in all, I hope 2021 is an improvement on 2020 – here’s to doing everything I can to make it a good one. I\u0026rsquo;ll leave you with a couple of local views I\u0026rsquo;ve enjoyed while cycling\\walking this past year.\nhttps://www.instagram.com/p/CDhAVEXBjLq/?utm_source=ig_web_copy_link\nhttps://www.instagram.com/p/CAP8ta7huch/?utm_source=ig_web_copy_link\n","date":"2021-01-05T00:00:00Z","permalink":"https://jpomfret.github.io/p/some-2021-goals-guaranteed-to-keep-me-on-track/","title":"Some 2021 Goals Guaranteed To Keep Me On Track"},{"content":"This was the 3rd year I participated in the Advent of Code (AoC). If you haven’t heard of AoC, it’s an advent calendar of coding puzzles. Each day between December 1st and 25th a two part puzzle is released, you can use whatever language you want to solve it, with the goal being just to get the right answer. Once you solve part 1 for the day, part 2 is unlocked and that builds on top of the story you had for part 1. For each part of each puzzle you complete you get a star, so there are two available per day.\nThis year I managed to complete both parts of the first 9 days of the calendar, and then just the first part of days 10 and 15, that’s 20 total stars out of a possible 50. That doesn’t sound great, less than 50%, so why am I writing a blog about this mediocre performance?\nMy goal was to gain more stars than last year, which I succeeded at. I only got 6 total stars last year. Now my goal for next year will be to beat this year\u0026rsquo;s performance. I did learn several neat things while working on these puzzles and those I thought were worth sharing.\nNamed Loops in PowerShell A lot of the puzzles involve iterating over an object and manipulating it. I depended on a lot of loops for this. My day 1, part 1 solution is below. You can see I nested two loops to iterate over the array and calculate the total. Without the named loops this worked – it just didn’t stop when it found the correct answer and I got duplicates. By naming the outer loop with :expenses you can then break all the way out of that loop with break expenses. Pretty useful!\n$expenses = Get-Content .\\Day01\\Input.txt\nPart 1 - 514579 :expenses foreach ($e in $expenses) { foreach ($f in $expenses) { if ([int]$e + [int]$f -eq 2020) { (\u0026ldquo;Part 1 answer: {0}\u0026rdquo; -f ([int]$e * [int]$f)) break expenses } } }\nSplit string into multiple variables at once A lot of the puzzles input required some string manipulation. Some of this I used regex for, and if it wasn’t that complicated I could use the split method. Previously I had always split strings into variables by first splitting into an array, and then specifying the index of the item for each variable:\n$split = (\u0026rsquo;test string\u0026rsquo;).split(\u0026rsquo; \u0026lsquo;) $firstVariable = $split[0] $secondVariable = $split[1]\nPowerShell has an easier option though- you can accomplish this same behaviour with just one line:\n$firstVariable, $secondVariable = (\u0026rsquo;test string\u0026rsquo;).split(\u0026rsquo; \u0026lsquo;)\nSplit a string into a maximum number of substrings Another string splitting tip is if you only want to split a certain number of times there is an overload for the string method for that. I have used the split method for years, but I have never looked any further into what it can do. A handy reminder that reading the docs for even simple methods/functions is a worthwhile endeavour (perhaps a 2021 goal?!?). (\u0026lsquo;I only want two substrings\u0026rsquo;).split(\u0026rsquo; \u0026lsquo;,2)\nThis results in:\nI only want two substrings.\nI’m not a Computer Scientist The final thing I learnt is that I’m not a computer scientist. The first few days of puzzles were pretty straightforward – I had no problem working out what was needed and writing a solution. Was it the most effective and beautiful code ever, probably not, but it got the right answer and that was all we needed. Once we got into the second week the difficulty picked up- I didn’t study maths or computer science and found I was severely lacking when it came to needing more complicated algorithms to solve the puzzles.\nThat’s ok though. Although it’s definitely a gap in knowledge when it comes to solving code puzzles, it hasn’t really caused problems or issues in my day to day work. I’m still able to use PowerShell to automate tasks and manage a large database estate.\nSaying that, I am interested in learning more about these topics. I love a puzzle and the Advent of Code is a great way to finish the year with some challenges and learning.\nIf you\u0026rsquo;re interested in my efforts, all of my code is on Github.\n","date":"2020-12-31T00:00:00Z","permalink":"https://jpomfret.github.io/p/advent-of-code-2020/","title":"Advent of Code 2020"},{"content":"\nIt’s December, the last T-SQL Tuesday for 2020. I’ve managed to participate in seven this year, including hosting in February – I wonder if in 2021 I will be able to complete a full year. I always look forward to these monthly blog parties, so thanks to Lisa for hosting this month. Lisa has asked us to share something we’ve learnt from presenting that didn’t relate directly to the topic we were presenting on. I think this is a great topic. We already know that to present on a topic you have to really know it, so preparing for a presentation does wonders for your own personal knowledge on that topic. Lisa has identified another bonus- all the ancillary knowledge that comes along with it.\nI gave my first presentation in June 2018, and although it’s only been around 2.5 years since then, I have learnt so much from both presenting and contributing to the community. I have a couple of areas I’m going to mention here.\nGit \u0026amp; GitHub My first real contributions were not presenting but writing code and tests for dbatools. Writing PowerShell was not something that was new to me, so I felt comfortable writing the function I wanted to add. Through code review it did get some much needed tweaking to ensure it met the standards and format of the project. Getting the code from my laptop into the dbatools GitHub repo was another story – it was totally foreign to me.\nLuckily the dbatools team is fantastic. They have written great guides on using git and GitHub in order to make your first pull request, and Chrissy even coached me through how to get my code into GitHub that first time. As I continued to contribute I got more familiar with using Git and GitHub for source control, and since then there have been many times where I’ve needed that knowledge both for community projects as well as my real job. I even host my presentation demos and slides there now.\nDocker I think one thing that every presenter has had to learn is how to set up some form of a lab to be able to run demos in a reliable and repeatable way. I started off with a VM on my local laptop that I could connect to and run demos against. This took some learning to get the networking setup. Something I still consider a weak area!\nAfter a while I started reading more about containers, and specifically running SQL Server in containers. I now (where possible) run all my demos for presentations against demo environments in containers that run on my laptop. I have already written a post on this, Data Compression Demos in Containers, but this has given me a great opportunity to learn and play with a really interesting technology.\nOne of the main benefits I see when running demos against containers is how easy it is to wipe away your practice runs and have a fresh environment ready for the presentation. I’ve even built pester tests into my setup script to ensure everything is in the perfect state for the demo gods. I’ve also written about that if you’re interested in what I test, Keeping the demo gods at bay with Pester.\nSummary It took me a long time to make the first step into giving back and presenting content in front of people, but if I look back at the fun I’ve had over the last 2.5 years and all the knowledge I’ve gained it’s easy to see what a great decision that was. If you are considering speaking I would highly recommend giving it a go – as Lisa mentioned in her prompt, Allen White’s famous speech, everyone has something to teach.\nThanks again for hosting Lisa, and I look forward to reading all the other responses.\n","date":"2020-12-08T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#133-what-else-have-you-learned-from-presenting/","title":"T-SQL Tuesday #133: What (Else) Have You Learned from Presenting?"},{"content":"Last week I gave a presentation at Data South West on dbachecks and dbatools. One of the questions I got was whether you could run dbachecks against Azure SQL Databases, to which I had no idea. I always try to be prepared for potential questions that might come up, but I had only been thinking about on-premises environments and hadn’t even considered the cloud. The benefit is this gives me a great topic for a blog post.\nStep 1 – Create an Azure SQL Database I created a SQL Database through the Azure Portal. The wizard is pretty straightforward and the only real decisions needed were around sizing. Since this is just going to be for a test environment I chose a small ‘Basic’ database.\nStep 2 – Connect to the Database Once the database had been created I navigated to the resource pane in the portal. At the top there is a drop down that helps you get connected using Azure Data Studio.\nOnce Azure Data Studio opened, I was asked to confirm I wanted to connect:\nThen a pane opened which enabled me to easily add a firewall rule so client IP could access the Azure SQL Database.\nOnce that was completed I was connected through Azure Data Studio and able to interact with my server and database.\nConnecting first through Azure Data Studio is not a requirement, but it does help us to get the firewall rules configured and confirm that connecting from our client machine will not be an issue.\nAnother good check to ensure we can connect to our database from PowerShell is to use dbatools’ Connect-DbaInstance:\n$cred = Get-Credential Connect-DbaInstance -SqlInstance \u0026lsquo;xxxxxx.database.windows.net\u0026rsquo; -SqlCredential $cred\nThe results show we were able to connect successfully.\nStep 3 – Run some dbachecks First of all let’s run a single check to ensure our database is online and in the expected state. For this we can use the ‘DatabaseStatus’ check.\n$checkSplat = @{ SqlInstance = \u0026lsquo;xxxxxx.database.windows.net\u0026rsquo; SqlCredential = $cred Check = \u0026lsquo;DatabaseStatus\u0026rsquo; } Invoke-DbcCheck @checkSplat\nHere you can easily see, both because of the green result text and plus icon to the left, that our tests were successful. Both the database we created, AzDb01, and the master database are online and have the expected status.\ndbachecks uses tags on the pester tests to enable you to either call specific tests or groups of checks. Each check has a unique tag. In our previous example it was DatabaseStatus as well as tags that group like checks, for example Database.\n$checkSplat = @{ SqlInstance = \u0026lsquo;xxxxxx.database.windows.net\u0026rsquo; SqlCredential = $cred Check = \u0026lsquo;Database’ } Invoke-DbcCheck @checkSplat\nRunning all the database checks against our Azure SQL Database we get some failures.\nTests completed in 70.68s Tests Passed: 37, Failed: 28, Skipped: 8, Pending: 0, Inconclusive: 0\nThere are a lot of tests that pass or fail with valid reasons. However, some of the failures are due to errors running the check. These are to be expected since this is a PaaS (Platform as a Service) database offering. One example is the suspect pages check.\nThe test failed due to an error in the context block, and it clearly states that the \u0026lsquo;msdb.dbo.suspect_pages\u0026rsquo; table isn’t available in this version of SQL Server.\nSqlException: Reference to database and/or server name in \u0026lsquo;msdb.dbo.suspect_pages\u0026rsquo; is not supported in this version of SQL Server.\nThere are plenty of tests that do work against an Azure SQL Database though, allowing you to keep tabs on many different aspects of your database including:\nDatabase Collation Database Owners Column Identity Usage Duplicate Index Disabled Index Auto Shrink Database Orphaned User Compatibility Level Database Status Database Exists And more… Summary So to answer the question: yes, we can run dbachecks against our Azure SQL Databases. As long as we can connect and the version of SQL Supports the features needed to run the test we can ensure our databases in the cloud are configured just how we like them.\n","date":"2020-12-01T00:00:00Z","permalink":"https://jpomfret.github.io/p/dbachecks-and-azure-sql-databases/","title":"dbachecks and Azure SQL Databases"},{"content":"It’s no secret that I love dbatools and dbachecks. I am certain that I run a dbatools command at least once a day. It has fundamentally changed how I work as a dba, and it makes my life so much easier. I mention this when I’m presenting on these topics, but today I want to highlight what I consider the special sauce of open-source software.\ndbatools not only provides us with easy to run functions that get/set/test so many aspects of our environments, but it also encapsulates the knowledge of industry experts, and that right there is part of the magic. For example, I no longer have to remember Jonathan Kehayias’ detailed calculations for max memory. I can just run Test-DbaMaxMemory to check whether I need to adjust my settings.\nI would like to just say I’m in no way suggesting that we can skip the learning here, reading posts from experts and understanding why is vital – just it’s nice to be able to quickly call this knowledge from PowerShell rather from the depths of my brain (and that’s hoping it’s still stored in there).\nAdding Query Store Expertise to dbatools Last week I was working on configuring Query Store, and knowing that Erin Stellato (b|t) is the expert on that I headed over to her blog. I found exactly what I needed. Erin has a bunch of great posts on query store, but this one caught my eye: \u0026lsquo;Query Store Best Practices\u0026rsquo;.\nI read through her suggestions and could easily translate those using dbatools to optimally configure Query Store. First by setting several query store options using Set-DbaDbQueryStoreOption:\n$queryStoreBP = @{ SqlInstance = \u0026lsquo;mssql1\u0026rsquo; Database = \u0026lsquo;TestDb\u0026rsquo; State = \u0026lsquo;ReadWrite\u0026rsquo; MaxSize = 2048 CaptureMode = \u0026lsquo;Auto\u0026rsquo; CollectionInterval = 30 } Set-DbaDbQueryStoreOption @queryStoreBP\nSecondly by configuring two trace flags using Set-DbaStartupParameter (reboot required):\nSet-DbaStartupParameter -SqlInstance mssql1 -TraceFlag 7745,7752\nOnce I was happy with my settings, I realised we were missing a ‘test’ command for dbatools. The suite of ‘test’ functions in dbatools (a lot that end up as checks in dbachecks btw!), give us an easy way to check our environment against best practices, or our desired settings.\nSince dbatools is open-source I was able to write this function (Test-DbaDbQueryStore) and get it added into the module. It’s included as of version 1.0.131, so make sure you’re up to date. Taking Erin’s suggestions and wrapping them in a little PowerShell, I can make it easier for myself and everyone else to make sure we’re following her guidelines.\nTo test a single database you can use the following. It will output each setting, it’s current value, the recommended value, as well as a note from Erin’s blog post on why we should choose that. Again, I recommend you read her post to fully understand the why.\nTest-DbaDbQueryStore -SqlInstance mssql1 -Database testdb\nThis also means we can find any databases across many instances that aren’t set up to meet best practices:\n$results = Test-DbaDbQueryStore -SqlInstance mssql1, mssql2 | Where-Object {-not $_.IsBestPractice} | Select-Object SqlInstance, Database, Name, Value, RecommendedValue $results | Format-Table\nSo, over to you Step 1 – Go and read the why - \u0026lsquo;Query Store Best Practices\u0026rsquo;.\nStep 2 – Easily make sure your environment is up to par.\n","date":"2020-11-24T00:00:00Z","permalink":"https://jpomfret.github.io/p/ensure-query-store-meets-best-practice-across-your-environment/","title":"Ensure Query Store meets best practice across your environment"},{"content":"\nThis month’s blog party is hosted by Taiob Ali (b|t), and they ask how are we coping with the pandemic. Last week England entered its second national lockdown, which is slated to last until December 2nd, so now more than ever is a good time to reflect on some effective coping strategies. Thanks Taiob for hosting and I’m looking forward to reading everyone else’s mechanisms.\nIn the prompt it was suggested we could break this post down into three buckets: mental health, physical health and professional growth. Three really important areas so I’ll do just that.\nPhysical Health Changing the order slightly, I’m going to start with physical health – as it’s easier for me to talk about. Fitness and being active has always been a big part of my life. I played football (soccer) from the time I could walk until I graduated college in 2009. After that my fitness floundered until around 2011 when a friend encouraged me to try CrossFit. CrossFit gets a bad rap in some circles, but my experiences with the sport have been almost all positive. I know that when I show up to class someone is going to tell me exactly what the plan for the day is and then I’m going to get after it with a group of people from every corner of life. This is perfect for me, it feels like I’m playing a team sport again, and the workouts are varied and challenging.\nIn the UK, gyms were closed during the first national lockdown from March until almost the end of July. Our gym was kind enough to let members checkout some equipment and still posted daily workouts that we could complete at home. This was great, except I struggled to stay motivated without the structure of going to the gym for a class. I started to skip workouts, which made me feel worse about the whole situation (physically and mentally).\nWhen gyms were able to re-open on July 25th things felt a lot easier for me, even though there were still a lot of restrictions in place. To start with I was really anxious about whether it was going to be safe, but the gym did a fantastic job of implementing COVID measures to enable us to workout safely. My wife and I figured out a new routine, with me working from home we were able to go to the gym at lunch most days, and I almost immediately started feeling more in shape and healthier again. We found a new ‘normal’ that worked pretty well for us.\nThen November 5th happened. England has now entered lockdown 2 and gyms are back to being closed. Add in another factor, sunset today is 4:29pm! The days are getting shorter and it’s dark after work which is going to make it harder to stay motivated. It’s been a bit of a ramble up until now, but this is where my coping strategies come in!\nMy strategy is just to do something. I know that I feel better when I’m active so I just need to dig in and do it, easier said than done. I’m going to split this into three parts:\n1. Lunchtime workouts\nI’m unbelievably lucky that my wife is the most motivated person ever and I know she’ll work out most days. I just need to stick with her and I’ll be ok. My goal is to workout, bike, or walk on lunch as many days as possible.\n2. Yoga (3-4x a week)\nI also want to add in some yoga. My flexibility is shockingly bad and I can easily do yoga inside which will be a good after work activity.\n3. Plank Challenge\nI saw a tweet from Kendra Little last week on her fitness plans for lockdown and she included working on planks every day. I am also terrible at planks so I’m going to jump in on this and join her with a plank a day.\nhttps://twitter.com/Kendra_Little/status/1323192706064142336\nWith a bit of a plan under my belt I hope that I can stay more active this time around and I know it’ll make everything else associated with this lockdown easier.\nMental Health The second topic is something that I (and I think a lot of people) find harder to talk about. This year has been tough (understatement!) as my wife and I uprooted our lives in Ohio and moved to England in January. I had a great career opportunity and we decided to take the leap, with the biggest pros being the opportunity to travel around Europe more easily and to be able to spend time with my family, neither of which have really worked out.\nInstead we had just a couple of months of that plan before we were locked down, unable to travel and unable to even visit my family for several months. We now lived in a place where we knew basically no-one, perfect for a global pandemic.\nHowever, we managed to make the most of what we did have available. The rules during the first lockdown were that we could leave our house 1 time per day for exercise, so we did a lot of walking. We are lucky that where we live there are miles of public footpaths through beautiful English countryside. We had fun making new routes and mostly being able to follow them (I have zero sense of direction).\nhttps://www.instagram.com/p/B_AuvMuBsal/?utm_source=ig_web_copy_link\nWe also watched lots of movies. I’ve probably watched more movies in 2020 than I have in the rest of my life all together. It was nice to have some time to do that.\nThe coping strategy here is to simply make the most of what you have. I will work on being grateful for the things I do have available and overall try and stay positive.\nProfessional Growth\nOne benefit of this global pandemic is I have more time on my hands. Without the commute to work everyday I get almost 2 hours back in my day. Also my calendar has never been this low on social events, so there is free time everywhere! Have I put this to good use and worked hard in this professional growth category? Not really, no. I’ve found it really hard to be motivated to blog, something I’d previously worked hard on making more consistent. I have made a couple of short videos and spoken at a couple of virtual conferences, but overall I’ve been struggling to get content out.\nI did manage to find some motivation during Hacktoberfest, which is a month-long party to get people contributing to open source projects. I submitted some PRs and was reminded how much fun it is to fix bugs and write new dbatools commands.\nMy coping strategy here is to ride this renewed excitement and get some content created. I have a whole list of ideas, I just need to set aside some time and work on them. Summary This has turned into quite the ramble, so thanks for reading if you’re still with me! To sum it up, I know I need to stay active – this is really the base for everything else. If I workout I feel better about everything else. It’s as simple as that.\nI know that this year has been tough for everyone. I hope some of these strategies will be useful to other people too. Feel free to reach out to me on Twitter if there is anything I can do to help. We can get through this together.\n","date":"2020-11-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#132-how-are-you-coping-with-pandemic/","title":"T-SQL Tuesday #132: How Are You Coping with Pandemic?"},{"content":"In an ideal situation it probably shouldn’t matter which node of a failover cluster your resources and roles are hosted on, but the real world is often far from ideal. This post will talk through how we can record the current owner nodes and then use Pester to ensure we’re in the ideal configuration. This could be useful post maintenance activities or as a daily check to ensure things are as you expect.\nStep 1 – Store the current resource owners If we are going to test that we’re in our expected configuration, we need to record what that configuration looks like. I have a hard coded list of cluster names. However, you could easily pull them from a text file, or a database. Once we have the list of clusters we can use Get-ClusterGroup to determine the cluster roles and their current owners.\nTo persist this owner information I’m using ConvertTo-Json and then outputting it to a file. This creates a file that can easily be read back into PowerShell as an object using ConvertFrom-Json.\nIt’s also probably worth mentioning that this ideal configuration can be stored in source control. That’ll keep the file safe and you can easily keep track of any changes that are made to it.\n$clusters = ‘ClusterName1’,’ClusterName2’ $owners = $clusters | % { Get-ClusterGroup -Cluster $PSItem | select Cluster, Name, State, OwnerNode } $owners | % { [PSCustomObject] @{ Cluster = $_.Cluster.Name Name = $_.Name OwnerNode = $_.OwnerNode.Name State = $_.State -as [string] } } | ConvertTo-Json | Out-File ClusterGroupOwners.json\nYou’ll notice I’m creating a PSCustomObject to pipe to the ConvertTo-Json. Without that, the object from Get-ClusterGroup is exploded, with all properties, including nested properties exported into the JSON output. This is more than we need, and I think there is some value in having a clear concise output file. I’m also using -as [string] on the state property. PowerShell automatically translates the real state to a text value when outputted as it’s an enumeration type – but when you pipe that to ConvertTo-Json you get the raw integer value.\nStep 2 – Test the current configuration When it’s time to test our configuration we can read in our ClusterGroupOwners.json and then convert it back to a PowerShell object using ConvertFrom-Json. Now we have a PowerShell object of our ideal configuration we can loop through each cluster, checking the current group owners using Get-ClusterGroup again. This current state can then be matched against the desired configuration.\nI am using a pretty simple pester test for this work, saving it as Check-ClusterOwners.tests.ps1.\n$desiredConfig = Get-Content ClusterGroupOwners.json | ConvertFrom-Json $clusters = $desiredConfig | Select -Unique Cluster\nDescribe \u0026lsquo;The cluster resources should be owned by the same node as before\u0026rsquo; -Tag ClusterOwner { Foreach ($cls in $clusters) { Context (\u0026lsquo;Cluster owners are the same for {0}\u0026rsquo; -f $cls.Cluster) { $groups = $desiredConfig | Where-Object Cluster -eq $cls.Cluster $currentOwner = Get-ClusterGroup -Cluster $cls.Cluster foreach ($grp in $groups) { It (\u0026rsquo;{0} should be owned by {1}\u0026rsquo; -f $grp.Name, $grp.OwnerNode) { ($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode } } } } }\nWe’ll call this test using Invoke-Pester .\\Check-ClusterOwners.tests.ps1.\nIf everything is as expected we’ll get output similar to this for each cluster – depending on the resources you have set up in your cluster.\nDescribing The cluster resources should be owned by the same node as before Context Cluster owners are the same for clustername [+] RoleName should be owned by nodename 56ms [+] RoleName2 should be owned by nodename 92ms\nIf you have a resource that is not on the node that is expected, you’ll easily be able to see that in the output:\nContext Cluster owners are the same for clusterName [-] RoleName should be owned by NodeB 102ms Expected strings to be the same, but they were different. String lengths are both 13. Strings differ at index 12. Expected: \u0026lsquo;NodeB\u0026rsquo; But was: \u0026lsquo;NodeA\u0026rsquo; 13: ($currentOwner | Where-Object name -eq $grp.name).OwnerNode.Name | Should -Be $grp.OwnerNode\nThis method of testing can be useful to ensure you’re in the ideal state in many scenarios. For example you could store any databases in your estate that are not ‘online’ and then confirm post reboots/patching that all the databases are in the expected state.\n","date":"2020-09-29T00:00:00Z","permalink":"https://jpomfret.github.io/p/pester-test-your-cluster-role-owners/","title":"Pester test your Cluster Role Owners"},{"content":"\nThanks to Elizabeth Nobel (b|t) for hosting this month’s T-SQL Tuesday party and apologies for being as late as possible to the party! I love the topic of automation so felt sure I’d write something and then time slipped away. Luckily Mikey Bronowski (b|t) convinced me that it wasn’t too late to write something on my lunch break today (Wednesday in the UK) as it’s still Tuesday on Baker Island. Interesting fact Baker Island uses UTC-12:00 because since it’s uninhabited the islands time zone is unspecified (Wikipedia).\nAutomating dbachecks with scheduled task I wanted to write about automating your daily checks with dbachecks, there are many ways of expanding on this post, but this should give you a good basis to build from.\nThe environment I have two docker containers running on my laptop, one running SQL Server 2017 and one running SQL Server 2019. I will use these SQL Server instances to run my sample daily checks against.\nI have also created a database on the 2019 instance (mssql2) called dbachecks to store our daily check results.\nThe checks There are hundreds of checks available within the dbachecks module, and on top of that you can even write your own and include those. For this example I’m going to use the ‘DatabaseStatus’ check to ensure all my databases are online as expected.\nThe automation To automate the running of our daily checks we’ll first create a PowerShell script and then schedule that using task scheduler. If you have other enterprise scheduling tools available you could easily use those instead to invoke the PowerShell script.\nThe script for my example, shown below, is pretty simple. I have a section to define where the data will be stored (the ability to save dbachecks result information straight into a database was introduced with dbachecks 2.0 and so I would highly recommend updating if you’re on an earlier version).\nThe next section (lines 7-9) lists my SQL instances that I want to check, and the checks that should be run. The list of SQL instances could easily be pulled from a text file, a central management server (CMS) or a database to enhance the script.\nThe final three lines (lines 11-13) run the checks, apply a label of ‘MorningChecks’ (this allows for grouping of test results in the reports) and then inserts the results into the database.\nImport-Module dbachecks, dbatools\nDbachecks Database Connection $dbachecksServer = \u0026lsquo;mssql2\u0026rsquo; $dbachecksDatabase = \u0026lsquo;dbachecks\u0026rsquo;\nDefine instances and checks to run $SqlInstance = \u0026lsquo;mssql1\u0026rsquo;,\u0026lsquo;mssql2\u0026rsquo; $checks = \u0026lsquo;DatabaseStatus\u0026rsquo;\nInvoke-DbcCheck -SqlInstance $SqlInstance -Checks $checks -PassThru | Convert-DbcResult -Label \u0026lsquo;MorningChecks\u0026rsquo; | Write-DbcTable -SqlInstance $dbachecksServer -Database $dbachecksDatabase\nI saved this script to C:\\dbachecks\\dbachecks.ps1 and then ran the following PowerShell to schedule the execution of the script daily at 7am.\n$RunAs = Get-Credential $taskSplat = @{ TaskName = \u0026lsquo;Daily dbachecks\u0026rsquo; Action = (New-ScheduledTaskAction -Execute \u0026lsquo;powershell\u0026rsquo; -Argument \u0026lsquo;-File dbachecks.ps1\u0026rsquo; -WorkingDirectory C:\\dbachecks) Trigger = (New-ScheduledTaskTrigger -Daily -At \u0026lsquo;07:00\u0026rsquo;) User = $RunAs.UserName Password = ($RunAs.GetNetworkCredential().Password) RunLevel = \u0026lsquo;Highest\u0026rsquo; } Register-ScheduledTask @taskSplat\nIt’s important to note that the account used to run this scheduled task needs to be an account that has access to all of the SQL instances you want to check, as well as the SQL instance you are writing the final data to.\nResults Since this is now scheduled daily we can grab our morning coffee, sit down at our desk and immediately review our estate and ensure everything is as expected.\nWe wrote the data to a SQL Server so you can go and query the data directly. By default there will be two tables created in the database.\nCheckResults – contains the actual results of the checks against your server dbachecksChecks – contains the metadata of the checks including tags and descriptions for each check you have invoked. The other option is to use the dbachecks PowerBi dashboard, by running the following you can load the dashboard and connect to your dbachecks results database:\nStart-DbcPowerBi -FromDatabase\nWhen this opens you can see there were some failures on mssql1, right clicking on the orange bar you can drill through to see the details.\nOn the details pane you can see there are two offline databases that I need to look into.\nSummary Finally, this automation is just the starting piece of automating your daily checks. There are many ways to expand on this, but this is how you can get started with automating daily health checks with dbachecks.\nThanks again for hosting, and sorry for being so late!\n","date":"2020-09-09T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#130-automate-your-stress-away/","title":"T-SQL Tuesday #130 – Automate Your Stress Away"},{"content":"I got a message from a friend on Twitter last night asking ‘Is there an easy way to get dbachecks backup info into an Excel spreadsheet?’. I sent them a couple of ideas, but figured this is a great use case that many people might be interested in. Pairing infrastructure testing using dbachecks with creating Excel reports with the ImportExcel module is a great addition to your automation tool belt. I also had ImportExcel on my mind this week after watching some great demos from Mikey Bronowski (b|t) at a user group earlier this week.\nRun the Checks First step is to run some checks. I’ve previously written about using dbachecks to check on your SQL Server database backups, so I’m going to use that as a base here.\nI’m not going to change any of the configuration options, but that is covered in the post I linked to above. I am going to add the DatabaseStatus check with the default configuration to ensure all my databases are online.\n$sqlinstances = \u0026lsquo;mssql1\u0026rsquo;,\u0026lsquo;mssql2\u0026rsquo;,\u0026lsquo;mssql3\u0026rsquo;,\u0026lsquo;mssql4\u0026rsquo; $testResults = Invoke-DbcCheck -SqlInstance $sqlinstances -Check LastBackup, DatabaseStatus -PassThru\nYou can see I have a nice balance of green (passed tests) and red (failed tests). Not really the balance we’re looking for in production, but perfect for a demo environment.\nUsing the -PassThru parameter means that the test results are both displayed on screen and saved to my $testResults variable. We’ll use that to create our report.\nCreate the Report – Option 1: Export-Csv The first option we have here is to just get the data into a csv. We can do that natively in PowerShell using the Export-Csv function.\n$testResults.TestResult | Select-Object Describe, Context, Name, Result, FailureMessage | Export-Csv c:\\temp\\backups.csv -NoTypeInformation\nThis will get our data into a csv, which we can then manipulate in Excel.\nCreate the Report – Option 2: Export-Excel The second option is to use the ImportExcel module. This is easily in my top 5 all-time favourite PowerShell modules. With this module we can create a great looking Excel report in just a few lines. The following will take our test results and create two worksheets in one Excel file. The first sheet will contain our raw data, formatted as an Excel table with some conditional formatting to highlight the failed tests. The second tab will contain a pivot table/chart of our results broken down by the test type and result.\n$ConditionalFormat =$( New-ConditionalText -Text Failed -Range \u0026lsquo;D:D\u0026rsquo; )\n$excelSplat = @{ Path = \u0026lsquo;C:\\Temp\\Backups.xlsx\u0026rsquo; WorkSheetName = \u0026lsquo;TestResults\u0026rsquo; TableName = \u0026lsquo;Results\u0026rsquo; Autosize = $true ConditionalFormat = $ConditionalFormat IncludePivotTable = $true PivotRows = \u0026lsquo;Describe\u0026rsquo; PivotData = @{Describe=\u0026lsquo;Count\u0026rsquo;} PivotColumns = \u0026lsquo;Result\u0026rsquo; IncludePivotChart = $true ChartType = \u0026lsquo;ColumnStacked\u0026rsquo; }\n$testResults.TestResult | Select-Object Describe, Context, Name, Result, FailureMessage | Export-Excel @excelSplat\nThe final results are shown below. With so many more checks available in the dbachecks module it would be easy to expand on this example and get a comprehensive report of your environment.\nThe full script is available on my Github demos repo.\nResults Worksheet:\nPivot Table/Chart:\n","date":"2020-08-28T00:00:00Z","permalink":"https://jpomfret.github.io/p/dbachecks-meets-importexcel/","title":"dbachecks meets ImportExcel"},{"content":"Have you ever had someone send you the name of a SQL Server and database to do some work, but when you try to connect to the server you can’t? Then,come to find out, there are four named instances on the box and you don’t know which one hosts the database? No? Just me?\nLuckily, dbatools has a couple of commands that can help us out with this. Firstly, we can use Get-DbaService to get a list of instances that are running on the server:\n$SqlInstances = Get-DbaService -ComputerName mssql1 -Type Engine | Select @{L=\u0026lsquo;SqlInstance\u0026rsquo;;e={(\u0026rsquo;{0}\\{1}\u0026rsquo; -f $_.ComputerName, $_.InstanceName)}}\nI went ahead and piped this to the Select-Object and built the SqlInstance property to be ‘ServerName\\InstanceName’. We can now use this in any of the other dbatools commands. For my use case I wanted database information, so I went with Get-DbaDatabase:\nGet-DbaDatabase -SqlInstance $SqlInstances.SqlInstance | Format-Table SqlInstance, Name, Status, RecoveryModel -AutoSize\nThis made it easy for me to find the database in question without having to connect to each instance manually.\nYou could also use this if you had a list of servers by just passing in a comma seperated list to the -ComputerName parameter on Get-DbaService.\nJust a short post today, but hopefully useful to somebody.\n","date":"2020-08-18T00:00:00Z","permalink":"https://jpomfret.github.io/p/get-a-list-of-databases-from-named-sql-instances/","title":"Get a list of databases from named SQL Instances"},{"content":"Recently I was tasked with troubleshooting an incident on a SQL Server at a certain point in the past, the issue being a high CPU alert. It’s hard (without monitoring solutions set up) to go back in time and determine what the issue is. However, one thing we can check is the windows event log to see if there was anything happening on the server at that time.\nNow, you probably know that my favourite tool of choice is PowerShell, so let\u0026rsquo;s take a look at how we can use Get-WinEvent to see what was happening in the past.\nGet-WinEvent is the newer revamped version of Get-EventLog, and there are two improvements I believe are worth mentioning. Firstly, with the introduction of filter parameters we can now find certain events much easier, which we’ll talk about a little later. Secondly, the performance of Get-WinEvent is much faster than using the legacy command. I believe this is due to the filtering happening at the event engine instead of within PowerShell.\nFinding some logs First things first, let\u0026rsquo;s see what logs we have available on our target machine. I’m targeting a remote machine with the code below, but if we’re investigating an issue on our local machine we can just exclude the -ComputerName parameter.\nThis snippet will output all of the logs on my remote machine that contain records to a gridview. I love to use Out-GridView for these kinds of tasks because this returned 101 logs and I can now add some text into the search bar to filter for things I might be interested in.\nGet-WinEvent -ListLog * -ComputerName dscsvr2 | Where-Object RecordCount | Out-GridView\nYou can see if I add dsc into the search bar of Out-Grid View I have one log with records in that I could investigate further.\nFiltering events I already mentioned this, but the new Get-WinEvent gives us three options for filtering events. I’ll show this example using -FilterHashTable, but just know there are two other options available, -FilterXPath and -FilterXml.\nThe full list of key/value pairs that we can use to filter on are under the -FilterHashTable parameter section of the Microsoft Docs. For now, we’ll look at filtering based on log name and time.\nSo, say I have an alert for high CPU at 21:30 on 2020-07-31 and I want to know what was happening around that time on the server.\nFirst I’ll set up a few parameters. I’m going to set the $computerName (I could add multiple here if I wanted to collect logs from more than one server) and the $issueDateTime. I’ve then also specified the $windowMins, I’m going to use to create a window of time around my issue to collect events for.\n$computerName = \u0026lsquo;dscsvr2\u0026rsquo; $issueDateTime = get-date(\u0026lsquo;2020-07-31 21:30\u0026rsquo;) $windowMins = 30\nNext we’ll build the filter hash table. You can do this inline when you call the command, but I personally like to break it out just for readability.\n$winEventFilterHash = @{ LogName = \u0026lsquo;system\u0026rsquo;,\u0026lsquo;application\u0026rsquo; StartTime = $issueDateTime.AddMinutes(-($windowMins/2)) EndTime = $issueDateTime.AddMinutes(($windowMins/2)) }\nFinally, we’ll call Get-WinEvent, then pass in the filter hash table and the computer name. I’m selecting just a few standard properties, as well as a calculated property to get the username instead of just the sid. The final piece of this pipeline is to use Format-Table. I would also recommend using Export-Excel to pipe it straight to an excel file for analysis.\n(Note - I use Export-Excel in this post if you\u0026rsquo;re interested in that - Getting OS and SQL Version information with dbatools)\nGet-WinEvent -FilterHashtable $winEventFilterHash -ComputerName $computerName | Select-Object LogName, ProviderName, TimeCreated, Id, LevelDisplayName, @{l=\u0026lsquo;UserName\u0026rsquo;;e={(New-Object System.Security.Principal.SecurityIdentifier($_.UserId)).Translate([System.Security.Principal.NTAccount])}}, Message | Format-Table\nAs you can see below, the results are returned from both the application and system logs along with the time, level, username and message. Now, this is just a test box in my lab, but this is a great way of grabbing a window of events from your server to help troubleshoot issues in the past.\n","date":"2020-08-04T00:00:00Z","permalink":"https://jpomfret.github.io/p/using-get-winevent-to-look-into-the-past/","title":"Using Get-WinEvent to look into the past"},{"content":"TLDR; This code will script out foreign keys and views (including object level permissions), drop the objects, truncate all the tables, and recreate the objects.\nThe Details The most popular post on my blog so far was called ‘Disable all Triggers on a Database’ and this one is a good follow up from that post.\nThe scenario here is you need to remove all the data from the tables in your database. This could be as part of a refresh process, or perhaps to clear out test data that has been entered through an application. Either way, you want to truncate all the tables in your database.\nUsing a copy of the AdventureWorks2017 database for my demos, the easiest option to truncate all the tables is to script out truncate statements using the metadata stored in sys.tables.\nSELECT \u0026lsquo;Truncate table \u0026rsquo; + QUOTENAME(SCHEMA_NAME(schema_id)) + \u0026lsquo;.\u0026rsquo; + QUOTENAME(name) FROM sys.tables\nYou’ll get a results set like shown below which you can copy out into a new query window and execute.\nThe problem is if you have foreign keys, even if you order the truncate statements to remove the dependent data first, you can’t issue a truncate statement. The way around this is to script out the foreign keys, drop them, run the truncate statements and then recreate the foreign keys. This is not difficult in T-SQL, but it’s easier with PowerShell and a little bit of dbatools magic.\nTruncate tables with PowerShell The full script is available up on my Github, but I’ll walk through the process here. During my post on disabling triggers I stored the previously enabled triggers in a variable to reuse during the script. I had a really great comment on this post that pointed out a problem: if the session crashed for some reason we would lose the list of triggers we wanted to enable. I will solve that problem in this post by instead of using a variable, saving the information in a temporary file.\nFirst things first, we need to set up a couple of variables to define our SqlInstance, database and the folder we’ll use as our workspace.\nI’ll then use Connect-DbaInstance to connect to the instance and save the smo object. This will save having to reconnect to the instance multiple times.\n$sqlInstance = \u0026lsquo;mssql1\u0026rsquo; $database = \u0026lsquo;AdventureWorks2017\u0026rsquo; $tempFolder = \u0026lsquo;C:\\temp\u0026rsquo;\n$svr = Connect-DbaInstance -SqlInstance $sqlInstance\nThe next step is to collect the foreign keys that we’ll need to drop and recreate. It’s important to note here there are also some views that depend on these tables, so I can also collect that information at the same time. # Collect up the objects we need to drop and recreate $objects = @() $objects += Get-DbaDbForeignKey -SqlInstance $svr -Database $database $objects += Get-DbaDbView -SqlInstance $svr -Database $database -ExcludeSystemView\nNow that we have collected the objects into a variable we can pipe this to the Export-DbaScript command to generate T-SQL scripts for both dropping and then recreating the objects. Something to take into consideration when dropping and recreating views is that if there are permissions set at the object level we need to include those in our create scripts. We can use the New-DbaScriptingOption command to set the options we care about when we create the scripts.\nHere we are including the permissions, the ‘ScriptBatchTerminator’, which will add ‘Go’ between objects, and finally setting the file type to ANSI. When we call Export-DbaScript we can then use these options for the -ScriptingOptionsObject parameter.\n# Script out the create statements for objects $createOptions = New-DbaScriptingOption $createOptions.Permissions = $true $createOptions.ScriptBatchTerminator = $true $createOptions.AnsiFile = $true\n$objects | Export-DbaScript -FilePath (\u0026rsquo;{0}\\CreateObjects.Sql\u0026rsquo; -f $tempFolder) -ScriptingOptionsObject $createOptions\nWe also need to script out the drop statements. To do that we’ll create another options object, this time setting ScriptDrops to true. Then we’ll again call Export-DbaScript with the -ScriptingOptionsObject parameter.\n# Script out the drop statements for objects $options = New-DbaScriptingOption $options.ScriptDrops = $true $objects| Export-DbaScript -FilePath (\u0026rsquo;{0}\\DropObjects.Sql\u0026rsquo; -f $tempFolder) -ScriptingOptionsObject $options\nOnce we have the scripts safely in our temporary folder we’ll run three simple statements.\nFirst, we’ll run the drop statements we scripted out.\nSecond, remember we saved the smo connection to our server in the $svr variable. We’ll use that to access all the tables in our database, pipe that to a Foreach-Object and call the TruncateData method.\nThird, we’ll call Invoke-DbaQuery to recreate the foreign keys and the views we previously dropped.\n# Run the drop scripts Invoke-DbaQuery -SqlInstance $svr -Database $database -File (\u0026rsquo;{0}\\DropObjects.Sql\u0026rsquo; -f $tempFolder)\nTruncate the tables $svr.databases[$database].Tables | ForEach-Object { $_.TruncateData() }\nRun the create scripts Invoke-DbaQuery -SqlInstance $svr -Database $database -File (\u0026rsquo;{0}\\CreateObjects.Sql\u0026rsquo; -f $tempFolder)\nThe final step is to clear up the script files we saved to the temporary folder.\n# Clear up the script files Remove-Item (\u0026rsquo;{0}\\DropObjects.Sql\u0026rsquo; -f $tempFolder), (\u0026rsquo;{0}\\CreateObjects.Sql\u0026rsquo; -f $tempFolder)\nThis script can be reused for any database that you may need to clear out. As I was writing this post, I realised this could probably be a dbatools command… watch this space. ?\n","date":"2020-06-30T00:00:00Z","permalink":"https://jpomfret.github.io/p/truncate-all-the-tables-in-a-database-with-powershell/","title":"Truncate all the Tables in a Database with PowerShell"},{"content":"I’m a SQL Server Database Engineer by day, but I must say my blog has a lot more PowerShell and automation posts than T-SQL. However, last week I found a really great T-SQL aggregate function that I had no idea existed, so I thought I’d share it with you.\nI have been working on a project to document our SQL Server environment and create GitHub issues for things that need fixed. Issues are written in markdown so you can easily generate some pretty good looking issues with plenty of data using PowerShell. This is worth a blog post of it’s own, so keep an eye out for that soon.\nLong story short I wanted a way to be able to list all the SQL Server instances on the server I was logging the issue for. I have a database with two tables, one that contains server information and one that contains instance information. Running the following gets me one row per server/instance combination.\nSELECT s.ServerListId, s.ServerName, i.InstanceListId, i.InstanceName FROM ServerList s INNER JOIN InstanceList i ON s.ServerListId = i.ServerListId\nServerListId ServerName InstanceListId InstanceName 1 MSSQL1 1 MSSQLSERVER 2 MSSQL2 2 MSSQLSERVER 3 MSSQL3 3 MSSQLSERVER 2 MSSQL2 4 NAMEDINST1 2 MSSQL2 5 NAMEDINST2 I started thinking about how to group this data by server name and then concatenate the instance names together. Luckily a quick google found STRING_AGG(). This T-SQL aggregate function has only been available since SQL Server 2017, and does exactly what I needed. It takes two parameters, the first being the column name that should be aggregated and the second a separator to use.\nFor this example I’ll group by ServerName, and aggregate the InstanceName column using a comma to separate the values.\nSELECT ServerName, STRING_AGG(InstanceName,\u0026rsquo;, \u0026lsquo;) as InstanceName FROM ServerList s INNER JOIN InstanceList i ON s.ServerListId = i.ServerListId GROUP BY ServerName\nServerName InstanceName MSSQL1 MSSQLSERVER MSSQL2 MSSQLSERVER, NAMEDINST1, NAMEDINST2 MSSQL3 MSSQLSERVER Hope some of you find this quick T-SQL post useful. It definitely fit my need well for this scenario.\n","date":"2020-06-23T00:00:00Z","permalink":"https://jpomfret.github.io/p/using-t-sql-to-aggregate-strings/","title":"Using T-SQL to Aggregate Strings"},{"content":"\nI feel like I say this every month, but it’s already time for another edition of T-SQL Tuesday. This months blog party is hosted by Kenneth Fisher (B|T) and he’s looking for tips \u0026amp; tricks, but nothing DBMS related. As you might already know, I love shortcuts and tips \u0026amp; tricks – so I was really excited to see this prompt. First, because I easily found a few I wanted to share, but secondly, I can’t wait to read about everyone else’s tips \u0026amp; tricks.\nI had a hard time narrowing this down to just one shortcut, so I’ve picked three – but they fit together nicely. First, I have to let you in on a small secret:\nI’m a tab hoarder Whether it’s chrome or SSMS, I cannot help myself when it comes to opening new tabs. It’s not uncommon for me to have so many chrome tabs open that you can only see the logos.\nI even got called out by my good friend Andrew (B|T) this last week:\nhttps://twitter.com/awickham/status/1267503576571674624\nMy tips \u0026amp; tricks are focused around managing tabs, and they work in all browsers (at least all that I have on my laptop. Chrome, Edge, IE11).\nCtrl + Tab – Switch tabs Ctrl + W - Close a tab Ctrl + Shift + T - Reopen the last closed tab The first two are pretty self-explanatory, but the third one deserves a special mention. I’m sure it’s happened to everyone- you close a tab and the instant it’s gone, you realise you actually needed that one. Well Ctrl + Shift + T to the rescue. This shortcut will revive that last closed tab from the dead and the magic here is if you keep pressing it, it’ll keep opening tabs you shut previously.\nIn fact, if you close an entire browser window full of 20 tabs (that I’m sure you need all of) and in another browser window you press Ctrl + Shift + T, like magic, all the tabs come back to you.\nBonus Content Now I started off using these shortcuts in internet browsers, but then I discovered they work in my favourite code editor as well. All three of the keyboard shortcuts above work in VSCode, so now you can easily switch between scripts with Ctrl + Tab, close a script with Ctrl + W and then reopen it with Ctrl + Shift + T.\nI love this crossover functionality, and I feel like it adds so much when you can open a program and it works in a way that you’re used to.\n","date":"2020-06-09T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#127-non-sql-tips-tricks/","title":"T-SQL Tuesday #127 – Non SQL Tips \u0026 Tricks"},{"content":"I’ve written previously about using containers for demos on my laptop, specifically for my data compression talk. Since I switched those demos over I haven’t looked back- if it’s possible to run my demos off of containers I always choose that option.\nI recently presented a talk called ‘Life Hacks: dbatools edition’ which walks through 6 scenarios where you can immediately implement dbatools to quickly reap the rewards. The demos can all be run on containers, but I did need to get a little more complex to be able to show off dbatools migration commands. To do this I used a docker compose file.\nThe compose file creates one instance straight from the Microsoft SQL Server 2019 image and a second one from a dockerfile that specifies the base SQL Server 2017 image, copies in the files needed to attach the AdventureWorks2017 database, and runs some SQL to get everything setup exactly as desired. Feel free to check out this setup on my Github.\nOne of the things that bothered me about running my demos on containers was that I couldn’t use windows authentication. Instead I had to pass in a SQL login to connect for every command.\nEnter PSDefaultParameterValues I first heard about PSDefaultParameterValues from a PSPowerHour session by Chrissy LeMaire in 2018. After rewatching this recently, I realised she even mentioned this exact scenario. However, it took until I recently rediscovered this handy preference variable that it all clicked together.\nPSDefaultParameterValues does exactly what the name suggests- it lets you specify default values for parameters. PSDefaultParameterValues can be set as a hash table of parameter names and values that will be used in your session for any function that can use it. A simple example is the verbose parameter. If you wanted to turn on the -Verbose switch for every function you run you could add -Verbose to each function call, or you could set PSDefaultParameterValues.\nOption 1 – Add -Verbose to individual commands Get-DbaDbBackupHistory -SqlInstance mssql1 -Verbose Repair-DbaDbOrphanUser -SqlInstance mssql1 -Verbose\nOption 2 – Set PSDefaultParameterValues $PSDefaultParameterValues = @{ \u0026lsquo;*:Verbose\u0026rsquo; = $True } Get-DbaDbBackupHistory -SqlInstance mssql1 Repair-DbaDbOrphanUser -SqlInstance mssql1\nOne thing to note when specifying PSDefaultParameterValues as I have above: this will overwrite any parameters you already have saved to PSDefaultParameterValues, so be careful. Another way to set -Verbose to true would be to use the following notation:\n$PSDefaultParameterValues[\u0026rsquo;*:Verbose\u0026rsquo;] = $True\nGetting more specific In the above examples I’m using a wildcard (*) on the left side to specify that this parameter is for all functions. You can also focus in PSDefaultParameterValues by specifying one certain function name that the parameter value will apply to:\n$PSDefaultParameterValues[\u0026lsquo;Get-DbaDbTable:Verbose\u0026rsquo;] = $True\nYou can also specify just the dbatools commands by taking advantage of their naming conventions and using:\n$PSDefaultParameterValues[\u0026rsquo;*-Dba*:Verbose\u0026rsquo;] = $True\nPSDefaultParameterValues for connecting to containers As I mentioned, my use case was to avoid having to specify a credential for every function that connected to my SQL Server running in a container. To use this for dbatools I need to specify a few parameter names. Most dbatools functions take the credential for the -SqlCredential parameter, but for the copy commands there is both -SourceSqlCredential and -DestinationCredential that need to be specified.\nFirst, I create a PSCredential that contains my username and password (note: this is for a demo environment and is insecure as the password is in plain text. If you are using this for other scenarios you’ll want to protect this credential). $securePassword = (\u0026lsquo;Password1234!\u0026rsquo; | ConvertTo-SecureString -asPlainText -Force) $credential = New-Object System.Management.Automation.PSCredential(\u0026lsquo;sa\u0026rsquo;, $securePassword)\nOnce I have the credential I can specify all the parameters that should use that credential by default:\n$PSDefaultParameterValues = @{\u0026quot;*:SqlCredential\u0026quot;=$credential \u0026ldquo;*:DestinationCredential\u0026rdquo;=$credential \u0026ldquo;*:DestinationSqlCredential\u0026rdquo;=$credential \u0026ldquo;*:SourceSqlCredential\u0026rdquo;=$credential}\nNow whenever I call a function within this session, the specified parameters will use my credential. Therefore I can run the following and it’ll automatically use my saved sa login credential.\nGet-DbaDatabase -SqlInstance mssql1\nPSDefaultParameterValues in your profile Setting PSDefaultParameterValues will only persist in the current session, however you can add the code above to your profile so that these default values are always provided. If I do, whenever I open a PowerShell window I can easily connect to my containers without having to specify the credential.\nOne thing to note is that this might be overkill. In my situation this is my demo machine. I always use the same sa password for any containers I run, and the majority of the time I’m running commands with a SqlCredential parameter I want to connect to those containers.\nOverride Even if you have set PSDefaultParameterValues in your profile you can still override that default value on any command just by specifying a new value. For example, running the following will pop up the credential request window for you to enter new credentials.\nGet-DbaDatabase -SqlInstance mssql1 -SqlCredential (Get-Credential)\nSummary To wrap this up, I’ve found a lot of time savings by adding PSDefaultParameterValues to my profile. I can now quickly fire up PowerShell and start running functions against my containers. It also keeps my demo scripts clean and easier to read. There is no need to specify the same parameters over and over again when it’s always going to be the same value.\n","date":"2020-05-27T00:00:00Z","permalink":"https://jpomfret.github.io/p/using-psdefaultparametervalues-for-connecting-to-sql-server-in-containers/","title":"Using PSDefaultParameterValues for connecting to SQL Server in containers"},{"content":"\nWell it’s May T-SQL Tuesday time! Honestly I’m not sure if time is crawling or flying by, it seems like forever ago we got writing for the April prompt on unit testing databases. Thanks to Glenn (b|t) this month for hosting an interesting topic. I’m looking forward to reading all the responses. Also a bigger thanks for publicising Folding@Home and setting up the #SQLFamily team!\nGlenn wants to know what we’re doing in response to COVID-19 and if we’re contributing to the FAH #SQLFamily team, what our experience has been.\nFolding@Home I installed the Folding@Home client almost a month ago now on my Intel NUC. The NUC is connected up to my TV and mainly used as a media server. Occasionally I’ll use it to build out a lab to test something but most of the time it’s idle. Perfect to donate.\nMy FAH setup is pretty standard. I installed the client, requested a passkey, and set it loose. One thing I am a little uncomfortable with is the CPU is at 100% all the time FAH is running a workload, and it gets a little hot. I started manually setting the workload to ‘finish’ (finish the workload currently running but then pause) in the evenings and then setting it back to ‘fold’ in the morning.\nSince I’m human, sometimes I forgot.\nEnter PowerShell.\nI found that you could pass commands into the FAHClient.exe and therefore set the status from PowerShell. I created a simple module PsFah with a function to control the FAH client Status. When I say simple, it currently has one function that sets the local client status to fold, pause or finish. Perhaps I’ll add more over time (I started getting more side-tracked by this and then realised I needed to actually write this post).\nI then set up two scheduled tasks (details below) that set the client to start folding at 7am and then set it to finish folding at 8pm. Removing the human, and therefore improving the process.\n$taskSplat = @{ Action = New-ScheduledTaskAction -Execute PowerShell.exe -Argument \u0026lsquo;Set-FahStatus -Status Fold\u0026rsquo; Trigger = New-ScheduledTaskTrigger -Daily -At \u0026lsquo;07:00\u0026rsquo; Description = \u0026lsquo;Start Folding\u0026rsquo; Principal = New-ScheduledTaskPrincipal -UserID \u0026ldquo;NT AUTHORITY\\SYSTEM\u0026rdquo; -LogonType ServiceAccount -RunLevel Highest\n} Register-ScheduledTask @taskSplat -TaskName \u0026lsquo;Start Folding\u0026rsquo;\n$taskSplat = @{ Action = New-ScheduledTaskAction -Execute PowerShell.exe -Argument \u0026lsquo;Set-FahStatus -Status Finish\u0026rsquo; Trigger = New-ScheduledTaskTrigger -Daily -At \u0026lsquo;20:00\u0026rsquo; Description = \u0026lsquo;Finish Folding\u0026rsquo; Principal = New-ScheduledTaskPrincipal -UserID \u0026ldquo;NT AUTHORITY\\SYSTEM\u0026rdquo; -LogonType ServiceAccount -RunLevel Highest } Register-ScheduledTask @taskSplat -TaskName \u0026lsquo;Finish Folding\u0026rsquo;\nCommunity Involvement Another way I’ve been trying to give back a little is with community involvement. I was lucky enough to be selected to speak as part of the Data Weekender event. It was great to be able to deliver a session and share some dbatools knowledge. It was even better to watch some sessions and chat with the community. I miss that interaction that you get at conferences, and this day helped to fill that void – if only virtually.\nI am also really excited to be speaking at the GroupBy conference, which when this post goes live will be today!\nI enjoy travelling and speaking and I was really disappointed that I had been selected to speak at a couple of events that were then cancelled because of COVID. Being able to still speak and contribute to the community virtually has been a really great experience.\nFinally It’s easy to feel like you’re not doing enough during this crisis. it’s hard to stay motivated and to be productive when there is so much stress and anxiety in the world. First things first, we have to look after ourselves and our people. If you have extra energy left over there are plenty of ways to give back, but it’s important for all of us to remember that just surviving right now takes more energy than usual.\nStay safe folks.\n","date":"2020-05-12T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#126-foldinghome/","title":"T-SQL Tuesday #126 – Folding@Home"},{"content":"I wrote previously about how I loved little life hacks and shortcuts for my February T-SQL Tuesday prompt. If you read that you’ll know I use VSCode a lot and really love all the shortcuts available in that program. This is just a quick tip that I’m so glad I found, read – took the time to work out.\nI specifically write a lot of PowerShell in VSCode and so often find myself using F8 to run the selected line in the integrated console. One thing that always drove me a little crazy was that my cursor stayed in the integrated console after execution, rather than returning to the script I was writing. I hadn’t managed to find the shortcut to return to the editor window I was working in until recently when I decided to figure it out.\nSince I started writing this post a couple of weeks ago, I discovered an even better solution thanks to the following tweet from Simon Sabin. The tweet also links to a GitHub issue where there is a discussion on why this is the default behaviour.\nhttps://twitter.com/simon_sabin/status/1252253795603681281\nSo by adding the following to your settings.json file you can override that behaviour, and keep the focus in your script pane.\n\u0026ldquo;powershell.integratedConsole.focusConsoleOnExecute\u0026rdquo;: false\nOriginal Solution The original solution to this dilemma is to use the keyboard shortcut to return focus to the script pane. Using Ctrl+1 will return you to the first editor group (unless you are using ZoomIt!).\nIf you have more than one editor group open you can use CTRL+2 to get to the second group, or CTRL+3 to get to the third group.\nAlso, as with most shortcuts in VSCode, you can also customise the key bindings, by opening the command palette (F1) and choosing ‘Open Keyboard Shortcuts’ either in the GUI or JSON format.\nThe commands to customise are below with the defaults:\n- workbench.action.focusFirstEditorGroup: Ctrl+1\n- workbench.action.focusSecondEditorGroup: Ctrl+2\n- workbench.action.focusThirdEditorGroup: Ctrl+3 (and so on up to the eighth editor group)\nA bonus tip for you: If you’re in the GUI keyboard shortcut editor and you right click and copy, or press Ctrl+C, you’ll actually copy the JSON you’d need to customise your key bindings in the keyboard shortcuts (JSON) file.\n{ \u0026ldquo;key\u0026rdquo;: \u0026ldquo;ctrl+5\u0026rdquo;, \u0026ldquo;command\u0026rdquo;: \u0026ldquo;workbench.action.focusFifthEditorGroup\u0026rdquo; }\nI’m really excited to have discovered a couple of optimisations so now when I’m writing a script and I execute a line of PowerShell in the integrated console I will easily be able to navigate back to my editor pane using Ctrl+1 instead of having to reach for the mouse.\n","date":"2020-05-06T00:00:00Z","permalink":"https://jpomfret.github.io/p/changing-focus-on-code-execution-in-vscode/","title":"Changing focus on code execution in VSCode"},{"content":"It’s an interesting time we’re living in right now. I’m based in the UK and we just received the announcement last week that we have at least three more weeks of lockdown. I’ve been working at home for over a month now and I feel really lucky that I can work from home. There are a lot of folks who can’t and are instead working hard on the front lines to keep us safe and well. Firstly, a shout out to those folks! Thanks for all your doing.\nNow, this is not a post on how to set up your home office, or how to be the most productive work from home employee. This is a post to solve one simple problem. I’ve no idea what day it is.\nMonday through Friday I wake up, drink some coffee and then head to my desk to work my day job. I usually workout in the garden with my wife at lunch and then we walk after work. Every day is pretty much the same, so I’ve written an earth-shattering PowerShell function for you.\nfunction Find-WhatDayAreWeDoing { param ( [switch]$Raw )\nif($raw) { (Get-Date).DayOfWeek } else { (\u0026quot;We're doing {0} today!\u0026quot; -f (Get-Date).DayOfWeek) } }\nYou can add this into your profile and then simply just call the function to remind you what day we’re on.\nI’ve also added the -Raw switch so you can use that to just return the day, this is useful if you wanted to add it into your prompt.\nTo change the prompt in PowerShell you just need to create a function called ‘Prompt’. For example, if I run this in my PowerShell session or put it in my profile, I’ll be able to see what day it is constantly.\nfunction Prompt { Write-Host (\u0026quot;[{0}] PS\u0026gt; \u0026quot; -f (Find-WhatDayAreWeDoing -raw)) }\nThe prompt is a really useful snippet of code and there are many great ideas out there. I use the amazing dbatools prompt. Their prompt lists the current time and the execution time for the last statement run. You could change it to add in the day also:\nI’ll be honest, development on Set-WhatDayWeAreDoing is not going as well. Stay safe folks and keep keeping on.\n","date":"2020-04-21T00:00:00Z","permalink":"https://jpomfret.github.io/p/anyone-know-what-day-it-is/","title":"Anyone know what day it is?"},{"content":"\nIt’s T-SQL Tuesday time again! March felt really long for a lot of us as we got used to our new way of life, so I’m excited we made it to April’s prompt.\nThis month’s topic is asking for a discussion around whether unit testing for databases is valuable. Since getting involved with dbatools and beginning to write more structured PowerShell (meaning modules and DSC resources rather than just an odd script) I have learnt a lot more about testing code. However, I have zero experience with testing databases. So I’m excited for this topic. Thanks for hosting Hamish (b|t).\nI recently watched Steve Jones give a session as part of Redgate Streamed on using tSQLt for database testing. It’s not the first time I’ve heard about this tool and it’s been on my list for way too long as something I want to investigate a little further. I figured this was a good excuse.\nUnit tests for PowerShell Writing tests for PowerShell code makes a lot of sense to me. There is something really nice about being able to test all the (covered) functionality of a function or cmdlet, after you’ve fiddled around with it. It’s easy to get lost in fixing a bug, or adding some great new functionality and accidentally break something else. Having tests written ensures the function still works as desired, and as you add functionality or fix bugs you can add to the test cases, making the whole process more robust. I’m 100% for unit and integration testing for code. No doubt about it.\nSo why not databases? Data is always the difficult bit when people talk about DevOps or agile development practices, and to be honest, I’d be way out of my depth to talk about how and why to test your data. I’m instead going to look at using tSQLt to test functions and stored procedures. After all, there’s code in our databases too! After seeing the benefits of writing tests for PowerShell functions, I can easily see the benefits of testing the programmable objects in our databases.\nInstalling tSQLt Getting started with tSQLt is really easy- you download a zip file, unzip the contents and then run the tSQLt.class.sql script against your development database.\nThere are a couple of requirements, including CLR integration must be enabled and your database must be set to trustworthy. These could open up some security considerations, but for my development environment it’s no issue.\nAfter I ran the tSQLt.class.sql script I got the following in the messages tab of SSMS. We’re ready to go:\nInstalled at 2020-04-07 15:45:03.200\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | | | Thank you for using tSQLt. | | | | tSQLt Version: 1.0.5873.27393 | | | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\nCompletion time: 2020-04-11T14:25:21.6912488+01:00\nTest a simple function To start with I made a really simple function to make sure I understood the tSQLt syntax and could get my first test written successfully. It takes two numbers and returns the sum.\nCREATE OR ALTER FUNCTION dbo.AddTwoNumbers (@int1 int, @int2 int) RETURNS INT AS BEGIN RETURN @int1 + @int2 END\nIn tSQLt we’ll use a test class to group similar tests together, which will enable us to run a suite of tests in one go. Since this is for TSQLTuesday I’ll create a test class named testTSQLTuesday.\nEXEC tSQLt.NewTestClass \u0026rsquo;testTSQLTuesday\u0026rsquo;; GO\nThis is basically just a schema for our tests to live in. I’m now ready to create my first test. The documentation for tSQLt has some good examples to get us started, and I followed the ‘AssertEquals’ example from their tutorial.\nWhen we run our tSQLt tests the stored procedure name will be the name of the test, so it’s important to make those meaningful. I named this one ‘test the addTwoNumbers function works’.\nFrom the little I do know about test-driven development, I understand we should write the test to initially fail. That’ll confirm that I haven’t set it up in a way that will provide false positives. In the below test I’m saying that I expect the sum of 1 and 2 calculated by my function to be 4, obviously untrue.\n-- create the test to fail CREATE OR ALTER PROCEDURE testTSQLTuesday.[test the addTwoNumbers function works] AS BEGIN DECLARE @actual INT; DECLARE @testInt1 INT = 1; DECLARE @testInt2 INT = 2;\nSELECT @actual = dbo.AddTwoNumbers(@testInt1, @testInt2); DECLARE @expected INT = 4; EXEC tSQLt.AssertEquals @expected, @actual; END;\nGO\nYou can see this is a pretty simple test to set up. I declared two numbers that I’ll pass to the function and then fill in what I expect the result to be (still the wrong answer at this point).\nTo run the test I’ll use the tSQLt.Run stored procedure, passing in my test class name.\nEXEC tSQLt.Run \u0026rsquo;testTSQLTuesday';\nReviewing the messages pane in SSMS I can see my test has failed, as expected, and it let’s you know it expected 4 but got 3.\n[testTSQLTuesday].[test the addTwoNumbers function works] failed: (Failure) Expected: \u0026lt;4\u0026gt; but was: \u0026lt;3\u0026gt;\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ |Test Execution Summary| +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\n|No|Test Case Name |Dur(ms)|Result | +\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+ |1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 13|Failure| Msg 50000, Level 16, State 10, Line 43 Test Case Summary: 1 test case(s) executed, 0 succeeded, 1 failed, 0 errored. Completion time: 2020-04-11T14:37:18.8297577+01:00\nNow I’ll fix the test so if the function is working as expected the test should pass.\n-- create the test CREATE OR ALTER PROCEDURE testTSQLTuesday.[test the addTwoNumbers function works] AS BEGIN DECLARE @actual INT; DECLARE @testInt1 INT = 1; DECLARE @testInt2 INT = 2;\nSELECT @actual = dbo.AddTwoNumbers(@testInt1, @testInt2); DECLARE @expected INT = (@testInt1 + @testInt2); EXEC tSQLt.AssertEquals @expected, @actual; END;\nGO\nGood news, our first tSQLt test has passed.\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ |Test Execution Summary| +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\n|No|Test Case Name |Dur(ms)|Result | +\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+ |1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 3|Success| Test Case Summary: 1 test case(s) executed, 1 succeeded, 0 failed, 0 errored. Completion time: 2020-04-11T16:33:56.6201835+01:00\nTest a stored procedure that changes data One thing I thought that was really cool with tSQLt is that it executes the tests in a transaction, and then rolls it back after testing to ensure things are left the way they were before we started testing. To show this I’ve created a simple stored procedure that allows us to update an email address in the Person.EmailAddress table from AdventureWorks2017.\nCREATE PROCEDURE Person.UpdateEmailAddress @NewEmailAddress varchar(100), @BusinessEntityID int AS\nUPDATE Person.EmailAddress SET EmailAddress = @NewEmailAddress where BusinessEntityID = @BusinessEntityID\nGO\nThen I wrote a test, similar to the first example, which compares the actual value that is in the Person.EmailAddress table after running the procedure with what I would expect to be in there.\nCREATE OR ALTER PROCEDURE testTSQLTuesday.[test the UpdateEmailAddress procedure] AS BEGIN DECLARE @actual varchar(100); DECLARE @newEmail varchar(100) = \u0026rsquo;test@test.com\u0026rsquo;; DECLARE @businessEntityID INT = 2;\nEXEC person.UpdateEmailAddress @newEmail, @businessEntityId SELECT @actual = EmailAddress FROM Person.EmailAddress WHERE BusinessEntityID = @businessEntityID DECLARE @expected varchar(100) = @newEmail EXEC tSQLt.AssertEquals @expected, @actual; END; GO\nI added this to the same test class so we’ll execute both tests with the same call to tSQLt.Run.\nEXEC tSQLt.Run \u0026rsquo;testTSQLTuesday';\nThe results now show I have two tests that passed.\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ |Test Execution Summary| +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\n|No|Test Case Name |Dur(ms)|Result | +\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+ |1 |[testTSQLTuesday].[test the addTwoNumbers function works]| 6|Success| |2 |[testTSQLTuesday].[test the UpdateEmailAddress procedure]| 7|Success| Test Case Summary: 2 test case(s) executed, 2 succeeded, 0 failed, 0 errored. Completion time: 2020-04-11T17:17:36.4799073+01:00\nTo run this test we had to actually call the stored procedure that changes data in our tables. As I mentioned though, this all happened within a transaction that was never committed. We can confirm our data was unchanged by checking the table.\nSELECT EmailAddress FROM Person.EmailAddress WHERE BusinessEntityID = 2;\nThis is just scratching the surface of what tSQLt can do. The full user guide is available online. This goes through all the available assertions (we only looked at AssertEquals here) as well as more complicated topics such as how to isolate dependencies to be able to effectively unit test.\nSummary Writing this post has been a great excuse for me to try out tSQLt and write my first T-SQL unit tests. To be honest, it was a lot easier than I expected. I will say I’m comfortable writing pester tests for my PowerShell code so that base was helpful, but the documentation for tSQLt made getting setup and starting to test databases pretty straightforward.\nThe cost of unit testing your database code is the time investment to get comfortable with the tools and to write the tests as you develop the database code. The benefit though is a more reliable, easier to maintain database with less bugs, which will in the end make your data safer.\nI’d say there is a significant benefit to applying unit testing to databases, and I believe we’ll see a significant increase in the number of folks applying unit tests to their database environments.\n","date":"2020-04-14T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#125-unit-testing-databases/","title":"T-SQL Tuesday – #125: Unit Testing Databases"},{"content":"I was browsing twitter the other day when a tweet about dbatools caught my eye (I use TweetDeck and so have a column for tweets that contain @PSdbatools).\nhttps://twitter.com/way0utwest/status/1242891971473137666\nA dbatools bug!! Oh no!\nOne of the reasons this caught my eye was that I’ve seen this error in my environment with that same command. I had discounted that it was a bug and figured it was instead something in my environment. I presumed it was something related to the fact I was using containers and Azure Data Studio connections.\nStep one for dbatools bug fixing is to check for an issue on the GitHub repo and create one if there isn’t one already. It turned out that there was one already created so we’re covered there.\nSo I figured I’d take a look and see what was happening and how we could fix it. Now I’m going to be honest with you, my usual method of debugging involves adding Write-Host 'Hi\u0026rsquo;, or piping objects to Out-GridView. I did start down this route, but the Get-DbaRegServer function calls an internal function, and things quickly got complicated.\nLuckily, the PowerShell extension for VSCode includes a debugger so we can level up our game and use that to track down our issues. Since I haven’t already used this for my dbatools folder when I click the ‘Run’ icon on the left navigation bar I see the following:\nPressing the ‘Run and Debug’ button will run your active file and, if you have breakpoints set up, then it’ll break at those points for you to troubleshoot. This is really useful if you have written a script and it’s not working correctly. Since I’m troubleshooting the call of a function I could write a simple script with the code to call the function, save it and then press ‘Run and Debug’. However there is another option, and that is to launch an interactive debugger. Pressing the ‘create a launch.json file’ link opens the command palette with the option to choose your PowerShell debug configuration. Choosing the ‘Interactive Session’ configuration means we can use the integrated console within VSCode to call functions and launch the debugger.\nThis will open a launch.json file that you can edit to add more functionality and customization, but we’ll just save it as is right now.\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026ldquo;version\u0026rdquo;: \u0026ldquo;0.2.0\u0026rdquo;, \u0026ldquo;configurations\u0026rdquo;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;PowerShell: Interactive Session\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;PowerShell\u0026rdquo;, \u0026ldquo;request\u0026rdquo;: \u0026ldquo;launch\u0026rdquo;, \u0026ldquo;cwd\u0026rdquo;: \u0026quot;\u0026quot; } ] }\nAs soon as you save it the left ‘Run and Debug’ pane will change to look like this. Now we’re ready to run the interactive debugger by pressing the green play button or F5.\nSo now that we’re set up, let’s start troubleshooting.\nStep one is to reproduce this issue. This particular bug was easy to reproduce. The only requirements are that you have Azure Data Studio installed and at least one connection set up, then just running Get-DbaRegServer caused the error.\nNext we need to add some breakpoints. These need to be the positions in the code where you want to stop execution and take a look at how things are set in the moment. It’s also a great way to see if you entered certain sections of the code that may be guarded by conditional logic.\nRunning Get-DbaRegServer in the integrated console you can see the error, even down to the line from the function where the error is being thrown. In the screenshot below you can see hovering over that line in VSCode allows you to follow the link to open the function and navigate to the exact line.\nLine 180 of the Get-DbaRegServer is the following:\n$tempserver.ConnectionString = $adsconn.ConnectionString\nWe’ll insert a breakpoint here by clicking in the gutter at line 180.\nNow pressing F5 the interactive debugger will start, and we can rerun Get-DbaRegServer in the interactive console. When we do that as soon as the execution gets to line 180 the code will stop, waiting for us to respond.\nYou can see below that we are able to find the $adsconn variable in the variables pane on the left and see that it’s actually an object with three values – which is the issue here – we’re expecting to only have one returned.\nI read back through the Get-DbaRegServer function to find where the $adsconn variable was set and found it was from calling the internal function Get-ADSConnection. I added in another breakpoint within that function to dig in deeper.\nAdding the breakpoint within the second function means that when we call Get-RegServer and then that calls Get-ADSConnection the code will wait within the second function and allow you to inspect variables within that function.\nThis meant that I was able to determine that there were several connection strings being returned for each server and that we needed to filter down to one.\nChanging line 174 in the Get-DbaRegServer function to include an additional filter, shown below, meant that only one connection string was returned and solved the problem.\n$adsconn = $adsconnection | Where-Object { $_.server -eq $server.Options[\u0026lsquo;server\u0026rsquo;] -and -not $_.database }\nHopefully this walkthrough shows a useful way of using the interactive debugger to hunt down bugs.\n","date":"2020-04-07T00:00:00Z","permalink":"https://jpomfret.github.io/p/interactive-debugging-in-vscode/","title":"Interactive debugging in VSCode"},{"content":"A short while ago (it’s getting further and further away, but let’s stick with short for now) I was a football/soccer player. As with many athletes, I was pretty superstitious as far as my pregame routine. I always felt better going out onto the pitch if everything had gone smoothly as I got ready. I put my boots, shin-pads and socks on in a certain order and even taped my socks up in a certain way. The good news is I’ve managed to find a slightly more reliable way to get ready for my presentations – and I’m going to share the secret.\nFirst you put your right sock on, then your left sock on. Follow that by putting on your right shoe, and then your left shoe… just joking. You use Pester tests!\nIf you don’t know what Pester is, it’s a test framework for PowerShell. In the simplest explanation, using their Domain-Specific Language (DSL) you describe how things should look. If all looks good it returns output in green and if it doesn’t you get red output. There are a lot of great use cases for Pester, like using it to ensure your code does what it’s supposed to, using it to validate your SQL Server environment (dbachecks), or in this example using it to make sure your demos are setup and ready to go.\nWhen I’m preparing for a presentation I go through the demos over and over again, so it’s easy to accidentally leave things in a state that will cause issues when I go to do my demos in the presentation. If you’re creating a table, for example, during the demo and you already created it practicing and then forgot to drop it, the demo gods will strike and it’ll fail when it matters most! A simple Pester test to check whether the table exists will solve this issue.\nSo what do I test?\nLast Wednesday I presented my ‘Life hacks: dbatools Edition’ session for the Southampton Data Platform and Cloud meetup so I’ll talk you through the tests I ran to make sure I was ready to present that session, and it’s a demo heavy one!\nFirst things first, I test that I can import the dbatools module. I make sure I’m getting the version and the number of commands I expect. dbatools puts out new versions all the time, so I usually update this in the weeks leading up to my presentation as I’m practicing.\nDescribe \u0026ldquo;Module is good to go\u0026rdquo; { Context \u0026ldquo;dbatools imports\u0026rdquo; { $null = Import-Module dbatools $module = Get-Module dbatools It \u0026ldquo;Module was imported\u0026rdquo; { $module | Should Not BeNullOrEmpty } It \u0026ldquo;Module version is 1.0.99\u0026rdquo; { $module.Version | Should Be \u0026ldquo;1.0.99\u0026rdquo; } It \u0026ldquo;Module should import 587 commands\u0026rdquo; { (get-command -module dbatools | Measure).Count | Should Be 587 } } }\nMy demo setup involves two containers running on my laptop. Because of that, I’m using the sa credential to connect and I’m setting some PSDefaultParameterValues so I don’t have to include the $credential in every function call. I can test all that is setup correctly like so.\nDescribe \u0026ldquo;Credentials exist\u0026rdquo; { Context \u0026ldquo;Credential exists\u0026rdquo; { It \u0026ldquo;Credential is not null\u0026rdquo; { $credential | Should Not BeNullOrEmpty } } Context \u0026ldquo;username is sa\u0026rdquo; { It \u0026ldquo;Username is sa\u0026rdquo; { $credential.UserName | Should Be \u0026ldquo;sa\u0026rdquo; } } Context \u0026ldquo;PSDefaultParameterValues are set\u0026rdquo; { $params = $PSDefaultParameterValues It \u0026ldquo;PSDefaultParameterValues contains expected values\u0026rdquo; { $params.Keys -contains \u0026lsquo;*:SqlCredential\u0026rsquo; | Should Be True $params.Keys -contains \u0026lsquo;*:SourceSqlCredential\u0026rsquo; | Should Be True $params.Keys -contains \u0026lsquo;*:DestinationCredential\u0026rsquo; | Should Be True $params.Keys -contains \u0026lsquo;*:DestinationSqlCredential\u0026rsquo; | Should Be True } } }\nI then have a couple of simple checks to make sure I can connect to both my instances.\nDescribe \u0026ldquo;Two instances are available\u0026rdquo; { Context \u0026ldquo;Two instances are up\u0026rdquo; { $mssql1 = Connect-DbaInstance -SqlInstance mssql1 $mssql2 = Connect-DbaInstance -SqlInstance mssql2 It \u0026ldquo;mssql1 is available\u0026rdquo; { $mssql1.Name | Should Not BeNullOrEmpty $mssql1.Name | Should Be \u0026lsquo;mssql1\u0026rsquo; } It \u0026ldquo;mssql2 is available\u0026rdquo; { $mssql2.Name | Should Not BeNullOrEmpty $mssql2.Name | Should Be \u0026lsquo;mssql2\u0026rsquo; } } }\nI then make sure that my databases are set up as expected. I am using two databases on my mssql1 SQL Server instance, AdventureWorks2017 and DatabaseAdmin. I make sure each of those exist, are online, and that the compatibility level is set correctly. I also check that the indexes on the Employee table are set up as I expect since I use those in my demos.\nDescribe \u0026ldquo;mssql1 databases are good\u0026rdquo; { Context \u0026ldquo;AdventureWorks2017 is good\u0026rdquo; { $db = Get-DbaDatabase -SqlInstance mssql1 $adventureWorks = $db | where name -eq \u0026lsquo;AdventureWorks2017\u0026rsquo; It \u0026ldquo;AdventureWorks2017 is available\u0026rdquo; { $adventureWorks | Should Not BeNullOrEmpty } It \u0026ldquo;AdventureWorks status is normal\u0026rdquo; { $adventureWorks.Status | Should Be Normal } It \u0026ldquo;AdventureWorks Compat is 140\u0026rdquo; { $adventureWorks.Compatibility | Should Be 140 } } Context \u0026ldquo;Indexes are fixed on HumanResources.Employee (bug)\u0026rdquo; { $empIndexes = (Get-DbaDbTable -SqlInstance mssql1 -Database AdventureWorks2017 -Table Employee).indexes | select name, IsUnique It \u0026ldquo;There are now just two indexes\u0026rdquo; { $empIndexes.Count | Should Be 2 } It \u0026ldquo;There should be no unique indexes\u0026rdquo; { $empIndexes.IsUnique | Should BeFalse } } Context \u0026ldquo;DatabaseAdmin is good\u0026rdquo; { $db = Get-DbaDatabase -SqlInstance mssql1 $DatabaseAdmin = $db | where name -eq \u0026lsquo;DatabaseAdmin\u0026rsquo; It \u0026ldquo;DatabaseAdmin is available\u0026rdquo; { $DatabaseAdmin | Should Not BeNullOrEmpty } It \u0026ldquo;DatabaseAdmin status is normal\u0026rdquo; { $DatabaseAdmin.Status | Should Be Normal } It \u0026ldquo;DatabaseAdmin Compat is 140\u0026rdquo; { $DatabaseAdmin.Compatibility | Should Be 140 } } }\nOne of my demos shows the backup history for AdventureWorks, so I test that with Pester before I start to make sure there is history to show. Nothing worse than getting up to show a wonderful set of dbatools functions and nothing being returned because I haven’t actually taken any backups!\nDescribe \u0026ldquo;Backups worked\u0026rdquo; { Context \u0026ldquo;AdventureWorks was backed up\u0026rdquo; { $instanceSplat = @{ SqlInstance = \u0026lsquo;mssql1\u0026rsquo; } It \u0026ldquo;AdventureWorks has backup history\u0026rdquo; { Get-DbaDbBackupHistory @instanceSplat | Should Not BeNullOrEmpty } } }\nWhile I was writing my demos I came across issues where my PowerShell environment was set to x86 so I added a test for that to make sure it doesn’t happen again.\nDescribe \u0026ldquo;Proc architecture is x64\u0026rdquo; { Context \u0026ldquo;Proc arch is good\u0026rdquo; { It \u0026ldquo;env:processor_architecture should be AMD64\u0026rdquo; { $env:PROCESSOR_ARCHITECTURE | Should Be \u0026ldquo;AMD64\u0026rdquo; } } }\nFinally, I check to see what’s running on my computer. Zoomit, everyone’s favourite screen zoom tool should be running, and we should make sure that Slack and Teams are not.\nDescribe \u0026ldquo;Check what\u0026rsquo;s running\u0026rdquo; { $processes = Get-Process zoomit*, teams, slack -ErrorAction SilentlyContinue Context \u0026ldquo;ZoomIt is running\u0026rdquo; { It \u0026ldquo;ZoomIt64 is running\u0026rdquo; { ($processes | Where-Object ProcessName -eq \u0026lsquo;Zoomit64\u0026rsquo;) | Should Not BeNullOrEmpty } It \u0026ldquo;Slack is not running\u0026rdquo; { ($processes | Where-Object ProcessName -eq \u0026lsquo;Slack\u0026rsquo;) | Should BeNullOrEmpty } It \u0026ldquo;Teams is not running\u0026rdquo; { ($processes | Where-Object ProcessName -eq \u0026lsquo;Teams\u0026rsquo;) | Should BeNullOrEmpty } } }\nNow there are obviously ways that the demo gods can still strike, but using Pester to test your demos is a great way to try and tilt the odds in your favour.\nYou can view all the code, including the tests, for this presentation on my Github.\nHere\u0026rsquo;s what the output looks like:\nExecuting all tests in \u0026lsquo;.\\Tests\\demo.tests.ps1\u0026rsquo;\nExecuting script .\\Tests\\demo.tests.ps1\nDescribing Module is good to go\nContext dbatools imports \\[+\\] Module was imported 1.59s \\[+\\] Module version is 1.0.99 287ms \\[+\\] Module should import 587 commands 162ms Describing Credentials exist\nContext Credential exists \\[+\\] Credential is not null 263ms Context username is sa \\[+\\] Username is sa 83ms Context PSDefaultParameterValues are set \\[+\\] PSDefaultParameterValues contains expected values 80ms Describing Two instances are available\nContext Two instances are up \\[+\\] mssql1 is available 592ms \\[+\\] mssql2 is available 26ms Describing mssql1 databases are good\nContext AdventureWorks2017 is good \\[+\\] AdventureWorks2017 is available 863ms \\[+\\] AdventureWorks status is normal 32ms \\[+\\] AdventureWorks Compat is 140 46ms Context Indexes are fixed on HumanResources.Employee (bug) \\[+\\] There are now just two indexes 1.53s \\[+\\] There should be no unique indexes 49ms Context DatabaseAdmin is good \\[+\\] DatabaseAdmin is available 256ms \\[+\\] DatabaseAdmin status is normal 15ms \\[+\\] DatabaseAdmin Compat is 140 17ms Describing Backups worked\nContext AdventureWorks was backed up \\[+\\] AdventureWorks has backup history 627ms Describing Proc architecture is x64\nContext Proc arch is good \\[+\\] env:processor\\_architecture should be AMD64 125ms Describing Check what\u0026rsquo;s running\nContext ZoomIt is running \\[+\\] ZoomIt64 is running 150ms \\[+\\] Slack is not running 43ms \\[+\\] Teams is not running 17ms Tests completed in 6.86s Tests Passed: 21, Failed: 0, Skipped: 0, Pending: 0, Inconclusive: 0\nGood news, all passed and I\u0026rsquo;m ready to give my demos!\n","date":"2020-03-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/keeping-the-demo-gods-at-bay-with-pester/","title":"Keeping the demo gods at bay with Pester"},{"content":"I’ve recently found myself in a few situations where I’ve needed a certain lab setup to test something. Most scenarios these days I find myself firing up some docker containers to run demos or tests against. However, there are a few circumstances where you may need a little more than that, perhaps to test something Windows OS related (my container setup is using Linux) or just to replicate a production environment more closely than you can with containers. In these situations I turn to PowerShell.\nThere is a fantastic PowerShell module called AutomatedLab that can enable you to easily build out a lab for the specific scenario you need to test. Even better is the module comes with 70 sample scripts that you can start with and adapt to meet your needs.\nThe module gives you the option to work with Hyper-V or VMWare. I will say most of the examples are using Hyper-V, and that is what I’ll be using also.\nFor my lab I want a SQL Server 2019 instance joined to a domain, and a separate client machine that I can manage the SQL Server from. On the client I would need to be able to connect to the internet as I want to be able to download PowerShell modules from the gallery easily.\nStep 1: Get the module There is some setup involved to make sure the pieces needed to build your lab are in the right places. AutomatedLab gives you two options to ensure you’re set up correctly. The documentation on GitHub already does a great job of outlining these so I’ll just point you in that direction for reference.\nIn my lab I used the second method to download from the PowerShell Gallery and then run the configuration command. I ran the following to get my laptop setup:\nInstall-Module -Name AutomatedLab -AllowClobber New-LabSourcesFolder\nStep 2: Stage the ISOs In order to build our lab we will need to store the ISOs needed for installation where AutomatedLab can access them. We used the defaults during our install so that location is C:\\LabSources\\ISOs. You can see below I’ve placed the ISOs for Windows Server 2019, SQL Server 2016 and SQL Server 2019 in this folder.\nYou can test whether AutomatedLab can see the operating system ISOs by using the Get-AvailableOperatingSystems command. You can see below it has found my ISO folder and therefore lists the available operating systems that I can use. You’ll notice that the Windows Server 2019 ISO allows me to install two editions, Standard or Datacenter, as well as choosing between installing the core version or the more traditional desktop experience. It is important to ensure you have the appropriate licensing in place for the OS you install. You can use evaluation editions or enter product keys at a later time.\nStep 3: Craft the lab setup script Now this sounds like a scary step, but fear not! I mentioned before the AutomatedLab team have included over 70 sample scripts, which means it’s likely there is something similar already written. I found that there were two samples that covered all the parts needed for my lab, so I was able to combine them to create the exact scenario needed.\nI’ve added comments throughout the script below so you can easily see the pieces needed:\nhttps://gist.github.com/jpomfret/15dd515bfd421ae13c047f95d65eb333\nBy default, if you don’t specify a the -VmPath parameter on the New-LabDefinition function AutomatedLab will test the speed of the available drives and choose the fastest.\nStep 4: Install the lab With the script written, open the PowerShell console using ‘Run as Administrator’ and execute the script. .\\AutomatedLab_SQLServer.ps1\nThere will also be some security settings to consider and accept for the first install such as enabling WinRM and adding ‘*’ to your TrustedHosts. Once those are confirmed and the prework is complete, a toast will pop up showing that the lab has started building:\nThis is the perfect time to grab a cup of coffee as there are some steps that can take a while, especially if this is the first install.\nAutomatedLab uses a base disk for installations of the same OS. Creating this base disk can take a while but once it is created it significantly reduces the amount of disk space you need for your lab. If you deploy 4 VMs with the same OS, as I have here, the base disk will be created and then differential disks can be used for each VM.\nIf you build a second lab in the same VM folder with the same OS it can again reuse this base disk.\nStep 5: Use the lab Once the install is complete, another toast will appear and it’s time to log in and start using your lab.\nThe summary we displayed using Show-LabDeploymentSummary will show the machine names, administrator password and some other connection information. We have again used the defaults for these settings but they can be overwritten.\nRemove the lab Once you are done with your lab you can quickly and easily run the following to destroy all the pieces created by your lab (except the base OS disk).\nRemove-Lab SQLLab\n","date":"2020-03-03T00:00:00Z","permalink":"https://jpomfret.github.io/p/using-automatedlab-to-setup-a-sql-server-lab/","title":"Using AutomatedLab to setup a SQL Server Lab"},{"content":"I have just a quick tip for you today using the BurntToast module to notify us when a backup is complete. As DBAs there is always plenty to do, so we don’t want to have to sit and watch a long running script to catch the moment when it finishes. Usually what happens to me is I kick off the script, move on to something else and then totally forget about it, perhaps until someone asks if it’s done yet. Oops. Well this tip will help avoid that.\nThe BurntToast module, created by Josh King (b|t), allows you to easily add Windows toast notifications to your PowerShell scripts. I’m going to show you how to use BurntToast to keep track of a database backup.\nBy this time you should know about my love for dbatools, so today we’re going to take a look at how to take a copy-only backup. This is a backup that doesn’t upset the LSN chain of your regular database backups and can be used to just save a specific point in time for perhaps pre-upgrade, or to restore the database somewhere else.\nThe following script will take a copy-only backup of the AdventureWorks2019 database on the mssql2 instance. Since I’m not specifying a path it will go to the default backup directory defined for that instance. I’m also saving the results of the command to the $backup variable.\nThe second section below that will run directly after the backup completes will use the results in $backup to notify us using BurntToast:\n## Take a copy only backup, using splatting for readability $backupSplat = @{ SqlInstance = \u0026ldquo;mssql2\u0026rdquo; Database = \u0026ldquo;AdventureWorks2019\u0026rdquo; CopyOnly = $true } $backup = Backup-DbaDatabase @backupSplat\nNotify the backup is complete, using splatting for readability toastSplat = @{ Text = (\u0026ldquo;Backup of {0} completed in {1}\u0026rdquo; -f $backup.Database, $backup.Duration.ToString()) AppLogo = \u0026ldquo;C:\\temp\\dbatools.png\u0026rdquo; } New-BurntToastNotification @toastSplat\nThat’s it, 2 commands to take a backup and notify us on completion. I’ve also used the -AppLogo parameter to add the dbatools logo. You can see that this backup only took 1 second to complete (hopefully I didn’t get sidetracked in that time) but if the backup takes a few minutes or longer this is a useful tip to let you know when it’s finished.\nYou could also add some error handling to this script to make it a little more robust by perhaps using try-catch to change the message if the backup was unsuccessful.\nThis is just one use case for using BurntToast with dbatools You could use this in any script that you’re writing to keep you notified when it’s done. This will allow you to get on with whatever else you have on your plate and not have to worry about remembering that backup you kicked off a while ago.\nSince writing this post I saw that Josh has created the PoshNotify module which allows you to generate popups cross-platform. So if you are not using Windows you can adapt the above script to use this module instead.\n","date":"2020-02-25T00:00:00Z","permalink":"https://jpomfret.github.io/p/backups-with-dbatools-burnttoast/","title":"Backups with dbatools \u0026 BurntToast"},{"content":"\nThere was an amazing response to my TSQL2sday prompt for February 2020 where I encouraged folks to share their life hacks. So firstly, thanks to everyone who participated.\nA lot of people had more than one life hack so I recommend reading all of the posts linked to below. For this summary post I’ve tried to pick one or two hacks from each post and group them into logical buckets.\nThere are several hacks shared that I plan on integrating into my life, and I hope this post will serve as a good reference for us all going forward.\nIf I missed any posts please let me know!\nSQL Server Management Studio Chris, a first time TSQL2sday poster (welcome!), talks about adding shortcuts to run common queries, for example selecting the top 100 rows, or getting the number of rows in a table. Jason has three life hacks for us, my favourite being the scroll bar map in SSMS. I use that a lot in VSCode, and had no idea you could do that in SSMS as well. Kenneth’s hack is to configure your tools and shows some great tips for making SQL Server Management Studio work for you. Kevin talks us through executing sp_whoisactive as a temporary procedure to leave no trace and using SSMS registered servers. Taiob has some great tips to make using SSMS more effective, including adding your own keyboard shortcuts. Tim shares a few hacks including multiline editing in SSMS, and a homemade bread proofing box. T-SQL/PowerShell James has a hack for using windowing functions in T-SQL to aggregate aggregates and a really well laid out post to explain that concept. Lisa has a couple of stored procedures to add to your toolbelt. One that recursively finds objects your code calls, and one that recursively finds code that calls your objects. Sander’s life hack is to make the most out of your PowerShell profile, and discusses the contents of his profile. Keyboard/word Shortcuts Cathrine has some great keyboard shortcuts to make us more productive, showing us how to move both lines of text and windows around. Martin levels up from using keyboard shortcuts to keyWORD shortcuts, including some to insert useful code snippets into SSMS for tasks he completes often. Organisation/Time management Andy has several great life hacks which he mentions are also habits at this point. He includes using cloud storage to sync up your documents and VSCode extensions across computers. Aaron has a ton of little life hacks built into his daily schedule while he works from home. My favourites are a scheduled hour of ‘disengagement’ time to focus on non-collaborative work and his travel pack of gear. Brent suggests using a (half) hour glass to assess progress made and keep him on track. He also links to a beautiful wallet damaging hourglass. Drew highlights the Eisenhower matrix to improve delegation, and then goes on to list several more great productivity tips. Also bonus points for including a February fact! Mala has two great tips for us, first to get the benefits of both working at home and in the office. Secondly, a way to squeeze in some learning time. Shane’s life hack is not just to use the Pomodoro technique, which is pretty manual. He’s written a PowerShell script for it complete with toast notifications. Tracey has a host of useful life hacks including queuing Ola’s index maintenance scripts and using a real paper planner for goals. Tracey also reveals she is the host for next month’s TSQL2sday! Tools/Apps Eitan talks about moving from an individual contributor to a manager and now utilising a bunch of “digital assistants” to get his menial tasks done. Elizabeth’s life hack helped her to get her book written (I saw it just came out!). She’s been using dictation software to get the ideas out of her head and onto paper. Erin has two life hacks for us. First, using the Toggl app to track your time. The second is listening to movies in the background while working to improve focus. Kevin shares several tools he likes to use including Glenn Berry’s diagnostic scripts especially for finding indexes, dbatools and some other free community tools. Plus wearing jackets with lots of pockets for short flights. Mike, another first time TSQL2sday poster (welcome to you too!), is using Apple’s ‘Speak Screen’ option to catch up on his backlog of blogs to read. Personal/Health Allen has a great life hack for dealing with people. Inspired by a role he played, Will Parker, he tries to be friendly and open to everyone. Mikey has a simple but effective hack- get involved with #SQLFamily. He has a good list on where to find people. Rich has a life hack that I can relate to, getting your exercise in, and I’d agree it’s a great life hack both physically and mentally. Rob talks about a shortcut from his childhood French lessons to handle pronouns, and goes on to talk about the importance of us all doing better with each individual’s pronouns after this past week\u0026rsquo;s SQLFamily discussions. Todd’s life hack is to use virtual reality in the morning to get into the flow zone before he starts his day. The videos shared in this post were amazing, and I’ve added the book he references to my to-read list. ","date":"2020-02-18T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#123-summary-of-life-hacks/","title":"T-SQL Tuesday #123: Summary of Life Hacks"},{"content":"\nIt’s time for the February edition of T-SQL Tuesday. I am really grateful to be able to host this edition and look forward to reading everyone’s contributions. In case you are new to T-SQL Tuesday this is the monthly blog party started by Adam Machanic (b|t) and now hosted by Steve Jones (b|t). It’s a way of encouraging blog posts from the community and helping to share the knowledge.\nSo here we are, the first Tuesday of February. I personally always find February to be the month where my motivation is a little low. I live in the northern hemisphere so it can be a pretty dreary winter month where it still feels like there is a long way to spring (I will say this January I moved from Ohio back to England and the distinct lack of piles of snow is helping this cause somewhat). This makes my topic even more relevant as we need a little extra help to be productive and get through the month.\nMy topic is looking for your favourite ‘life hack’, something you use to make your day easier. This could be anything from a keyboard shortcut in SSMS that runs ‘sp_whoisactive’, to a technique you use to get and stay organised. It doesn’t have to be directly related to a technology, just whatever you use to make your life easier.\nNow, I’m personally a huge proponent of using keyboard shortcuts to get things done faster. In the last year or so I’ve started using Visual Studio Code as my editor of choice and the number of little ‘life hacks’ I’ve found has grown incredibly. I’m going to share a couple that I use often to get your ideas flowing.\nMultiline Select - Ctrl + Alt+ Direction Key This is something I love for formatting queries, among other things. I know you can use T-SQL to generate some queries from the metadata but if you have a list of tables you want to truncate, for example, you can easily accomplish this. Select the start of each line by using Ctrl + Alt + down direction key, add the TRUNCATE TABLE text and then press end to get to the end of each line, no matter the length, to add the semicolon.\nThe other use I have for this hack is to generate names and descriptions of Active Directory groups for tickets to have them created. At my previous job we created read and admin groups for databases that users could then request access to. Multiline select made this really easy to generate the required information.\nYou can use multiline select at the beginning of the row. Start by selecting the first word and copying it (Ctrl+C), then you can type to format your group name. For example, I put SqlDb- before the database name and then -Read afterwards. Pressing enter at the end of the group name will create a second line for all three groups where you can add the description. Notice I can now use paste (Ctrl+V) to add the database name that we copied from each line.\nThis ability to change multiple lines at once is really powerful and once you get the hang of what you can do with it you’ll find so many opportunities.\nChange all occurrences – Ctrl + F2 A similar hack to my first, VS Code also lets you change multiple occurrences of characters. I say characters because you can select whole words, parts of words, or even punctuation. This is really handy, for example, for formatting a comma separated list on one row into a list with each value on a separate row.\nCarrying on from my previous example, now that we have formatted the group names and description. I can select the word ‘Read’ and replace all with ‘Admin’. Just like that I have all I need to get the group request off to the help desk for creation.\nCommand Palette - F1 or Ctrl+Shift+P VS Code also has a really great Command Palette that offers a lot more for you to explore. A few of my favourites are:\n- Sort Lines Ascending/Descending – Select some lines in VS Code and easily alphabetise them.\n- Git: Undo Last Commit – Rescue that last commit back from your source control. Useful if you realised a second too late you committed to the wrong branch.\n- File: Compare Active File With – This clearly highlights differences between two files.\nOver to you I hope my VS Code life hacks have got your ideas flowing, so now it’s over to you. Finally, the important part, the rules. You can read the full rules here, but the keys are:\n- Write about the topic described above\n- Include the T-SQL Tuesday Logo and link back to this post\n- Comment on this post so I make sure not to miss your contribution\n- Post your blog on February 11th between 0:00 - 23:59 UTC\nIf you have an idea for a future T-SQL Tuesday you can contact Steve Jones.\nBonus February Fact Just in case anyone is still reading, while I was looking for a nice way to tie my topic to the month of February I discovered that one of the old English names for this month was Kale-monath, which means “cabbage month.” As if February couldn’t get any worse! This doesn’t really tie my topic and February together, just a useless fact to add to your collection. You’re welcome!\n","date":"2020-02-04T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#123-life-hacks-to-make-your-day-easier/","title":"T-SQL Tuesday #123: Life hacks to make your day easier"},{"content":"\nWhat a great topic for the start of the year as we’re all thinking about goal setting and personal improvement. Thanks to Jon Shaulis (b|t) for hosting this first monthly blog party of the decade.\nImposter syndrome is mentioned so often in our community, and it’s really interesting to me the wide number of people, who I consider experts, that say they still feel this sometimes. I’m really excited to read about how other people combat this feeling and hopefully pick up some tips for when it creeps into my head.\nI’ve only been speaking and blogging for a couple of years now and I’ve found myself most often feeling like an imposter as I’m getting ready for a presentation. The scenario I imagine the most is that an attendee asks a question that I don’t have an answer for, or even worse points out a mistake I made or something I said that isn’t true.\nTo combat these feelings I try and prepare as fully as I can for my presentation, including reading and re-reading any Microsoft Docs on the topic. Also if others in the community have written blogs on the topic I try and read those too. To be honest though, I learn best by doing so as I’m getting ready for a presentation I make sure I work through my demos and fully understand what is happening in a lab environment.\nOn the other side, I have to accept that there could be questions that I can’t answer, and instead of stuttering and panicking I just say something along the lines of, ‘Great question. I’ve got no idea, but I’ll find out and let you know’. First, this is a great way to generate content for blog posts. Second – you’ll gain the respect from the audience that you didn’t just try and make something up on the spot.\nHopefully as I speak more and gain confidence in my ability these feelings will subside. However, it’s also possible it’ll always be in the back of my mind. Perhaps it’s a good thing- if I didn’t experience this maybe I wouldn’t be as well prepared for presentations and I’d do the audience a disservice by being overconfident in my ability.\n","date":"2020-01-14T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#122-imposter-syndrome/","title":"T-SQL Tuesday #122 – Imposter Syndrome"},{"content":"\nThis is a time for material gift giving, for many of us. It might also be a time to consider the many gifts we have received through the year, and perhaps use this opportunity to appreciate people or situations that we were blessed with.\nWhen I first saw Mala’s post last week for the last T-SQL Tuesday of the year I immediately started thinking of all the gifts I have received this year and how thankful I was with how 2019 has turned out. So first things first, thanks to Mala (b|t) for hosting this final prompt of 2019.\nThe gift of #SQLFamily Around this time I found my local user group in Cleveland and started attending meetings. Attending meetings, talking to no one and going home, but still learning a lot from the great speakers and topics that were shared. This really started my journey as I started to interact with the community.\nI was also starting to follow members of the SQL Server community on Twitter. I occasionally used the #SQLHelp hashtag to get some guidance, but mostly played the role of lurker. I’d read articles that were shared, learning plenty, but not fully realizing the potential here.\nIn 2015 I changed jobs, which meant in 2016 I had the chance to attend my first PASS Summit. What an experience. It was like our local user group on steroids, all day, for 3 whole days. I was exhausted by the end of the week and that was just from the conference portion as I mostly kept to myself during the evenings.\nAs time went on I still attended my local user group almost every month. I started talking to the people there more and more and began to understand this concept of #SQLFamily that I had heard mentioned so many times before.\nMy friends at the user group continually provided support, guidance, and encouragement and eventually I started to believe I could give something back to the community. I started this blog in February 2018 and gave my first user group presentation in June 2018. This is when I really started to believe in the magic of the #SQLFamily.\nSince that slightly shaky presentation on Data Compression in June 2018, I’ve presented 3 different sessions a total of 14 times at various user groups, SQL Saturdays and conferences. Although I still feel like I have a long way to go, it was really great to present back to my local user group this last month to show them I’m getting better and their support is working!\nI will also give a special shout out to the Grillen guys. In June 2019 I was lucky enough to present my DSC \u0026amp; SQL Server presentation at DataGrillen. This was my first big conference- I had a few SQL Saturdays under my belt but this felt huge. William (b|t) and Ben (b|t) were the most amazing hosts, not only having created an amazing space to present and learn in, but also ensuring both myself and my wife had the best trip possible. I highly recommend checking this conference out if you can. The good news is, at this time, the call for speakers for 2020 is still open for 5 more days!\nWe are so lucky to live and work in a community that has this concept of #SQLFamily. It took me a while to see it but now I can truly appreciate it and I can honestly say I wouldn’t be where I am today without every one of you!\nThe gift of supportive family \u0026amp; friends The second gift I will talk about is I have the most supportive friends and family. Thanks to the #SQLFamily, I have met and made connections with folks from all over the world. Thanks to Twitter, I randomly stumbled across this tweet one afternoon:\nhttps://twitter.com/SQLDiplomat/status/1161342206675955713\nI’m originally from Chippenham in the South West of England (about 1.5 hours from Southampton), and my wife, who’s American, wasn’t opposed to living in England at some point in our lives. Although it wasn’t in the immediate plan we decided to find out more and it turned out this opportunity was too good to miss. I’m really looking forward to 2020 and this new opportunity.\nI’m so excited to move back to England, to be closer to my family, and explore the UK and Europe with my wife. With the excitement comes absolute chaos as we prepare to move the 3,500 miles across the Atlantic. We’re currently working through selling our home, cars, things and packing up our lives in the US. We are very lucky to have the most supportive family on both sides of the pond as well as so many amazing friends that, although are sad to see us move, are being amazing at helping us realize (I guess it’s realise now!) this dream.\nLiterally carrying me into 2020!\nThe final shoutout is to my wife, who to be fair has the rougher deal here. I am moving towards my family, with a job lined up that I’m excited about. She is stepping into a world of unknown, leaving her family and her dream job in the US for an adventure in England. You wouldn’t know this though, she’s the one keeping us organized and on track while I write blog posts to avoid packing ?. Her support and sense of adventure is the gift I’m most thankful for, as I know I couldn’t have accomplished half the things I did this year without her in my corner.\nI’ve had a great 2019. It is ending in no way how I expected, and that’s largely due to the amazing gifts I’ve received from #SQLFamily and my own friends and family. So a big thank you to everyone involved!\nI hope everyone has a safe and enjoyable holiday season. Thanks again Mala for hosting!\n","date":"2019-12-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#121-gifts-received-for-this-year/","title":"T-SQL Tuesday #121 - Gifts received for this year"},{"content":"I’ve been thinking about the cloud a lot lately, and I feel it’s an area that I would benefit from learning more about. I’ve attended a couple of presentations on SQL Managed Instances and have read enough to be dangerous (or accidentally spend a lot of money, one of my biggest fears when working in the cloud). However, I always find I learn best and really get to understanding a topic by building something.\nThis post will be the first in at least a two part series on SQL Managed Instances (MI). My goal in this post is just to deploy an MI and have it ready to use for my next post.\nI’ve chosen to start with MI as it feels like the easiest route to the cloud. Microsoft advertises an easy ‘Lift \u0026amp; Shift’ experience to move your on-premise applications to the cloud and offers “near 100% compatibility with on-premise”. Along with these you still get the existing benefits from using a SQL Database (PaaS) with reduced administration and management needs.\nYou can read more about the specifics of “Azure SQL Database managed instance”, including pricing, in the Microsoft docs.\nCreating a SQL managed instance Step one is to head to the portal (https://portal.azure.com) and get logged in. From there you can search ‘SQL’ and then select ‘SQL managed instances’.\nOn the next pane you’ll see any existing managed instances you have setup. I have none at this point so I’ll select the ‘+Add’ button on the toolbar to start creating one (One thing to note in this blog post, Azure changes often and quickly so these screenshots and steps might not be exact by the time you get to reading this).\nBasics The creation wizard takes you through several tabs. The first ‘Basics’ is where you’ll create a resource group (to group like resources and easily manage them), name your server, size your server, and setup the administrator account (cut off in the below screenshot). Your server name can’t contain reserved words, and also can only contain lowercase letters, numbers and a hyphen.\nTo size your managed instance you’ll need to choose between the two tier options. Business critical has faster disks (local SSD) for high I/O workloads and built in high availability with Always On. For today we’re going to stick with a general purpose instance.\nAn interesting note here is that I chose the smallest managed instance I could, and if I selected the same in different regions the price estimation was slightly different.\nNetworking On the networking tab you will create a virtual network for your managed instance to live in. You can also define the connection type (proxy or redirect) and enable a public endpoint. If you enable the public endpoint your managed instance will be available over the internet, which adds some security considerations that should be fully understood before simply enabling the option.\nAdditional Settings The third and final settings tab allows you to configure your instance collation and time zone. The default time zone is UTC so you might want to change that to match your usual time zone for servers you build on premise.\nReview \u0026amp; Create The final tab reviews your settings to ensure they are all valid and allows you to create your managed instance. Depending on whether this also requires changes to the underlying cluster that your managed instance will be deployed on top of, this operation could take a while. Since this is my first managed instance I don’t have an existing cluster so Azure warns me this could take up to 6 hours to complete.\nUntil Next Time It’s really quite simple to get a managed instance created. There are not too many decisions to make along the way. I imagine the most important part is to understand the networking aspect to ensure we’re going to be able to securely connect to the database from the application.\nOnce my underlying cluster is created and my managed instance is ready to go the next step will be to migrate some databases from my on-premise lab up into the cloud.\n","date":"2019-11-04T00:00:00Z","permalink":"https://jpomfret.github.io/p/creating-my-first-sql-managed-instance/","title":"Creating my first SQL Managed Instance"},{"content":"Today, October 11th is National Coming Out Day in the United States. This (2019) will be the 31st anniversary of its founding by Robert Eichberg and Jean O\u0026rsquo;Leary. The idea was to raise awareness and support for the LGBT community. If you know someone who identifies as LGBT you are more likely to support equal rights.\nMost people think they don\u0026rsquo;t know anyone gay or lesbian, and in fact, everybody does. It is imperative that we come out and let people know who we are and disabuse them of their fears and stereotypes.\n– Robert Eichberg, in 1993[3]\nWhen I first heard about National Coming Out Day I was in college and I didn’t really get it. Why did we need a day to celebrate coming out? Can’t people just do it when they’re ready, or not? Shout out to the #tsql2sday prompt from this month, which was about changing your opinion on something - I’ve changed my mind on this too.\nWhy is National Coming Out Day Important? Firstly, for yourself – trying to hide a part of who you are is exhausting. Trying to constantly cover your tracks or not talking about the great weekend you had with your partner or LGBT friends makes you feel guarded and unwilling to fully integrate into conversations.\nA friend once told me about a diversity and inclusion exercise where they go around the group and have to talk about their weekend without revealing their partners gender, which is harder than you might think – and that was just one conversation. Now try doing it all day! Secondly, for everyone behind you. This is the magic if you ask me. The more people that are out the more it becomes normal. As we already mentioned, if you know someone who is LGBT, or really of any diverse group, you are much more likely to try to understand and to be cognizant of things that affect them.\nIt also creates role models for the next generation. Usually when we mention role models we think of actors/actresses, sports stars and celebrities. When there are LGBT people in the limelight it’s exciting, it’s cool to know \u0026ldquo;hey that person is like me, maybe this will be alright\u0026rdquo;. I’d like to argue the more important role models are the regular people like you and me. Just going about our daily lives, trying to adult.\nThis isn’t about my personal coming out story. I am not looking for praise on coming out or being out. This is just about my regular, everyday life. I don’t hide who I am, what you get all the time is me, a slightly socially awkward lesbian IT nerd with a great sense of humour (depending on who you ask, but I like to think so). I got married almost a year ago now. We both have jobs we enjoy, we bought a house, we have two cats. It’s all pretty ordinary. – my hope is that by making it normal it’ll be easier for the next group of people coming behind us. Anyone who feels like they don’t fit in for whatever reason, or has feelings they don’t understand. can look around and see people like them, making it.\nOne of the posts that came out of #tsql2sday prompt mentioned this exact idea, “Representation matters”. First go queue this link up to read next, it’s excellent. “What Convinced me that diversity is important”. In the post Eugene talks about his own type of diversity, and how his life was impacted by seeing successful people like him. This simple act of living your life openly really can make a difference for people.\nNow this is not an encouragement for everyone currently in the closet to just come out and join the party. It’s a big step and it’s not all unicorns and rainbows out here. It’s vital that you are safe and have the support you need. At the end of the day, you do you, because every situation is different. If you need someone in your corner feel free to reach out to me. I’m definitely no expert but I’ll do my best.\nFinally, if you feel so inclined head over to http://amtwo.lgbt and take a look at my good friend Andy’s post. He’s currently raising money for The Trevor Project, an organization whose mission is to end suicide among gay, lesbian, bisexual, transgender, queer \u0026amp; questioning young people. Also, he has the “Unofficial Queer Guide to PASS Summit”, if you’re heading there next month.\nThanks for reading – and happy National Coming Out Day!\n","date":"2019-10-11T00:00:00Z","permalink":"https://jpomfret.github.io/p/national-coming-out-day/","title":"National Coming Out Day"},{"content":"\nIt’s time for our monthly T-SQL Tuesday blog party again and we have another interesting prompt to write about. Thanks goes to Alex Yates (b|t) for hosting this month.\nI’ve thought about this a lot over the last few days and really wanted to find a technical topic to write about, but over and over I came back to the same thoughts. This last week I attended two conferences and gave three presentations (one topic twice), and I still can’t believe I’ve given a single technical presentation.\nAllen White (b|t) has a famous spiel that he gives before every one of his presentations, encouraging his audience that everyone has something they can teach others. I created my PASS profile in 2011, and that was about the time I first heard Allen present. I remember laughing to myself that I would never be able to do that. Giving presentations during my college years was something I dreaded, why on earth would I volunteer now to do that.\nWell, fast forward to June 2018, I gave my first user group presentation. I stepped way out of my comfort zone for that, but since then I have presented 11 additional times. I’ve presented at a mixture of SQL Saturdays, online virtual groups and conferences, and every time I’ve got extremely nervous beforehand, and experienced an unbelievable high after.\nThis past week I gave my DSC \u0026amp; SQL Server presentation on Friday in Columbus at DogFoodCon (a great conference with a very different crowd than my usual data platform events) and then I gave two sessions at SQL Saturday Pittsburgh.\nAt this point I still feel like I’m not really a speaker, every time I walk into the speaker room at an event I expect someone to ask what I’m doing in there. However, I’ve received a lot of great feedback, and more importantly had a lot of great conversations with people about things they learnt in my session. I’m getting there.\nThe other amazing part of this adventure is I’ve had so much fun, I’ve met and become friends with some great people and I’ve traveled as far as Germany to explore the world and deliver my sessions.\nI can 100% say I’ve changed my opinion on whether I can become a speaker in this community, and I’ve bought into Allen’s spiel that everyone has something to give back. If you are reading this and laughing to yourself that it can’t mean you, it does! Maybe you’re not ready to speak, but start a blog and share some things you’ve done – other people are one month behind you, trying to work out how to do what you just did. If writing isn’t your style, go contribute some code to open source software- dbatools and dbachecks are on GitHub and always looking for new contributors. Go squash some bugs or add new functionality.\nThanks again to Alex for the great prompt.\n","date":"2019-10-08T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#119-changing-your-mind/","title":"T-SQL Tuesday #119 - Changing your mind"},{"content":"\nThis month our T-SQL Tuesday host, Kevin Chant (b|t), has challenged us to propose a new fantasy feature for SQL Server. Firstly, thanks to Kevin for hosting this month’s blog party.\nWhen I first read the invitation I didn’t have any ideas jump straight into my mind. Since then, I’ve come up with two. I’m going to share both since they are for different areas… cheating I know ?.\nLIKEIN Operator First up, I am proposing the introduction of the LIKEIN operator for T-SQL queries. It would be useful to be able to combine the functionality of the LIKE and IN operators. To be able to select multiple strings that include wildcards, without having to use multiple OR clauses would be a great addition to our T-SQL tool belts.\nMy example comes from the AdventureWorks2017 database where we need to select the addresses we have listed within two regions. In the United Kingdom postal codes are split into two parts. For example, the postcode for Wembley Stadium is HA9 0WS (Yes, I’m currently watching the England Vs Bulgaria game). The first half, HA9, is known as the outward code and can be 3 or 4 digits long. This specifies the postcode area and district. The second half of the postcode helps to identify a specific address. Using my new LIKEIN operator this can easily be accomplished:\nselect AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode from Person.Address where PostalCode likein (\u0026lsquo;RG41%\u0026rsquo;, \u0026lsquo;V9B%\u0026rsquo;)\nSince we don’t currently have this functionality I’ll suggest a few other options.\nFirst, as mentioned above we could use multiple OR clauses. The main downside to this option is as soon as we add OR into our WHERE clause we lose the ability to do index seeks and instead are forced to scan the entire index. Hopefully with the implementation of LIKEIN we would still be able to use index seeks, as long as the string doesn’t start with a wildcard.\nselect AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode from Person.Address where PostalCode like \u0026lsquo;RG41%\u0026rsquo; or PostalCode like \u0026lsquo;V9B%\u0026rsquo;\nWe could also use the substring and charindex functions to match just the first part of PostalCode, before the space with our two desired areas. This also removes our ability to use index seeks. Anytime you add a function onto the right side of the WHERE clause you are forced to scan the whole index or table.\nselect AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode from Person.Address where substring(PostalCode,0,charindex(\u0026rsquo; \u0026lsquo;,PostalCode,0)) in(\u0026lsquo;RG41\u0026rsquo;,\u0026lsquo;V9B\u0026rsquo;)\nFinally, here’s an option that allows us to effectively use our index. We could use two separate queries and combine them with UNION ALL. This results in two seeks and then a concatenation operator in the execution plan to combine the results. Also, since these result sets are not overlapping, UNION ALL can be used instead of UNION since we won’t have duplicate results to remove.\nselect AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode from Person.Address where PostalCode like \u0026lsquo;RG41%\u0026rsquo; UNION ALL select AddressId, AddressLine1, AddressLine2, City, StateProvinceID, PostalCode from Person.Address where PostalCode like \u0026lsquo;V9B%\u0026rsquo;\nAgent Alerts – Raise error when message doesn’t contain text My second new feature is a lot simpler. When configuring SQL Server alerts you have the option to only raise the alert if the message contains certain text. My proposed feature is the opposite- don’t raise the alert if it contains certain text.\nA severity 20 alert, such as the one below contains the client IP address. It would be really nice to exclude any alerts that contained a certain misbehaving client IP, to help keep the noise down.\nError: 17836, Severity: 20, State: 17. Length specified in network packet payload did not match number of bytes read; the connection has been closed. Please contact the vendor of the client library. [CLIENT: xx.xx.xx.xx]\nThanks for reading my fantasy features. I’m looking forward to reading all the other new ideas out there.\n","date":"2019-09-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#118-your-fantasy-sql-feature/","title":"T-SQL Tuesday #118 – Your fantasy SQL feature"},{"content":"When you enact a configuration against a target node by default the process runs as the local system account. For most of your DSC resources this is fine. However, if you need to access something like a file share, active directory or user registry settings, you might start to run into permission issues. DSC Resources have a built in property, PSDscRunAsCredential, that when configured changes the account that the resource will be executed under.\nTo use this you will use a PSCredential object in your resources as shown below. Note that this example doesn’t require the use of other credentials and would have had the required permissions as the local system account.\n$Cred = Get-Credential Configuration DtcServiceRunning {\nImport-DscResource -ModuleName PSDesiredStateConfiguration Node 'dscsvr2' { Service MsDtcRunning { Name = 'MSDTC' Ensure = 'Present' State = 'Running' PsDscRunAsCredential = ($creds) } } }\nGenerates MOF file DtcServiceRunning -Output .\\Output\\\nWhen you use credentials in your configurations there are a couple of gotchas- we’ll talk through these next.\nWarning 1: Plain Text Passwords Although it feels like you are handling the credential securely, when you run the code above you will get an error explaining that storing passwords in plain text is a bad idea.\nWhen you run your configuration, DtcServiceRunning in my example, a MOF file is generated and with our current setup the passwords will be stored in plain text (you can read more about generating MOF files here). The correct way to handle this issue is to generate a certificate and use that to encrypt the MOF file, a topic I will blog about one day soon. To skirt around this issue in test we can add a configuration property that basically forces us to accept we know this is a bad idea, but we’re going for it anyway.\n$configData = @{ AllNodes = @( @{ NodeName = \u0026ldquo;dscsvr2\u0026rdquo; PsDscAllowPlainTextPassword = $true PsDscAllowDomainUser = $true } ) } DtcServiceRunning -Output .\\Output\\ -ConfigurationData $configData\nOnce we run this we’ll see our MOF file has been successfully generated.\nWarning 2: Changing the password used in a MOF file Once you push your MOF file out to the target node and it is enacted, the MOF remains on the target node in a designated folder and is named current.mof. The credentials, including the password (hopefully encrypted by a certificate!), are within the file.\nYou can run a couple of commands to check your current configuration that compare the desired state, defined in the current.mof file, to the nodes current state.\nIf the password you used for the PsDscRunAsCredentials property has changed since the MOF file was enacted you’ll get the following errors when you try and run Get-DscConfiguration or Test-DscConfiguration.\nThe user name or password is incorrect.\nTo fix this you must rerun your configuration, which will in turn generate a new MOF file that contains the new password. Once this MOF file has been generated you’ll push it out to your target node.\nNow that the MOF file contains the new password you are able to check in on your current configuration again.\nThis wasn’t something I thought about before I used PsDscRunAsCredential on all the resources in my configuration. A while later I went to check on my target nodes and realized I couldn’t without pushing out a new MOF file. After some discussion in the DSC Slack channel around this, the recommended approach is to only use PsDscRunAsCredential if necessary, and to be aware of the requirement to generate a new MOF file once you change the password.\n","date":"2019-09-04T00:00:00Z","permalink":"https://jpomfret.github.io/p/desired-state-configuration-a-few-warnings-when-using-psdscrunascredentials/","title":"Desired State Configuration: A few warnings when using PSDscRunAsCredentials"},{"content":"Sometimes it’s best not to ask why. However, if for some reason you have a number of triggers on tables within a database that you would like to temporarily disable, read on.\nI came across a situation recently while automating a process to refresh test environments where this exact scenario came up. As part of the process several scripts were run to obfuscate production data. While these ran all the UPDATE triggers were firing. Not only were the triggers adding a significant amount of time to the process, they were also updating dates and other values that we’d prefer kept their original values.\nNow, as I mentioned this is not a discussion on whether this is a good database design or not, this is just how to solve this issue.\nIn the snippet below I use Connect-DbaInstance from dbatools to create a $svr object. If you don’t have dbatools installed you could either install dbatools, or use New-Object Microsoft.SqlServer.Management.Smo.Server. The dbatools function is essentially a wrapper around this command that adds a lot of additional checks and options.\nI have also defined an array $triggers to keep track of the triggers I disable. It’s likely that you’ll want to put the environment back to how it started, so this will make sure you don’t enable any triggers that started off disabled.\nThen we get to the actual work. Using the $svr object we can loop through all the tables, and then all the triggers on those tables. If a certain trigger is enabled, it is added to the $triggers array and then disabled using $tr.isenabled. As with most (all?) changes made through SMO you then need to call the alter method ,$tr.alter(), to actually make the change on the server.\n$database = ‘AdventureWorks2017’ $svr = Connect-DbaInstance -SqlInstance server1 $foreach ($tbl in $svr.databases[$database].Tables) { foreach ($tr in $($tbl.Triggers | Where-Object Isenabled)) { $triggers += $tr | Select-Object @{l=\u0026lsquo;SchemaName\u0026rsquo;;e={$tbl.Schema}}, @{l=\u0026lsquo;TableName\u0026rsquo;;e={$tbl.name}}, @{l=\u0026lsquo;TriggerName\u0026rsquo;;e={$_.name}} $tr.isenabled = $FALSE $tr.alter() } }\nWhen you are ready to enable the triggers again you can use the following code. This loops through the triggers that we had previously disabled and added to our array and enables them.\nforeach($tr in $triggers) { $trigger = $svr.Databases[$database].Tables[$tr.TableName,$tr.SchemaName].Triggers[$tr.TriggerName] $trigger.IsEnabled = $true $trigger.alter() }\n","date":"2019-08-19T00:00:00Z","permalink":"https://jpomfret.github.io/p/disable-all-triggers-on-a-database/","title":"Disable all Triggers on a Database"},{"content":"This month’s T-SQL Tuesday is all about Memory Optimized Tables (MOT), a topic I will admit I know almost nothing about. When they first came out in 2014 I was excited about the new technology and the options that would bring, however I never found a good reason to implement them.\nThanks to Steve Jones (b|t) for hosting this month’s edition and after reading about MOT this week I’m looking forward to learning how people are using them.\nI’ve heard many times in the technology world that the third release of a software or new feature is the best time to adopt it. Reading about MOT, it seems like this theory fits here also. When Hekaton, as it was known then, was released with SQL Server 2014 there were a lot of restrictions. With the subsequent releases, presumably guided by feedback, some of these restrictions have been lifted. I’ve picked a couple to discuss here that I think could have a big impact on adoption rates.\nSchema Changes In 2014 schema changes were not supported. This meant you couldn’t ALTER a Memory Optimized Table or a natively compiled stored procedure, instead you had to DROP and recreate the object. This isn’t a huge concern for stored procedures. The main issue here would be losing any object level permissions that had been set.\nWith Memory Optimized Tables however, this means you need to make sure you handle the data within that table. There are ways around this obviously. You could copy it off into a staging table, drop and recreate your table and then re-insert the data. This does however add some complexity to managing your environment, and I could see this putting some people off.\nWith the 2016 release ALTER TABLE and ALTER PROCEDURE support was introduced, effectively removing this barrier to use. A further enhancement, the ability to use sp_rename, was then introduced with the 2017 release.\nAlthough there are still some restrictions on schema changes, including no extended properties, they are minimal compared to what was available in the 2014 release.\nForeign Keys Another huge missing piece when this technology was introduced in 2014 was foreign key support. My understanding was that this technology could be (reasonably) easily swapped in to help with I/O and contention bottlenecks in your current SQL Server environment. This idea becomes less of a possibility when you realize this lack of support. Most well designed databases use foreign keys to enforce referential integrity, so having to remove those in order to use In-Memory technology seems like an unlikely ask.\nAgain, with 2016 we saw this barrier removed. Not only was foreign key support introduced, so too was the ability to use UNIQUE and CHECK constraints on Memory Optimized Tables. This will make it much easier to take existing SQL Server workloads and move some key tables into memory.\nAlthough not directly related to foreign keys, 2017 saw the previous restriction of having a maximum of 8 indexes per Memory Optimized Table lifted. This gives you more freedom to add unique indexes and to index foreign key columns.\nSummary To wrap this up, although I haven’t myself used Memory Optimized Tables, it does seem like the barriers to use have been significantly reduced in the last two releases of SQL Server (2016, 2017). This technology has now had time to mature and grow in features, hopefully now it’ll be easier to find a good use case for implementation.\n","date":"2019-08-13T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#117-when-have-you-used-mot-tables/","title":"T-SQL Tuesday #117 – When Have You Used MOT Tables?"},{"content":"As a SQL Server DBA, aggregating data is first nature. I can easily throw together some T-SQL if I need to get the average sales per product group, or perhaps the number of employees hired per month. Yesterday I was writing a little PowerShell when I came across a problem. I needed to get the size of my database data and log files for several databases on a server, when I realized I didn’t know how to group and sum this data in PowerShell.\ndbatools has the Get-DbaDbFile command, which easily allowed me to collect information about the databases I needed.\n1 Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 Once I had this data, I wanted to group by the type of file and get the total size. To start I piped the output to Select-Object to trim down the fields in the result set, then piped this output to the Group-Object, specifying the field I wanted to group by.\n1 2 3 Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 | Select-Object Database, TypeDescription, Size | Group-Object TypeDescription Now that the data is grouped you can aggregate the size property for the files, which is within the Group. We’ll add an expression to the final Select-Object which uses the Measure-Object to sum the sizes.\n1 2 3 4 Get-DbaDbFile -SqlInstance $svr -Database CompressTest, AdventureWorks2017 | Select-Object Database, TypeDescription, Size | Group-Object TypeDescription | Select-Object Name, @{l=\u0026#39;Size\u0026#39;;e={($_.Group.Size.MegaByte | Measure-Object -Sum).Sum}} Another interesting note here is that since dbatools returns the Size as a special type, Sqlcollaborative.Dbatools.Utility.Size, you can specify that you want it to be returned in MBs. If you have very small databases you might want to change that so you don’t lose accuracy with any rounding.\nThe code for the examples in this post was run against a Docker Container I use for demos. You can read more information on getting that set up in my post Data Compression Demos in Containers.\n","date":"2019-07-26T00:00:00Z","permalink":"https://jpomfret.github.io/p/aggregating-data-with-powershell/","title":"Aggregating Data with PowerShell"},{"content":"One of the biggest obstacles people face when using DSC is the troubleshooting and reporting pieces. There are options here to integrate with third party tools to create a more polished enterprise solution, but if you’re going with just straight DSC you might feel it is lacking some in this area.\nWe do however have several tools available to troubleshoot issues with configurations or to monitor our nodes to determine whether they are still in the desired state. I’m specifically going to look at the options available if you’re using DSC in the Push refresh mode. If you are using DSC in pull mode with a web server or if you’re using Azure Automation you have some other options available. You can configure the Local Configuration Manager (LCM) to send reports to the pull server. These reports are stored in a database on the server and can be accessed by calling the web service. Perhaps the topic of another blog post.\nThe options we’ll look at today are the functions available within the PSDesiredStateConfiguration module, and the DSC Windows event logs.\nPSDesiredStateConfiguration Functions We should be fairly familiar with this module as it comes built in with WMF 4.0+ and contains several base resources as well as functions to manage and use DSC.\nIn order for us to explore these commands we need to have a target node with an active configuration. I pushed out a simple configuration that will ensure the directory c:\\temp exists on the target node. The configuration was successful and the folder was created.\nFirst up we have Get-DscConfiguration. The command based help for this function says it will get “the\ncurrent configuration of the nodes.” When I run this against my target node it returns details about the file resource I used to create the C:\\temp directory and notes that Ensure is Present. This is as I would expect.\nIf I manually delete the C:\\Temp folder, the node is no longer in the desired state. When I rerun Get-DscConfiguration it shows the folder is absent, which is not the behavior I expected from the help. I was expecting to get the current configuration that had been pushed out to the node. It seems that this function returns the resources included in the configuration and their current state. It does not however indicate if this is the desired state.\nThe next function available is Get-DscConfigurationStatus The description of this function states it will retrieve “detailed information about completed configuration runs on the system.” If we run that with just the -CimSession parameter to connect to our node we get some useful information about the last run. However, there is a lot of information available from this function that is not within the default columns returned. If we look into the output further we can in fact see whether resources are in the desired state or not. During my testing using Get-DscConfigurationStatus it did not always accurately report when there were resources not in the desired state. Therefore I wouldn’t rely on it for reporting, I would instead look at our third option below.\nThe third and final function I’ll highlight from this module is Test-DscConfiguration. The comment based help for this one states it “tests whether the actual configuration on the nodes matches the desired configuration.”\nJust running this with the target node as the -ComputerName parameter does not provide much information. It returns `False` telling us it is not in the desired state, but doesn’t explain why.\nRunning with the -Verbose switch goes into details. It returns the same verbose output you would get from running the configuration in the first place. However, reading through the output of a large configuration file with many resources could get time consuming and it would be easy to miss resources not in their desired state. Finally, we can use the -Detailed parameter which will return a PowerShell object with exactly the information we’re looking for. This object gives us more options on how to use this information. For example, we can return whether it is in the desired state and a list of resources not in the desired state is available.\nThis gives us some good information if our configuration was successful, but what happens if we’re troubleshooting a failed configuration? Running both Get-DscConfiguration and Test-DscConfiguration state they will run against a pending configuration and both return the error that the network path wasn’t found. No information is returned on which resource threw this error. If you have a large, complicated configuration it would be nice to receive a little more guidance on where to look next. Hint, it’s the event logs.\nWindows Event Logs The next step in the troubleshooting handbook is to head to the windows event logs. DSC has four event logs, but only two are enabled by default, and it doesn’t seem like much gets written to the Admin log.\nRunning the following will show you the enabled logs and the number of records:\n1 Get-WinEvent -ComputerName dscsvr2 -ListLog *dsc* Looking in the Operational log you can find the error and the related resource:\n1 2 3 4 Get-WinWvent -ComputerName dscsvr2 ` -LogName Microsoft-Windows-DSC/Operational -MaxEvents 10 | Select-Object TimeCreated, Message | Format-List The event log also points to a json file that contains more detailed logging, this matches what would be returned if you ran the Start-DscConfiguration with the -Verbose and -Wait switches.\n1 Message : Job {93E0C95E-8F21-11E9-85A2-00155D016620} : Details logging completed for C:\\Windows\\system32\\configuration\\ConfigurationStatus\\{93E0C95E-8F21-11E9-85A2-00155D016620}-0.details.json. If more detail is needed you can enable the debug and analytic logs for DSC and rerun the configuration. The analytic logs will contain a lot more messages that will help you get to the bottom of why your configuration failed.\n","date":"2019-07-16T00:00:00Z","permalink":"https://jpomfret.github.io/p/desired-state-configuration-troubleshooting-in-push-refresh-mode/","title":"Desired State Configuration: Troubleshooting in Push Refresh Mode"},{"content":"There have been a lot of blog posts and talk around upgrading your servers in the past. However, the chatter always seems to intensify when we start getting close to that dreaded ‘end of support’ date for your older Windows and SQL Server versions. I hope this isn’t the first place you are discovering this, but July 9th 2019 marks the end of support for both SQL Server 2008 and 2008R2, closely followed on January 14th 2020 with the end of support for Windows Server 2008 and 2008R2.\nWith these dates on the horizon it’s a good time to look at our estate and make sure we have a good understanding of the versions we currently support. I’m going to show you how to do that easily with a couple of dbatools functions. Then, bonus content, I’ll show you how to present it for your managers with one of my other favourite PowerShell modules ImportExcel.\nFirst things first, I need an object that contains our servers. At my work we use a central management server to keep track of servers, but you could just as easily pull server names in from a text file, or a database.\n$servers = Get-DbaCmsRegServer -SqlInstance CmsServerName\nLet’s first look at what operating systems we are running. The dbatools function we need for this is Get-DbaOperatingSystem. I’ll use the name property of my $servers object to get the OS information for all my servers and save it to a variable.\n$os = Get-DbaOperatingSystem -ComputerName $servers.name\nI chose to save the results to a variable for this since I’m going to examine the results using PowerShell and then also output them to Excel, saving me from having to gather the information from each server multiple times. If I only planned on looking at the results on screen I could instead have just piped the Get-DbaOperationSystem results straight into Group-Object.\nUsing Group-Object I can quickly see how many servers I have for each versions of windows, and how many I have going out of support in the near future.\n$os | Group-Object OSVersion | Sort-Object Name | Select-Object Name, Count, @{l=\u0026lsquo;Servers\u0026rsquo;;e={$_.Group.ComputerName -Replace \u0026lsquo;.domain.name,\u0026rsquo;\u0026rsquo; -Join \u0026lsquo;,\u0026rsquo;}}\nI have used the -Replace option in my Select-Object to remove the domain name from the output and instead only return the server name.\nWe can do the same with Get-DbaProductKey to get the SQL version information.\n$sql = Get-DbaProductKey -ComputerName $servers.name $sql | Group-Object Version | Sort-Object Name | Select Name, Count, @{l=\u0026lsquo;Servers\u0026rsquo;;e={$_.Group.SqlInstance -join \u0026lsquo;,\u0026rsquo;}}\nWith just 5 lines of code we can review our entire estate and make sure we know what we have nearing the end of support. This is pretty useful information, and also a good thing to export into a pretty spreadsheet and share with your team or management. Enter ImportExcel.\nIf you haven’t used this module before, prepare to have your mind blown. Doug Finke has crafted some PowerShell magic to enable you to both import from and export to Excel, using PowerShell, without needing Excel installed even.\nThe full code is below. We’ve already done the work of gathering our data so if you are following along skip the first 3 lines below.\nI’ve separated out the properties I want to select and will therefore end up in my spreadsheet. I’ve also used splatting to make the call to Export-Excel easier to read.\nThe important part of this script is the parameters used for the Export-Excel call so I’ll go through them here:\n[table id=7 /]\nThere you have it, a simple script to get the current OS and SQL versions you are running with a good looking Excel sheet as the output. Hope you don’t find too many instances out there nearing the end of support.\n","date":"2019-05-30T00:00:00Z","permalink":"https://jpomfret.github.io/p/getting-os-and-sql-version-information-with-dbatools/","title":"Getting OS and SQL Version information with dbatools"},{"content":"One of the reasons I love PowerShell is the comment based help. This allows you to easily get documentation for functions directly within your PowerShell session. By using Get-Help for a function you can retrieve a description, information on the parameters, and examples of how to use the function.\nRecently I was adding help to a function and wanted to add two lines of code to my example. Usually the syntax for an example looks like this:\n1 2 3 4 .EXAMPLE Get-Cat This will get me a default of one cat. The first line under .EXAMPLE will be formatted with a PowerShell prompt in front to show it is code. The second line is a description of the example.\nIf I want to add two lines of code, and I used the following, it would only display the first line with a prompt as shown in the screenshot below.\n1 2 3 4 5 .EXAMPLE $cats = 4 Get-Cat -NumberOfCats $cats Store the number of cats in a variable and then get that number of cats. You can see above the first example looks good, however in the second example the first two lines should both have a prompt to show they are code. I spent a little while Googling this without much avail. I then figured, somewhere within dbatools there must be an example with two lines of code. Sure enough I found my answer, and it’s pretty straightforward. You just add the prompt to the code yourself and then when the example is displayed it is formatted properly.\nI changed my examples to the following and you can see now they display as expected.\n1 2 3 4 5 6 7 8 9 10 .EXAMPLE PS C:\\\u0026gt;Get-Cat This will get me a default of one cat. .EXAMPLE PS C:\\\u0026gt;$cats = 4 PS C:\\\u0026gt;Get-Cat -NumberOfCats $cats Store the number of cats in a variable and then get that number of cats. I know you’re all dying to see the result of my Get-Cat function so here you go. If you need to add this to your PowerShell profile etc. so you can quickly brighten any day, the code is on my github.\n","date":"2019-05-16T00:00:00Z","permalink":"https://jpomfret.github.io/p/powershell-comment-based-help-examples-with-multiple-lines-of-code/","title":"PowerShell Comment Based Help: Examples with Multiple Lines of Code"},{"content":"Well it has been a little quiet here recently. I just (or it’s been two weeks now) got back from a 2 week trip to England and France. It was an amazing trip and there are a few pictures on Instagram if you are curious about what I got up to.\nThis is also going to be a quick post. I asked a question on Twitter last week about what happens when you have multiple triggers on a table. I got the answer (Thanks Aaron!), but figured this would be a good thing to demonstrate.\nI have also been playing with Azure Data Studio and the new notebook feature, so I answered this question with a step-by-step example in a notebook. I also found that you can easily store these notebooks on GitHub so I have uploaded it to my demos repo for you to follow along.\nTrigger Order Notebook\nTL;DR: Triggers execute one after the other. I demonstrated this by creating a table with three insert triggers that each waited 2 seconds and recorded the timestamp.\n","date":"2019-05-08T00:00:00Z","permalink":"https://jpomfret.github.io/p/execution-of-multiple-triggers-on-one-table/","title":"Execution of Multiple Triggers on one Table"},{"content":"A critical part of our DSC configuration is made up of resources. These are the building blocks we need to to define our desired state. There are two kinds of resources that we can use: class based and MOF based (most common). We are going to focus our efforts today on looking at MOF based resources.\nResources come packaged up as modules and our servers, which use at least WMF 4.0, come with several built-in. We have two main options for additional resources; we can find DSC resource modules in the PowerShell Gallery or we can write our own.\nFinding DSC Resources To find existing resources we have a few options. We could navigate to the PowerShell Gallery website and browse through the modules with the ‘DSC’ tag. We could also use PowerShell by using the `Find-Module` command and the `-Tag` parameter to match our results from the PowerShell Gallery.\n1 Find-Module -Tag DSC Our second option using PowerShell is to use Find-DscResource. This cmdlet finds specific resources that are contained in modules. Running a count against that right now (4/1/2019) would return 1,413 resources that are available to configure our environment. Using the -Filter parameter you can search for a keyword throughout the names, descriptions and tags of all these resources.\nInstalling Resources If you find a DSC Resource you want to use in your configurations, for example to install SQL Server we will want to use SqlSetup from the SqlServerDsc module, you can install the module as you would a regular PowerShell module. For example, using Install-Module.\nOnce the module is installed you can use these resources in your configurations. One thing to remember is the resources need to be available on both your authoring station and your target node.\nFive Useful Resources To wrap up I’m going to look at five resources I’ve used in my configurations to give you an idea of what is available.\nFile The File resource is one of the built-in resources and is useful for creating files and folders. For my SQL Servers I use it to create separate folders for the data, log, and tempdb files. Another use case would be to create a file and, by using the contents property, we could even add data to it.\nFirewall Part of the NetworkingDsc module, the Firewall resource can be used to configure firewall rules on your target node. This is useful when installing SQL Server so you can open up access remotely. There is also a SqlWindowsFirewall resource that will accomplish this task, but has less properties to configure. However, depending on your preferred setup, this may suffice.\nSqlSetup SqlSetup is the resource that installs SQL Server. Included in the SqlServerDsc module there are many properties available to configure the installation just right.\nScript One of the most flexible resources is the Script. This is also a built in resource and gives you the ability to code any PowerShell script into a simple resource. You basically write three mini functions: one that gets the current state, one that tests if it’s in the desired state, and finally one to ‘make it so’.\nSqlAgentOperator The final resource I’ve picked is part of the SqlServerDsc module and allows you to create a SQL Agent operator. This is useful if you like to send alerts or notifications from your SQL Servers. The reason I picked this resource is that I wrote it. One of the best parts of DSC is that most of the modules are open source and available on Github. If there is a resource missing that you’d find useful, you are encouraged to write it and submit it to the Microsoft repos. That’s pretty cool if you ask me.\n","date":"2019-04-08T00:00:00Z","permalink":"https://jpomfret.github.io/p/desired-state-configuration-resources/","title":"Desired State Configuration: Resources"},{"content":"Once we have crafted the perfect configuration and shipped it out to our target nodes, it’s time for the magic to happen. The MOF file that we created by executing our configuration is translated and enacted by the Local Configuration Manager (LCM) on each target node. The LCM is the engine of DSC and plays a vital role in managing our target nodes.\nThe LCM on each target node has many settings that can be configured using a meta configuration. This document is written very similarly to our regular DSC configurations and then pushed out to the target node. I’m going to cover a few of the important LCM settings for use in push mode. This is where the LCM passively waits for a MOF file to arrive. The other option is pull mode- this is a little more complicated to set up and in this scenario the LCM is set to actively check in with a pull server for new configurations.\nImportant LCM Settings As mentioned we are going to look at a subset of LCM settings. A full list is available at books online “Configuring the Local Configuration Manager”.\n[table id=6 /]\nConfigure the LCM We are going to change a couple of the LCM settings by writing a meta configuration document, compiling it as a MOF and pushing it to our target node. The LCM on that target node will receive this MOF file and enact it to put the LCM into the desired state. To start with we can check out the available settings by using the Get-DscLocalConfigurationManager cmdlet.\n1 Get-DscLocalConfigurationManager -CimSession dscsvr2 We are going to change two settings in this example. First, I’m going to change the ConfigurationModeFrequencyMins to 20 minutes, instead of the default of 15. Secondly, I will change the RebootNodeIfNeeded to true. This means if I push out a configuration that requires a reboot my node will automatically reboot.\n1 2 3 4 5 6 7 8 9 10 11 12 [DSCLocalConfigurationManager()] configuration LCMConfig { Node dscsvr2 { Settings { ConfigurationModeFrequencyMins = 20 RebootNodeIfNeeded = $true } } } I’ll then execute the LCMConfig configuration to generate a meta MOF file. You can see this is named with the target node name and then the extension is .meta.mof. For a regular configuration the file would just be named with the target node name and the extension of just .mof.\n1 LCMConfig -Output .\\output\\ We will then enact this configuration using Set-DscLocalConfigurationManager:\n1 Set-DscLocalConfigurationManager -Path .\\output\\ -ComputerName dscsvr2 -Verbose Once this is complete we can check our settings using the following:\n1 2 Get-DscLocalConfigurationManager -CimSession dscsvr2 | Select-Object ConfigurationModeFrequencyMins, RebootNodeIfNeeded Now our LCM is in our defined desired state and we are ready to push out a configuration to set the desired state of our server.\nApplyAndAutoCorrect The LCM can also play an important role in keeping our servers in the desired state. If we changed the ConfigurationMode to ApplyAndAutoCorrect the LCM would check every 15 minutes (default value for ConfigurationModeFrequencyMins) to ensure the server was still in the desired state. If it found it was not, the LCM would reenact the current MOF to put the server back to desired state. This is a pretty powerful feature but one that definitely requires some thought. I can imagine a 3rd party vendor wouldn’t be too happy if they set something on installation and my DSC configuration reverted that automatically.\n","date":"2019-03-25T00:00:00Z","permalink":"https://jpomfret.github.io/p/desired-state-configuration-local-configuration-manager/","title":"Desired State Configuration: Local Configuration Manager"},{"content":"In my last Desired State Configuration (DSC) post a couple of weeks ago I covered some of the concepts involved with DSC, and I also have a T-SQL Tuesday post to get you started writing your first configuration. So today we are going to look at the next step in the process: what happens after we’ve written a configuration?\nHere’s a quick recap on how to write a simple configuration. I’ve named this configuration CreateSqlFolder. I’m targeting the node dscsvr2 and within that node block I’m using the File resource to define my desired state of having a directory C:\\SQL2017\\SQLData exist. With the final line of the script I’m calling the configuration and specifying where the output should land.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Configuration CreateSqlFolder { Import-DscResource -ModuleName PSDesiredStateConfiguration Node dscsvr2 { File CreateDataDir { DestinationPath = \u0026#39;C:\\SQL2017\\SQLData\\\u0026#39; Ensure = \u0026#39;Present\u0026#39; Type = \u0026#39;Directory\u0026#39; } } } CreateSqlFolder -Output .\\Output\\ Generate a MOF file When I run this script I see the output in the screenshot below, a MOF file has been created in my output folder. Managed Object Format (MOF) files are used to describe Common Information Model (CIM) classes, these are industry standards which gives us flexibility in working with DSC. In DSC this is important as the MOF file is the artefact that will actually be used to configure our nodes. This MOF will be delivered to our target node and enacted by the Local Configuration Manager (LCM).\nThe LCM will be covered in more detail in a later post, but for now know that it can be configured to be in either ‘Push’ mode or ‘Pull’ mode. Pull mode is more complicated to set up but perhaps more appropriate for managing a large number of servers. For now, we will look at the ‘Push’ mode where we will deliver the MOF manually to the target node for the LCM to enact.\nExecuting configuration to create a MOF file for target node.\nPublish a MOF File To get the MOF from my authoring station out to the target node I have a couple of options. First, I can run Start-DscConfiguration. This will push out the MOF and immediately enact the configuration. Using the -wait and -verbose switches we can see the output returned to our PowerShell console as the configuration is applied.\n1 Start-DscConfiguration -Path .\\output\\ -ComputerName dscsvr2 -Wait -Verbose If we want to push out the configuration but not immediately enact it we can use Publish-DscConfiguration. I again used the -Verbose switch to return output:\n1 Publish-DscConfiguration -Path .\\output\\ -ComputerName dscsvr2 -Verbose You can see this in this screenshot it says ‘Configuration document successfully saved to pending state’, letting us know this is now ready for the LCM to enact. We can confirm our PendingConfiguration by running the following:\n1 Get-DscLocalConfigurationManager -CimSession dscsvr2 | Select LCMState To enact the pending configuration we would again use Start-DscConfiguration, only this time instead of specifying a path we’d add the -UseExisting switch.\nIt is important to note that if the LCM settings are currently set to the defaults this configuration will be automatically applied when the next consistency check runs within 15 mins.\nLook for a post coming soon where we’ll look at the LCM in more detail and examine some of the settings we have to manage how it works within DSC.\n","date":"2019-03-18T00:00:00Z","permalink":"https://jpomfret.github.io/p/desired-state-configuration-mof-files/","title":"Desired State Configuration: MOF Files"},{"content":"\nIt’s T-SQL Tuesday time again and our host this week, Shane O\u0026rsquo;Neill (b|t), has challenged us to a humble brag. This really is a challenge for most of us as we do awesome things quietly, so thanks to Shane for forcing us to share some ‘cookies’. This is also the second time in a couple of weeks that David Goggins’ book has been mentioned, which means I need to move it up on my to-read list.\nImposter syndrome is something a lot of us struggle with. I am going to share a couple of things that I’m proud of and that I can look back on when things get tough. As I’ve written before I’m working hard on stepping out of my comfort zone to prepare and deliver technical presentations. Recently I’ve been building a presentation that has caused me to wonder many times, what was I thinking. Hopefully this recap will remind me that I can do it.\nAutomating Nonproduction Refreshes This first story is a technical one. I was working on a project a year or so ago that involved complicated changes to business processes and therefore it was decided that development and test environments needed to be as similar to production as possible. This meant that I was inundated with requests to take backups of production and restore to many nonproduction environments. This got old fast.\nThe process not only involved the backup/restore piece, the data in production was both sensitive and encrypted using TDE. I started writing PowerShell scripts for each step in the process, restoring certificates and databases, unencrypting and then removing certificates and then masking the sensitive data (which involved calling a T-SQL stored procedure built by my colleague, Andrew). This was fine to begin with, but as the production data grew the process took longer and longer, which meant that the nonproduction environment was unavailable for longer each time.\nAfter awhile this process was no longer acceptable because the test environments were down for too long. I met with the team and we came up with a plan. We ended up agreeing that we would create a process to create ‘restore points’. These would basically be points in time from production, that were prepared for nonproduction use, off hours in a temporary environment. I utilized a combination of Urban Code Deploy (UCD), a product we already had in house, and PowerShell scripts to give the developers the power to first create a restore point, and then to be able to refresh environments from these prepackaged backups.\nThe process in UCD to create a restore point took the scripts I had been using manually and packaged them up so the result was a folder of already unencrypted, masked databases that were safe to restore to any of the test environments. Everyone was happy, I no longer had to do this mundane, boring request and the team could refresh whenever they needed to, only having to wait for the restore to finish.\nThere is still plenty of room for improvement in this process. Perhaps we could use some kind of database cloning technology to minimize space requirements and reduce the time to restore. Another option would be running the databases in containers. The developers could then just spin up an environment when they needed to test something. For now, though, the process is making everyone’s life easier, and that was a big win for both myself and the project team.\nSpeaking in Front of People! The second humble brag is that I’m speaking in front of people! A year ago I had just agreed to give my first user group presentation and I was terrified. I had set a goal of giving back to the community by blogging/writing and speaking, but the speaking part was by far the most difficult for me to get my head around. Thinking back to college, I dreaded finding out that class projects included final presentations. This hasn’t changed, and now I was voluntarily going to present. In the last year I’ve put together a pretty decent hour-long presentation on data compression that I’ve delivered not once, but four times. Each time I’ve grown in confidence and felt more like I could be myself in front of a crowd. One of my demos that I’m particularly proud of shows how data compression affects the data storage on the page. I then wrote about this demo and it was picked up and featured in Brent Ozar’s weekly links.\nI’ve also had my second presentation topic selected for DataGrillen and SQL Saturday Cincinnati. I’m working hard to make sure I can deliver an informative and useful session on the possibilities of using PowerShell Desired State Configuration to install and configure SQL Server. During this process it has often felt like I’ve bitten off more than I can chew, both on my topic choice and the fact I submitted it to an international conference with a superstar line up of speakers.\nBut here’s to remember the wins, and knowing I’ve already overcome these same doubts and challenges to get my first presentation up and running. This will be a great post to come back to when I need to dip in the cookie jar.\n","date":"2019-03-12T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#112-dipping-into-your-cookie-jar/","title":"T-SQL Tuesday #112 - Dipping into your Cookie Jar"},{"content":"I’m currently working on a pretty interesting project to explore using PowerShell’s Desired State Configuration (DSC) to manage SQL Servers. DSC uses declarative language to define the desired state of your infrastructure.\nEnsuring that the directory C:\\Test exists is a simple example. A more complicated example would be the complete configuration of a SQL Server. This is my end goal.\nThis post is aiming to just introduce DSC and a few of the concepts that come along with it, and give us a good building block for future posts that dive deeper into this topic.\nThe infrastructure that surrounds DSC warrants several posts on its own, so for this first scratch of the surface just know that we will write DSC Configuration documents and these documents will be managed and executed on our target nodes.\nDeclarative Vs Imperative If you are already familiar with PowerShell scripts you write imperative code, or the actual instructions on how to accomplish something. For example if I want to create a folder I’d write:\n1 New-Item -Path C:\\test -ItemType Directory However, when writing DSC configurations you use declarative language, where you describe the desired state without having to instruct exactly how to get there. Using the same example you would add the following resource block to your configuration document to ensure the C:\\test folder exists.\n1 2 3 4 5 File CreateTestDir { DestinationPath = \u0026#39;C:\\test\u0026#39; Ensure = \u0026#39;Present\u0026#39; Type = \u0026#39;Directory\u0026#39; } Resources Resources are one of the central building blocks in DSC. Each resource contains the code that takes the declarative syntax you write and makes it happen. In our example above our file resource will translate our desired state into regular PowerShell code, most likely using the same New-Item cmdlet that we had in our example. This resource is built into Windows so we can’t examine it to prove that.\nThere are currently 22 resources available within the built in PSDesiredStateConfiguration module. The table below contains the descriptions of a few, for a full list you can review the Microsoft docs.\n[table id=2 /]\nOn top of these built in resources are hundreds more that have been developed by Microsoft, or by the community. They come packaged just like modules and most can be installed directly from the PowerShell Gallery), some examples are:\n[table id=5 /]\nAs you can see DSC can be used to configure a wide variety of components. We can collect resources from several modules and then combine them into one configuration document to describe our desired state.\nIdempotent Another interesting aspect of DSC is that the resources are written to be idempotent. This means that in our file example above if the folder already exists it won’t try and create it again.\nThere are two main types of resources, class based and MOF based. We’ll be focusing on MOF based in this post. Within each resource are three functions: Get-TargetResource, Set-TargetResource and Test-TargetResource. When you run a configuration that contains our file resource example, the Test-TargetResource will fire first to see whether we’re already in the desired state. That function returns true or false. If the directory doesn’t exist, the Set-TargetResource will fire to create the folder.\nOn the other hand, if we ran the New-Item snippet and the directory already existed it would throw errors. To avoid this, we would have to wrap it with extra logic ourselves to test whether the folder exits as expected and if not, go ahead and create it.\nSo Why Use Desired State Configuration? DSC is a framework that provides the ability to manage our infrastructure with Configuration as Code. There are several benefits to managing our infrastructure this way. The two biggest reasons I think DSC will work well in my particular scenario is automation and that my infrastructure will be in source control.\nDSC enables automation for building SQL Servers by creating a configuration document that defines exactly how the server should be built. For example, the document tells you things like where the database data and log files should be stored, how tempdb is configured, and whether database mail is enabled.\nThe configuration document can then be combined with configuration data, which contains values specific to this build. For example the instance name and perhaps the edition and version of SQL Server to install would be found in the configuration data. We can reuse the same configuration document for every server, all we would need to do is provide the appropriate configuration data.\nUsing configuration as code for building SQL Servers gives us another great benefit because these documents can be checked into source control. We now know exactly what the servers should look like, and when we make a change that will be tracked in source control. This creates documentation on your entire build. If you needed to rebuild a server during disaster recovery, for example, you could just push that configuration out to a new server and wait for it to end up in your desired state.\nResources If you want to know more about DSC I have listed a few links below. I also plan on expanding this post into a series covering general DSC concepts as well as the specifics for managing SQL Servers with DSC.\nPro PowerShell Desired State Configuration: An In-Depth Guide to Windows PowerShell DSC Windows PowerShell Desired State Configuration Using configuration data in DS Infrastructure as Code ","date":"2019-02-26T00:00:00Z","permalink":"https://jpomfret.github.io/p/introduction-to-desired-state-configuration/","title":"Introduction to Desired State Configuration"},{"content":"Last year I gave my first user group presentation on data compression and since then I’ve also given this talk at both SQL Saturday Columbus 2018 and SQL Saturday Cleveland 2019. One of my favourite demos from the presentation is taking a look under the covers to see what SQL Server does with compressed data at the page level. This blog post is going to walk through this demo. If you’d like to follow along you can pull down my docker image and have your own environment up and running in no time. As long as you already have docker running on your machine you can use the following to get setup and the full demo script is available on GitHub.\ndocker pull jpomfret7/datacompression:demo\ndocker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d jpomfret7/datacompression:demo\nI’m using an empty database to start this demo so you only really need my containerized environment for the end when we need to load some more sample data.\nWithin my empty database, named CompressTest, I first create a simple table named employee and insert three rows. A couple of important things to note on this table. Firstly, the datatypes I’ve chosen are all fixed length, and the values inserted leave a lot of empty space. Secondly, there are a few examples of repeating data, both in the name columns and the city. These conditions make this table a great candidate for both row and page compression.\nUSE CompressTest GO\nCREATE TABLE employee ( employeeId bigint PRIMARY KEY, firstName char(100), lastName char(100), address1 char(250), city char(50) )\nINSERT INTO employee values (1, \u0026lsquo;Alex\u0026rsquo;,\u0026lsquo;Young\u0026rsquo;,\u0026lsquo;2 Sand Run\u0026rsquo;,\u0026lsquo;Akron\u0026rsquo;), (2, \u0026lsquo;Richard\u0026rsquo;,\u0026lsquo;Young\u0026rsquo;,\u0026lsquo;77 High St.\u0026rsquo;,\u0026lsquo;Akron\u0026rsquo;), (3, \u0026lsquo;Alexis\u0026rsquo;,\u0026lsquo;Young\u0026rsquo;,\u0026lsquo;1 First Ave.\u0026rsquo;,\u0026lsquo;Richfield\u0026rsquo;)\nCompression Level: None Once we have our table created we need to use a couple of undocumented, but widely used commands to view the underlying page. Step one is to find the page that contains our data. We\u0026rsquo;ll use DBCC IND for this and pass in our database and table name.\nDBCC IND (\u0026lsquo;CompressTest\u0026rsquo;, \u0026rsquo;employee\u0026rsquo;, 1);\nThe employee table has two types of pages shown here. The PageType of 1 is our data page and the one we are interested in today. Once we have our PageFID (1) and PagePID (376), we’ll take these values and use them as parameters for DBCC PAGE.\nWe first need to switch on trace flag 3604: this will write the output of our DBCC PAGE command to the messages tab instead of the event log.\nThere are 4 parameters for DBCC PAGE: we will need to pass in the database name (or id), the file number, the page id and the print option. Using a print option of 0 will give us just the page header. In these examples I’m going to use option 3 which gives us more details on the rows stored on the page. For more information on using DBCC PAGE I’d recommend Paul Randal’s post \u0026ldquo;How to use DBCC PAGE\u0026rdquo;.\nWe will run the following to inspect our page:\nDBCC TRACEON (3604); GO DBCC PAGE(\u0026lsquo;CompressTest\u0026rsquo;,1,376,3)\nThere is a lot of information returned. Before you get overwhelmed, we are only going to look at a few data points from this so we can show the changes as we apply compression. If you are interested in learning more about page internals, another Paul Randal post worth checking out is \u0026ldquo;Inside the Storage Engine: Anatomy of a page\u0026rdquo;.\nFrom the output below we’ll note the following: the pminlin (the size of the fixed length data fields) is 512, the m_slotCnt (the number of records on the page) is 3, and finally the m_freeCnt (the number of free bytes on the page) is 6545.\nSince we used option 3 for DBCC PAGE we can also scroll down and see the data on our pages. The first record is below and is currently 515 bytes, and you can see on the right there is a lot of unused space.\nCompression Level: Row We’ll now take our employees table and apply ROW compression to the clustered index. This physically changes how the data is stored on the page. Any fixed length datatypes will now be stored in variable length fields where the data only uses the minimum number of bytes needed.\nALTER TABLE employee REBUILD PARTITION = ALL WITH (DATA_COMPRESSION = ROW)\nWhen compression is applied the pages are rewritten to disk, we need to use DBCC IND to retrieve the new page information:\nDBCC IND (\u0026lsquo;CompressTest\u0026rsquo;, \u0026rsquo;employee\u0026rsquo;, 1);\nWe then use these values for DBCC PAGE. The trace flag we set earlier is good for the session, therefore if we’re in the same query window we don’t need to rerun that command.\nDBCC PAGE(\u0026lsquo;CompressTest\u0026rsquo;,1,384,3)\nNow that our table is ROW compressed you can see the pminlength is only 5, this is reduced from 512 when our table wasn’t compressed. You can also note m_slotCnt is still 3, which is expected, and the amount of free space on the page m_freeCnt has increased to 7971.\nIf we again scroll down to inspect our first row we can see it is now only 35 bytes and the highlighted area on the right clearly shows that the unused space within our row has been removed.\nCompression Level: Page Our final type of compression is PAGE compression. This compresses the data using three steps:\nRow compression: removing any wasted space from fixed length datatypes as we have already seen. Prefix compression: the engine will look for repeating data at the start of each column on each page and store that once in the anchor record. Each row will then store a pointer back to that anchor record which is stored within the compression information section of the page. Dictionary compression: similar to prefix compression except the repeating data can be anywhere on the page instead of being restricted to the same column. ALTER TABLE employee REBUILD PARTITION = ALL WITH (DATA_COMPRESSION = PAGE\nRunning DBCC IND again will get us our newly written page:\nDBCC IND (\u0026lsquo;CompressTest\u0026rsquo;, \u0026rsquo;employee\u0026rsquo;, 1);\nWe’ll examine it with DBCC PAGE:\nDBCC PAGE(\u0026lsquo;CompressTest\u0026rsquo;,1,376,3)\nThe interesting thing here is that nothing has changed, but if I check the DMVs the employee table shows as PAGE compressed.\nUse CompressTest\nSELECT schema_name(obj.SCHEMA_ID) as SchemaName, obj.name as TableName, ind.name as IndexName, ind.type_desc as IndexType, pas.row_count as NumberOfRows, pas.used_page_count as UsedPageCount, (pas.used_page_count * 8)/1024 as SizeUsedMB, par.data_compression_desc as DataCompression, (pas.reserved_page_count * 8)/1024 as SizeReservedMB FROM sys.objects obj INNER JOIN sys.indexes ind ON obj.object_id = ind.object_id INNER JOIN sys.partitions par ON par.index_id = ind.index_id AND par.object_id = obj.object_id INNER JOIN sys.dm_db_partition_stats pas ON pas.partition_id = par.partition_id WHERE obj.schema_id \u0026lt;\u0026gt; 4 ORDER BY SizeUsedMB desc\nSQL Server outsmarted us a little here. I have only inserted three rows into the employee table and we know after looking at the DBCC PAGE output that there is plenty of free space on this page. SQL Server will only apply PAGE compression if it needs to as there is a higher CPU cost to use prefix and dictionary compression. If page compression isn’t going to save any pages the engine leaves the table with just ROW compression applied.\nRunning the following will insert 200 more rows into my employee table. I’m getting the data from the vEmployee view within AdventureWorks2017.\nINSERT INTO employee (employeeId, firstName,lastName, address1, city) SELECT TOP 200 BusinessEntityID, FirstName, LastName, AddressLine1, CITY FROM AdventureWorks2017.HumanResources.vEmployee WHERE BusinessEntityID \u0026gt; 3\nNow when I run DBCC IND I can see the employee table uses four pages, two of them being data pages.\nDBCC IND (\u0026lsquo;CompressTest\u0026rsquo;, \u0026rsquo;employee\u0026rsquo;, 1);\nFinally, we’ll look at DBCC PAGE to see page compression in action:\nDBCC PAGE(\u0026lsquo;CompressTest\u0026rsquo;,1,376,3)\nYou can now see there are 102 rows on our page (m_slotCnt) and our fixed length data types are still 5 (pminlen). Directly after the page header is the compression information section. You can see on the right that repeating data values have been pulled out and stored here. There are two possible CI Header Flags and they are both set here. CI_HAS_ANCHOR_RECORD shows that prefix compression has been used and CI_HAS_DICTIONARY shows that dictionary compression has been used.\nIf I scroll down a little further, I’ll come to the first row. The record is now only 24 bytes and you can see that a lot of the data has been replaced. The values Alex, Young and Akron all now reside in the compression information and this record just contains pointers.\nSummary It’s easy to just apply compression to your databases and see massive space savings. This post hopes to shed a little light on what happens to your pages when using row and page compression. I recently gave this talk to the DBA Fundamentals virtual chapter so if you’d like to see the rest the recording is available on YouTube.\n","date":"2019-02-19T00:00:00Z","permalink":"https://jpomfret.github.io/p/data-compression-internals/","title":"Data Compression Internals"},{"content":"\nIt’s T-SQL Tuesday again and thanks goes this week to our host Andy Leonard (b|t). Andy has picked a great topic on why we do the things we do. With our hectic lives it’s good sometimes to sit down and have a think about something like this. I’ve split my ‘what I do’ up into three buckets that, although are separate, have a lot of common ‘whys’.\nDatabase Administrator I have been a SQL Server DBA since 2011, and it was kind of an accident. I graduated from The University of Akron with an undergraduate degree in Information Systems and a Masters in Business Administration, and marched off into the real world to find the perfect job. I had many interviews for all kinds of IT related positions and although I thought most of them went well and would provide a good first job for me, the folks on the other side of the table didn’t seem to agree. Part of this was I required a visa to work in the US. It was always going to be challenging to convince someone to take a chance on a new graduate and then spend a lot of money and effort to file paperwork.\nI was about a week or two away from having to leave the country as my current visa was expiring, when I got a job offer as a SQL Server DBA. I had gone on this interview without much hope. I didn’t feel like I was qualified, and although I enjoyed my databases classes in college I had very little experience actually doing this kind of work (I’d worked as a grad assistant CMS web admin for the past two years).\nThe first few weeks of this position were rough. I still remember one of the first days, I was being trained to handle simple change requests (adding permissions, changing agent jobs, creating tables etc.), my colleague told me to right click on a particular job and select properties so we could change the schedule. I had zero idea how to even find the job in management studio. I just looked at him blankly as he had to walk me through expanding ‘SQL Server Agent’ and then ‘Jobs’.\nOne thing I’m pretty proud of is I work hard at learning and growing. I like to know how things work and why, and although I was well out of my depth to start with I managed to gain a lot of knowledge in this role. I got more comfortable handling change requests and eventually got to the point where I could get through a two week on call rotation without having to wake up the senior DBA, even when our reasonably complicated replication setup caught fire in the middle of the night for no reason.\nAfter four years I moved companies to my current role, still working as a SQL Server DBA. It’s been over 8 years since that first day, and I still love what I do. I think the main reason is that the learning never stops. There is always new challenges to attack, new ideas to test out, and with that comes so many opportunities.\nCrossfitter CrossFit, for those of you that don’t know, is a workout regime consisting of “constantly varied high-intensity functional movements”. It combines cardio, weightlifting and gymnastics into a workout that will leave you led on the floor wondering why it was so hard. In my opinion though the beauty of CrossFit is what happens around the workout. You end up with a random group of people all struggling through the workout of the day (WOD) together, cheering each other on and then reminiscing about how they made it through. It’s not at all unlike team sports, and that’s why when I found CrossFit it filled a huge void.\nI played soccer (proper football) for the first 23 years of my life. I’m pretty sure as soon as I could walk I had a football at my feet. When my senior season of college was over in 2009 I stopped playing and I stopped working out. I didn’t stop eating like an athlete though – bring on the weight gain! Once I started CrossFit I found the same things I’d loved about football - a group of people all working towards the same goals coupled with the challenge of trying to constantly improve at something.\nCrossFit never seems to get any easier and I think that’s why I love it. I recently had a memory pop up on Facebook that 7 years ago I had got my first handstand pushup. These days I can (not going to say easily) get 50 in a workout, but now I’m working towards being able to walk on my hands. The challenges never end. Whenever I reach a goal, there is always the next one. Can I lift more weight, can I do that workout faster, can I catch my wife (no is the answer here…). These challenges, and the amazing community at my gym, are why I’ll keep on Crossfitting.\nCommunity Collaborator At some point in 2018 I decided it was time. Time to stop being a lurker and to give something back to the amazing SQL Server and PowerShell communities. I started this blog, actually with a T-SQL Tuesday post, and I signed up to present at my local user group in Cleveland. I’ve mentioned this before, but this was way out of my comfort zone. The thought of standing in front of people and sharing what I think I know is terrifying. However, it has also turned out to be an amazing experience so far.\nWhen I started out as a DBA, remember - knowing almost nothing, I leaned heavily on the community. I signed up for newsletters and RSS feeds, trying to read as much as I could. I also attended our user group meetings and some nearby SQL Saturdays and although I was terrible at the networking portion of the events, I got a lot of great tips from the sessions.\nThis past weekend I presented at SQL Saturday Cleveland. This was my second SQL Saturday as a speaker and I still can’t believe how much I loved it. The whole day I was on a high. I gave my session at 8:30am and it went pretty well. I attended a few great sessions, had lunch with some amazing friends and even recorded a video with Bert for his famous YouTube channel. I got to work on Monday and had a notification from LinkedIn that I had a message. Someone had taken the time to write and let me know they’d enjoyed my session and that it had been useful. This made my day, and made me realize I’m actually giving something back. I still have a long way to go but I’m definitely making steps in the right direction.\nSummary Throughout the three parts of my life I’ve talked about there have been a few common themes. I love a good challenge - from standing in front of a room of peers and giving a presentation to trying to walk on my hands, these targets that are just out of reach provide a lot of motivation to me. What goes hand in hand with this is my desire to keep learning and growing. The DBA landscape is never going to stay still or constant, and with that there are forever going to be opportunities for me to evolve as a data professional. That’s pretty exciting, and that’s why I do what I do.\n","date":"2019-02-12T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#111-what-is-your-why/","title":"T-SQL Tuesday #111 - What is Your \"Why\"?"},{"content":"This weekend, while I was having a great time at SQL Saturday Cleveland, I ran into my friend Bert (b|t). He had some dbatools questions, which I was happy to help him with. Now that dbatools has over 500 commands, it is both awesome and terrifying. Bert wanted to know how to automate his database backups and then check he was using the correct recovery model.\nBackup your databases Bert’s first question was how to automate his database backups. I showed him the Backup-DbaDatabase command and explained some of the parameters available.\nFirst, we backed up all the databases on his instance:\n1 Backup-DbaDatabase -SqlInstance localhost\\sql2017 We then looked at specifying a specific database to backup:\n1 ﻿Backup-DbaDatabase -SqlInstance localhost\\sql2017 -Database ApplicationDatabase This command will backup to the default backup location for your instance. If you want to override that you can use the BackupDirectory parameter:\n1 2 3 4 Backup-DbaDatabase ` -SqlInstance localhost ` -Database ApplicationDatabase ` -BackupDirectory C:\\backups\\﻿ The final options we looked at were two switches: CompressBackup,which will make use of backup compression, and CopyOnly, which will leave your LSN chain intact by taking a copy only backup.\n1 2 3 4 5 Backup-DbaDatabase ` -SqlInstance localhost ` -Database ApplicationDatabase ` -CompressBackup ` -CopyOnly Once we had Bert’s databases all backed up and safe he realized he also needed to make sure the database recovery model was set correctly.\nCheck Database Recovery Model Bert wanted to make sure he was using the Full recovery model for his databases. We went about finding any that were in Simple with the following:\n1 Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple﻿ We also talked about running this command against multiple instances, either by using a central management server or from a text file:\n1 2 3 Get-DbaDbRecoveryModel ` -SqlInstance $(Get-Content C:\\servers.txt) ` -RecoveryModel Simple﻿ We found some databases in the simple recovery model that we wanted to change. This can easily be accomplished by piping the output of our get command into the set command:\n1 2 Get-DbaDbRecoveryModel -SqlInstance localhost -RecoveryModel Simple | Set-DbaDbRecoveryModel -RecoveryMode Full Search dbatools Commands The final tip I had for Bert was how to use Find-DbaCommand to help him find the commands he needed to complete his tasks.\nA lot of the commands have tags, which is a good way to find anything relating to compression. For example:\n1 Find-DbaCommand -Tag Compression﻿ You can also just specify keywords and the command will search for any reference of these within the inline command based help for all the commands.\n1 Find-DbaCommand triggers Summary There are many more resources to get help with dbatools. Firstly, their website, https://dbatools.io/, has a lot of great information on how to get started.\nSecondly, the dbatools slack channel is always full of people who can lend a hand. You can get an invite here: https://dbatools.io/slack/.\nFinally, feel free to get in contact with me if you have any questions or need some help finding the commands you need to get going with dbatools.\n","date":"2019-02-05T00:00:00Z","permalink":"https://jpomfret.github.io/p/dbatools-with-bert/","title":"dbatools with Bert"},{"content":"One of the things I want to spend more time exploring this year is containers, specifically SQL Server running in containers. While I’ve been preparing to give my data compression talk at SQL Saturday Cleveland, which is only two weeks away, and generally procrastinating from all other responsibilities, I decided I should change my demos from running against a VM on my laptop to running against a containerized instance of SQL Server.\nFirst, a couple of blog shout outs. This idea had been in my mind for a little while after reading a great post by Cathrine Wilhelmsen where she wrote about moving her demo environments to containers. I’ve also spent a decent amount of time reading Andrew Pruski’s excellent container series. These were the source for the majority of both my knowledge and confidence that I could pull this off.\nDemo environment setup I use three databases in my data compression demos - first is a copy of AdventureWorks. On my VM I actually removed some of the tables I didn’t use to skinny it down a little. The second database I named ‘SalesOrderLarge’ and is just three copies of the SalesOrderDetail and SalesOrderHeader tables that have been enlarged using Jonathan Kehayias’ script. Finally, I have a ‘CompressTest’ database that is just an empty shell that I use to create one table in during the demos to show the internals of compression.\nCreating my Image The first step in this process was to stop the SQL Server service in my VM and copy out the database files (mdf \u0026amp; ldf) to use in my container. I’ll save these into a folder on my laptop for now and then they’ll be copied into my image as it’s built.\nI created the dockerfile below (following Andrew’s example) that will be used to build an image for my datacompression containers. This image is based off the SQL Server 2019 CTP 2.2 image from Microsoft, then we’ll create a folder and copy in a script and the files for my three databases. The last line runs the script and starts SQL Server.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # start with the SQL 2019 CTP 2.2 image FROM mcr.microsoft.com/mssql/server:2019-CTP2.2-ubuntu # create a folder and copy in the attach-db script RUN mkdir /var/opt/sqlserver COPY attach-db.sh /var/opt/sqlserver # copy in AdventureWorks database files COPY DatabaseFiles/AdventureWorks2017.mdf /var/opt/sqlserver COPY DatabaseFiles/AdventureWorks2017_log.ldf /var/opt/sqlserver # copy in CompressTest database files COPY DatabaseFiles/CompressTest.mdf /var/opt/sqlserver COPY DatabaseFiles/CompressTest_log.ldf /var/opt/sqlserver # copy in SalesOrderLarge database files COPY DatabaseFiles/SalesOrderLarge.mdf /var/opt/sqlserver COPY DatabaseFiles/SalesOrderLarge_log.ldf /var/opt/sqlserver # attach databases and start SQL Server ENTRYPOINT /var/opt/sqlserver/attach-db.sh \u0026amp; /opt/mssql/bin/sqlservr The attach-db.sh script uses sqlcmd to execute three CREATE DATABASE commands to finish the setup of my environment and I end up with a folder structure as shown below. You don’t have to put the database files in a separate folder, I only did that for neatness.\nEverything is setup so I’m ready to build my image. I’ll navigate to the DataCompression folder from my PowerShell console and run the docker build command:\n1 2 cd C:\\Docker\\DataCompression\\ docker build -t datacompression . To check out my new image I’ll use docker images:\nRunning my demo container I used to have a ResetEnvironment script that I would use to make sure my VM was setup for the start of my demos. This allowed me to run through and practice my demos as many times as I wanted (read, a lot, probably too many). With containers I will just start up a new one, run through my demos, then remove it, easy as that.\nSide note - I will run some activity against the AdventureWorks database once I start the container. This is needed to show the compression suggestions since these are partially based off index usage. I’ll probably still use some kind of script to start a container and run the load in preparation for the demos.\nFor now, I’ll just use the following to start up a container from my new datacompression image. The important parts of this code are firstly, setting -p 1433:1433, this maps the containers port 1433 to the host computers port. Secondly, we need to set two environmental variables, ACCEPT_EULA=Y to accept the end user licensing agreement and SA_PASSWORD=’password’ to create the SA password for the instance. In this case the password needs to match what I have used in my attach.sh script otherwise when that runs it’ll throw a failed login error and we won’t have any databases created.\n1 docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d datacompression My plan is to use this setup for my demos at SQL Saturday Cleveland. It’s pretty cool that this works exactly the same in my VM running Windows Server 2016 and SQL Server 2016, or in a linux container running SQL Server 2019.\nTry it yourself I\u0026rsquo;m going to write a few follow up posts that make use of this container so if you want to follow along or set up your own environment for tinkering you can pull it down from docker hub.\n1 2 docker pull jpomfret7/datacompression:demo docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=$SaPwd -p 1433:1433 -d jpomfret7/datacompression:demo ","date":"2019-01-23T00:00:00Z","permalink":"https://jpomfret.github.io/p/data-compression-demos-in-containers/","title":"Data Compression Demos in Containers"},{"content":"\nAutomation is something that interests me greatly, and I think if you have read even one of my previous posts you’ll know that my favorite tool for this kind of work is PowerShell. This is the perfect topic to kick off T-SQL Tuesday for 2019 so thanks goes to Garry Bargsley for hosting this month.\nOne of my goals for 2019 is to improve our server build process, it’s currently reasonably well scripted but there are some definite gaps. I’ve started looking at using PowerShell Desired State Configuration (DSC) to install and configure SQL Server to both meet our needs and increase our speed and efficiency. Although full automation is a stretch goal for this project the DSC technology can certainly scale to accomplish that.\nPowerShell Desired State Configuration PowerShell DSC is a platform to support the concept of Infrastructure as Code (IaC). It uses declarative syntax instead of the usual imperative syntax of PowerShell. This means that you describe your desired state rather than the specific steps needed to get there. There are two modes for DSC, push and pull, although pull mode offers more features and scalability, we’ll look at writing our configuration and using push mode for this blog post to keep it simple.\nI hope that this blog post will be the first of a series this year as I work to finalize the full process of installing and configuring SQL Server with DSC. For now I will share step 1, ensuring the freshly built Windows OS meets the necessary prerequisites for the SQL Server installation. Today we’ll look at installing two Windows Features ‘.NET 3.5 Features’ and ‘Active Directory module for Windows PowerShell’ on our target node.\nThe first thing we’ll want to do is to update the in-box DSC resources, PSDesiredStateConfiguration. This version comes with Windows PowerShell 4.0 and 5.0 however there have been notable improvements that we will want to take advantage of. Since the updated module is available in the PowerShell Gallery we can install it to our workstation using the following (note the name change):\n1 Install-Module PSDSCResources Since we are using the push mode we need to make sure any modules we use to write our configurations are available on our target nodes. I have manually copied the PSDSCResource module to a path within the $env:PSModulePath on the target node so it’ll be available when the configuration is enacted there. There are other ways to handle this including setting up a resource module file share.\nWriting our First Configuration We’re now ready to write our first configuration, although we are still writing PowerShell the syntax is a little different. One of my favorite things about PowerShell is using the command based help to discover how to execute new cmdlets and functions, DSC is no different here. We can use Get-DscResource to list all the resources we have available.\nSince we are going to use the WindowsFeature resource we can find out how to use that by passing in the -Syntax parameter to Get-DscResource.\nThe only required parameter for the WindowsFeature resource is Name so we’ll include that. I also like to include the ‘Ensure’ parameter, it defaults to ‘Present’ but I feel it makes it clearer to specifically define that. I also want to set ‘IncludeAllSubFeature’ to true so those get installed also. The below code is all we need to get started with our first configuration. Apart from the resource blocks that we have already mentioned there are a couple of other important parts to note. First is the keyword Configuration which shows we are writing a DSC Configuration document. In this case I’ve named our configuration ‘SQLServerPreReq’. Secondly, the Node keyword is important, this defines the target node for our configuration. It’s important to remember that this is a simple example, it is possible to pass in multiple node names or to parameterize that node name to make our configuration more useful.\nThe last line of code calls our SQLServerPreReq configuration and specifies the output path. When you call a configuration a MOF file is created which is the document that will be sent to the target node and used to both enact the configuration and monitor for configuration drift (a feature of the pull mode that we’ll save for a future post).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Configuration SQLServerPreReq { Import-DscResource -ModuleName PSDSCResources Node \u0026#39;DscSvr2\u0026#39; { WindowsFeature dotNet { Name = \u0026#39;NET-Framework-Features\u0026#39; Ensure = \u0026#39;Present\u0026#39; IncludeAllSubFeature = $true } WindowsFeature ADPowershell { Name = \u0026#39;RSAT-AD-PowerShell\u0026#39; Ensure = \u0026#39;Present\u0026#39; IncludeAllSubFeature = $true } } } SQLServerPreReq -Output .\\Output\\ Once we have our MOF file the final step is to enact our configuration against our target node. I’m using the -wait and -verbose parameters so that the configuration doesn’t run in the background and we can view the verbose messages on screen as it executes.\n1 Start-DscConfiguration -Path .\\Output\\ -Wait -Verbose Once this runs successfully you can confirm the features are installed using Get-WindowsFeature.\n1 2 Get-WindowsFeature -ComputerName dscsvr2 ` -Name @(\u0026#39;RSAT-AD-PowerShell\u0026#39;,\u0026#39;NET-Framework-Features\u0026#39;) Using Composite Resources This simple configuration shows how you can install two windows features using PowerShell DSC, however it is rather redundant to have to specify separate resource blocks for each feature. Since we updated our in-box DSC Resources to the newer PSDSCResources module we are able to use the new WindowsFeatureSet resource which is an example of a composite resource. We can review the syntax again using Get-DscResource:\n1 Get-DscResource -Name WindowsFeatureSet -Syntax The main difference is that WindowsFeatureSet takes an array of features to install and then translates this to use multiple WindowsFeature resources when the MOF file is created. This allows us to keep our configuration document as tidy and concise as possible.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Configuration SQLServerPreReq_v2 { Import-DscResource -ModuleName PSDSCResources Node \u0026#39;DscSvr2\u0026#39; { WindowsFeatureSet PreReqFeatures { Name = @(\u0026#39;NET-Framework-Features\u0026#39; \u0026#39;RSAT-AD-PowerShell\u0026#39;) Ensure = \u0026#39;Present\u0026#39; IncludeAllSubFeature = $true } } } SQLServerPreReq_v2 -Output .\\Output\\ That’s step one towards automating my SQL Server builds, I’m looking forward to adding more to this series. Thanks again to Garry for picking the perfect T-SQL Tuesday topic to kick off the year with.\n","date":"2019-01-08T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#110-automate-all-the-things/","title":"T-SQL Tuesday #110 - \"Automate All the Things\""},{"content":"2018 has been an amazing year for me. I never imagined when I started this blog in February that I would be going into 2019 even more excited to give back to the community. Public speaking is not something that comes easy to me, and in fact I find it pretty terrifying, which is why I’m amazed at how much I enjoyed presenting this past year. The SQL Server community is definitely a special group of professionals, so thank you to any and all of you who have made this year one of my best yet.\nNow I’m all about goals and targets so in May I wrote a T-SQL Tuesday post about giving back to the community and I set several goals to complete in 2018, let’s review.\n10 blog posts on jesspomfret.com (3 technical) This year end summary makes 12 posts for the year, and 7 were technical posts. My most popular post was ‘Checking backups with dbachecks’. I also contributed a guest post on the dbatools blog covering migrating application databases.\n50 PR’s to dbatools/dbachecks This goal was to encourage contributions to open source projects. My involvement with dbatools and dbachecks is what got me started down this path of blogging and speaking so I wanted to make sure I didn’t lose sight of that.\nSo far in 2018 I have 40 pull requests to dbatools and 12 to dbachecks, so I have met my goal here with a few days to spare. I also contributed to a few other open source projects this year, I had 3 PRs accepted to the Microsoft Sql-Docs repository, and I have 2 open PRs to the SqlServerDsc module.\n1 user group \u0026amp; 1 ‘other’ presentation This goal I surprised myself with, I thought this would be the hardest one to meet and it ended up being the most rewarding. I presented my first ever session to the Ohio North SQL Server User Group in June, it went ok. My topic was data compression and I found myself sweating, rushing and a little short on content. With some amazing feedback and support I added some content and delivered it again at SQL Saturday Columbus and it went much better. I then presented it for a third time to the DBA Fundamentals Virtual chapter, which was good although a little strange not being able to see your audience.\nI also presented a PSPowerHour lightening talk on apply data compression with dbatools, I’d say I well exceeded this goal.\nLooking forward to 2019 My main goal for 2019 is to improve my blog, regular content is going to be the most important part of increasing my traffic so I’m going to aim for 26 blog posts in 2019. I’m also excited to already have two speaking engagements on the calendar as I aim for 5 presentations this year. My data compression session has been accepted to SQL Saturday Cleveland, and a new session I’m working on ‘Installing SQL Server with PowerShell DSC’ was selected for DataGrillen.\nI hope everyone has a great last few days of 2018 and starts 2019 strong.\nCheers!\n","date":"2018-12-29T00:00:00Z","permalink":"https://jpomfret.github.io/p/2018-comes-to-a-close/","title":"2018 Comes to a Close"},{"content":"I came across a situation this week where I wanted to add the option of running an existing script, for a specific server/database combination. The script currently has no parameters and runs against all servers in the environment from a scheduled task. I wanted to make sure that behavior didn’t change. The other requirement was that if I specified Server, Database should be a mandatory parameter and vice versa.\nThe final solution was to add the two parameters to a parameter set and make them both mandatory. I also had to add a different DefaultParameterSet (thanks to Andrew for this idea), otherwise it defaulted to the defined parameter set, meaning the script always required both Server and Database parameters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [CmdletBinding(DefaultParametersetname=\u0026#34;Normal\u0026#34;)] param ( [Parameter(Mandatory = $True, ParameterSetName=\u0026#39;Specific\u0026#39;)] [string]$ServerName, [Parameter(Mandatory = $True, ParameterSetName=\u0026#39;Specific\u0026#39;)] [string]$DatabaseName ) if($PSCmdlet.ParameterSetName -eq \u0026#39;Normal\u0026#39;) { Write-Host \u0026#39;Running without params\u0026#39; } else { Write-Host (\u0026#34;Server: {0}\u0026#34; -f $ServerName) Write-Host (\u0026#34;Database: {0}\u0026#34; -f $DatabaseName) } I saved the above code as params.ps1 to ensure my test cases worked. As you can see in my testing below, I can still run params.ps1 without any parameters, this replicates the current behavior of my nightly job. I can also now pass in Server and Database parameters, if I specify one the script will prompt for the other since they are both required.\n","date":"2018-12-14T00:00:00Z","permalink":"https://jpomfret.github.io/p/psparametersets-mandatory-sometimes/","title":"PSParameterSets: Mandatory sometimes"},{"content":"It’s T-SQL Tuesday time again, I have struggled in the last month or two to get anything up on my blog. Turns out weddings are pretty time consuming ?! Now that I’m happily married and home from an amazing honeymoon in Hawaii it’s back to work on my blog and professional development. Which makes this T-SQL Tuesday topic a perfect one to get back to, so thanks to Malathi Mahadeven (B|T) for hosting this month.\nI feel like with last week’s PASS Summit (I didn’t attend this year so just watching from afar) it makes it even harder than usual to pick just one thing to learn. There are so many things right now that I want to read about or fiddle with.\nI’ve decided to pick a main subject, with an auxiliary bonus area attached - kind of cheating, I know. I’ve been working on a project at work to automate our SQL Server builds with Powershell Desired State Configuration (DSC) so this will be my main goal. I already have a basic understanding of how DSC works and how to install SQL Server with it, I want to improve this knowledge to the point where I can present a session on it.\nThe side goal is docker/containers/kubernetes (maybe), I’m wondering if I could use these to test my DSC configurations, maybe not to install SQL Server (I have no idea though) but I imagine I could configure SQL Server running in a container.\nI saw the tweet below last week from the beard, Rob Sewell, that quoted Bob Ward’s thoughts on learning directions. Feels like this is probably solid advice to justify my side goal.\nhttps://twitter.com/sqldbawithbeard/status/1061032613979267072\nLearning Plan Learn DSC Basics – completed I’ve already started learning DSC, I was lucky enough to take a PowerShell DSC class a couple of months ago and that combined with reading online documentation and blogs has given me a good base.\nResources:\nMicrosoft Docs - https://docs.microsoft.com/en-us/powershell/dsc/overview SQLServerDSC Github - https://github.com/PowerShell/SqlServerDsc DSC Install of SQL Server https://chrislumnah.com/2017/03/07/dsc-install-of-sql-server/ Making modules available in Push mode http://nanalakshmanan.com/blog/Push-Config-Pull-Module/ Learn more about how DSC resources are written and developed This is where I currently am, I have the basics up and running to install SQL Server (blog post coming one day) but there are things I’d like to configure that aren’t currently built into the SQLServerDSC module. Since this is open sourced on github I have the opportunity to learn while doing, I’ve already started working on adding an SQL Agent Operator resource so I can configure an operator during my install.\nDSC, SQL Server and Containers? Can I even use DSC to configure SQL Server running in a container? I have no idea, but I plan on finding out. If this is possible it feels like this would be a really easy way to spin up ‘unconfigured’ SQL Server and test my configurations. If not – hey maybe I learned a bit about containers along the way, and it feels like those are only getting more mainstream.\nFinal goal - Present a DSC session My final goal is to create a \u0026lsquo;Automate your SQL Server Install with DSC\u0026rsquo; session. Presenting on something forces you to learn it in depth, this will be great for myself and hopefully the community. Hopefully it\u0026rsquo;ll make its way to a SQL Saturday next year. The session would be a crash course on DSC specifically to install and configure SQL Server with the end goal of attendees being able to use this process to automate their own builds. Watch this space, currently in the idea phase.\n","date":"2018-11-13T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#108-non-sql-server-technologies/","title":"T-SQL Tuesday #108 - Non SQL Server Technologies"},{"content":"I recently set up an Availability Group with the intent of using the secondary as a read only replica for reporting. We have a few AG\u0026rsquo;s in our environment already but currently none are using this feature.\nI\u0026rsquo;m not going to step through setting up the AG or configuring the readable secondary as there are plenty of good posts out there as well as the official books online documentation.\nOnce my AG was created I set the \u0026lsquo;Connections in Primary Role\u0026rsquo; to \u0026lsquo;Allow read/write connections\u0026rsquo; and the \u0026lsquo;Readable Secondary\u0026rsquo; to \u0026lsquo;Read-intent only\u0026rsquo; as shown below. On a side note it\u0026rsquo;s important to set these for both instances, if you\u0026rsquo;re running with 01B as the Primary after a failover by setting both you\u0026rsquo;ll get the same behavior, with read only connections being routed to the now secondary, 01A server.\nThe other part I needed to set up was read-only routing, this enables SQL Server to reroute those read only connections to the appropriate replica. You can also list the read only replicas by priority if you have multiple available or you can group them to enable load-balancing.\nAlthough this seems to be setup correctly so that connections that specify their application intent of read only will be routed to the secondary node I wanted to prove it. I used the Connect-DbaInstance function from dbatools to connect to the listener name with the -ApplicationIntent property set to \u0026lsquo;ReadOnly\u0026rsquo;.\n$svr = Connect-DbaInstance -SqlInstance AGListenerName ` -Database DatabaseInAG ` -ApplicationIntent ReadOnly\n$svr.Query(\u0026lsquo;Select @@ServerName as ServerName\u0026rsquo;)\nServerName *******01B\nYou can see it routed correctly to 01B which is currently the secondary node. If I don\u0026rsquo;t specify the ApplicationIntent property on the connection it\u0026rsquo;ll be routed to the primary.\n$svr = Connect-DbaInstance -SqlInstance AGListenerName ` -Database DatabaseInAG\n$svr.Query(\u0026lsquo;Select @@ServerName as ServerName\u0026rsquo;)\nServerName *******01A\nThis was a quick and easy way to ensure my read only routing was working as expected, and another great use of dbatools.\n","date":"2018-09-07T00:00:00Z","permalink":"https://jpomfret.github.io/p/testing-availability-group-read-only-routing-with-dbatools/","title":"Testing Availability Group Read-Only Routing with dbatools"},{"content":"I recently gave my first usergroup presentation in Cleveland, closely followed by my first SQL Saturday presentation in Columbus. My chosen topic was row store data compression and I had a few great questions that I plan on answering with blog posts. First up\u0026hellip;\nWhat happens if I use data compression and backup compression, do I get double compression? This is a great question, and without diving too deeply into how backup compression works I\u0026rsquo;m going to do a simple experiment on the WideWorldImporters database. I\u0026rsquo;ve restored this database to my local SQL Server 2016 instance and I\u0026rsquo;m simply going to back it up several times under different conditions.\nAfter restoring the database it\u0026rsquo;s about 3GB in size, so our testing will be on a reasonably small database. It would be interesting to see how the results change as the database size increases, perhaps a future blog post.\nNow I\u0026rsquo;m not sure how to write a blog post without mentioning dbatools, I\u0026rsquo;m using my favourite PowerShell module to check current database compression (Get-DbaDbCompression), apply data compression (Set-DbaDbCompression) and to create the backups with and without compression (Backup-DbaDatabase).\nThe script I used to run through this experiment is available for you to test out on my github and the results are below:\n[table id=1 /]\nWe can clearly see that using backup compression gives us a huge space savings. On our database where none of the objects are compressed we get a 78% reduction in the size of the backup file. When all our objects are row compressed we get a 70% savings and even when all our objects are page compressed we still get a 60% reduction in size when we apply backup compression.\nNow, if we compare the difference in sizes for the three backups that used backup compression, we do get a small amount of additional space savings by using data compression in combination with backup compression. The backup file is 7% smaller when the database objects are row compressed and 6% smaller when page compression is applied, however, these savings aren\u0026rsquo;t nearly as significant as as just comparing whether backup compression is used or not.\nSo to answer the question, we don\u0026rsquo;t get double the compression by using both data and backup compression, but whether we use data compression or not within our database using backup compression will get you a pretty significant space saving when looking at the size of the backup file on disk.\n","date":"2018-08-20T00:00:00Z","permalink":"https://jpomfret.github.io/p/data-compression--backup-compression-double-compression/","title":"Data Compression + Backup Compression = Double Compression?"},{"content":"As soon as I saw Bert Wagner (t|b) post his T-SQL Tuesday topic last week I knew this was going to be a great one. I’m really looking forward to reading about everyone’s favorite code snippets so thanks Bert for hosting and choosing a fantastic subject!\nA lot of the code I can\u0026rsquo;t live without is either downloaded from the community (e.g. sp_whoisactive, sp_indexinfo, sp_blitz), or very specific to my workplace so I\u0026rsquo;m going to share some code that I\u0026rsquo;ve been meaning to blog about.\nI’ve been using this at work recently and it also relates to the presentation I gave at the ONSSUG June meeting around data compression. The beginnings of this script originated online as I dug into learning about the DMVs that related to objects and compression and then customized for what I needed.\nIf you run the below as is it will provide basic information about all objects in your database, except those in the \u0026lsquo;sys\u0026rsquo; schema, along with their current size and compression level.\nSELECT schema_name(obj.SCHEMA_ID) as SchemaName, obj.name as TableName, ind.name as IndexName, ind.type_desc as IndexType, pas.row_count as NumberOfRows, pas.used_page_count as UsedPageCount, (pas.used_page_count * 8)/1024 as SizeUsedMB, par.data_compression_desc as DataCompression FROM sys.objects obj INNER JOIN sys.indexes ind ON obj.object_id = ind.object_id INNER JOIN sys.partitions par ON par.index_id = ind.index_id AND par.object_id = obj.object_id INNER JOIN sys.dm_db_partition_stats pas ON pas.partition_id = par.partition_id WHERE obj.schema_id \u0026lt;\u0026gt; 4 \u0026ndash; exclude objects in \u0026lsquo;sys\u0026rsquo; schema \u0026ndash;AND schema_name(obj.schema_id) = \u0026lsquo;schemaName\u0026rsquo; \u0026ndash;AND obj.name = \u0026rsquo;tableName\u0026rsquo; ORDER BY SizeUsedMB desc\n(This is also available in my GitHub Tips and Scripts Repo)\nNow this T-SQL is great for a quick look at one database, but what if I want to run this script against every database in my environment? Well I popped over to PowerShell, fired up dbatools and ran the following:\nget-command -Module dbatools -Name *compression*\nBad news, there was no Get-DbaDbCompression, there were commands for compressing objects (Set-DbaDbCompression) and for getting suggested compression setting based on the Tiger Teams best practices (Test-DbaDbCompression), but nothing to just return the current compression status of the objects.\nWhat’s more exciting than just using the greatest PowerShell module ever created? Making it better by contributing! So I made sure I had the latest development branch synced up and got to work writing Get-DbaDbCompression. This has now been merged into the main branch and is therefore available in the Powershell gallery, so if your dbatools module is up to date you can now run the following to get the same information as above from one database:\nGet-DbaDbCompression -SqlInstance serverName -Database databaseName\nOr go crazy and run it against a bunch of servers.\n$servers = Get-DbaRegisteredServer -SqlInstance cmsServer | select -expand servername $compression = Get-DbaDbCompression -SqlInstance $servers $compression | Out-GridView\nI hope this post might come in handy for anyone who is curious about data compression in their environments. Both the T-SQL and PowerShell versions provide not just the current compression setting but the size of the object too. Useful if you are about to apply compression and would like a before and after comparison to see how much space you saved.\n","date":"2018-07-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#104-code-you-cant-live-without/","title":"T-SQL Tuesday #104 – Code you can't live without"},{"content":"Well tonight marks three weeks since I gave my first user group presentation and you know what, it’s been a total whirlwind since then so I’ve had little time to reflect. Myself and the fiancé closed on our first house and my parents flew in to visit. It’s very useful to have people on hand the first couple of weeks so we didn’t feel like completely unqualified homeowners. I spent some time right after the presentation to start breathing again and to jot down some thoughts, but this is the first chance I’ve had to report back.\nTL;DR I didn’t die, the SQL Server community is fantastic and I have amazing supportive friends.\nhttps://twitter.com/Pittfurg/status/1004125722082934784\nWhy Present?\nThe answer to this is twofold. Firstly this year I’ve challenged myself to get more involved in the SQL Server community. For several years now I’ve attended user group meetings, SQL Saturday’s and even made it to the PASS Summit a couple of times, but I’ve never contributed anything. It’s been all take. Last year I got involved with dbatools and that started my quest to return the favour. I’m still currently wrestling with the ideas of “who wants to listen to what I have to say” and “do I really have anything to contribute anyway,” but I’m doing my best to keep one foot in front of the other and see what happens.\nSecondly, whenever I review my strengths and weaknesses at annual review time, communication, or more precisely public speaking is always something that I consider a weakness. I’m not sure of a better way to improve in this area than to put myself out there and practice, so here’s to some professional development. I’m certain that gaining some knowledge, experience and confidence in this area will help me in many areas of my life.\nWhat to Present?\nI ended up presenting on SQL Server data compression. I talked about the types of data compression, and how the internals work before focusing on what you can compress, how to compress and how to decide what should be compressed. This topic stemmed mainly from an issue at work where data compression was implemented with a large performance benefit. This issue at work also encouraged me to spend some time looking at dbatools and compression. There were existing commands for Set-DbaDbCompression and Test-DbaDbCompression that I added some improvements to, and then I added Get-DbaDbCompression.\nThe culmination of both the issue at work and working on dbatools commands for compression left me feeling like this was a great topic to share. Since 2016 SP1 data compression is now a standard level feature, opening up the possibilities to a lot more people.\nImprovements\nOverall I think the presentation went well, I delivered most of what I had planned on saying and my demo’s did a decent job of explaining the process for deciding what to compress and then applying compression through T-SQL, SSMS and PowerShell. I was lucky to have some great friends in the audience (Andrew, Drew and Erin) who asked great questions which helped me to drive home certain points.\nMy timing was definitely a bit off. I’d prepared for what I thought would be 45-60 minutes of content and it was a bit shorter than that. I plan on adding some additional content and delivering slightly slower the next time I give this talk to fix this problem.\nI got some great feedback both from the speaker evaluations and from Erin Stellato who went above and beyond my request for any tips and feedback she may have. I’ll make sure to incorporate some real life stories on where compression has had an impact as well as adding some more demos.\nWhat’s next?\nThat’s right, there is a next! I took a chance and submitted my session to SQL Saturday Columbus and was lucky enough to be selected. I’ve been to quite a few SQL Saturdays in Cleveland, Columbus, Pittsburgh and even Minneapolis, but this will be my first as a speaker. If you’re in the area on July 28th and want to learn more about data compression I’ll be on at 8:30am in Room 6.\nI’m also going to work on some blog posts around data compression and the dbatools commands so watch out for that also.\n","date":"2018-06-27T00:00:00Z","permalink":"https://jpomfret.github.io/p/first-user-group-presentation-i-survived/","title":"First User Group Presentation - I Survived!"},{"content":"I was troubleshooting an issue last week which led to me firing up extended events to look at records being written to the transaction log, I typed into the search bar ‘Transaction’ hoping to find something that would do the trick and didn’t quite find what I was looking for.\nAfter a few more failed attempts I headed to the internet and found a post by Paul Randal describing exactly what I needed for this situation, using the [sqlserver].[transaction_log] event. Hold on, that’s exactly what I searched for. I ran the T-SQL within his blog post, the event was successfully created and gave me the information I was looking for.\nI then noticed someone asked in the comments whether it was a bug that the transaction_log event doesn’t show up in the XEvents GUI and Paul had replied:\nIt took me a second to find it but by default there is a filter on the ‘Channel’ column that doesn’t include ‘Debug’. Selecting that gives you a whole host of new XEvents to investigate (and use carefully, for example the transaction_log event can generate a lot of activity).\nI’m not sure how often I’ll need these ‘Debug’ events, but it sure is nice to know they exist. I feel like there should be some notation in the GUI that there is a filter being applied, similar to the icon in Excel when you have something filtered.\n","date":"2018-05-29T00:00:00Z","permalink":"https://jpomfret.github.io/p/extended-events-hidden-treasure/","title":"Extended Events - Hidden Treasure"},{"content":"First of all thanks to Riley Major (b|t) for hosting this months T-SQL Tuesday. This is a great topic for me as I have mentioned before my goal this year is to work on getting my blog going and starting to give back to this amazing community. I am always impressed by the content people produce and share, whether it be blog posts, presentations or just snippets on Twitter and I have learnt so much in my career so far from these sources. Honestly I often feel like I don\u0026rsquo;t have anything worth sharing and that what I know is obvious and everyone would know it. Then I think back to that first Ohio North SQL Server User Group presentation I went to where Allen White (b|t) gave his now famous introduction, everyone has something to teach, so here goes.\nI am all about goals and since we\u0026rsquo;re already almost six months into 2018 I\u0026rsquo;m going to set some goals down on paper that will force me out of my comfort zone. I have been working towards these haphazardly so far, so I hope that writing these out will hold me accountable.\nBlog Posts I started this blog back in February of this year and so far have managed four posts, including this one and one guest post on the dbatools blog. My goal for the year is to have 10 posts on this blog, so I have 6 left to write. My aim is to also write some more technical posts. I have been leaning on the T-SQL Tuesday topics so far, so I\u0026rsquo;ll aim for 3 of the 6 to be more technical in nature.\nGithub So far this year I\u0026rsquo;ve had 22 pull requests (PRs) merged into the dbatools module, and 8 PRs merged into the dbachecks module. Contributing to these modules has been so much fun and has really improved both my PowerShell and Pester skills. I\u0026rsquo;ll aim to get this up to 50 PR\u0026rsquo;s by the end of 2018, so 20 to go here.\nPresenting Now this third section is the one that produces the most anxiety. I have thought about this for a while now and it\u0026rsquo;s time. My goal here is to deliver one presentation to my local user group (it\u0026rsquo;s actually scheduled for June 5th, 28 days away but who\u0026rsquo;s counting\u0026hellip;), and one to another audience. I imagine it\u0026rsquo;ll be the same presentation, I\u0026rsquo;ve been working hard on one for the user group on data compression, but I would love to deliver this to a SQL Saturday or some other local conference. I was hoping I could present to the user group and then submit to a local SQL Saturday but my best chance of accomplishing this by the end of 2018 is to submit to SQL Saturday Columbus which I need to do before May 29th.\nGoals What\u0026rsquo;s better than a checklist? I hope 2018 is the start of many years of giving back and perhaps my 10th (or more) post of the year will be an update on my progress- watch this space.\n10 blog posts on jesspomfret.com 3 technical posts 50 PR\u0026rsquo;s to dbatools/dbachecks 1 user group presentation 1 \u0026lsquo;other\u0026rsquo; presentation ","date":"2018-05-09T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#102-giving-back/","title":"T-SQL Tuesday #102 - Giving Back"},{"content":"Thanks to Jens Vestergaard for hosting T-SQL Tuesday #101. When I saw the topic for this month’s T-SQL Tuesday, I knew instantly which tool I would write about. Although there are many great tools out there that make my job as a DBA easier (and I’m excited to read the summary for this month to see what everyone else leans on), there is one that has fundamentally changed far more than just my work day. First of all I love PowerShell; the ability to make my daily tasks both repeatable and automated is something that has always appealed to me. Then I found dbatools, which combines everything I love about PowerShell into an ever-evolving open source module.\nOnce you install the module you can run the following to list all the available commands in your toolbox. It’s a good idea to keep your copy of the module updated and check this often as people are always adding new commands.\nGet-Command -Module dbatools -CommandType Function | Out-GridView\nHaving hundreds of commands can be a little overwhelming. In no particular order, these are the top five that I use most or that save me the most time.\nTest-DbaSqlBuild When I found this command I couldn’t have been more excited. My day-to-day job requires the care and watering of over 100 SQL Server instances of varying versions. Using this command you can get the current build of all your instances and then compare that to the most recent available. There are also parameters for how far you want to be from the latest version. Setting the -latest switch means just that, your server will only be seen as compliant if it’s on the latest release, passing in -1CU means that it can be no more than 1 cumulative update behind.\nThis snippet takes the registered instances from our central management servers and pipes them into Test-DbaSqlBuild to determine if they are on the latest version. It creates an easy list of what needs patched.\nGet-DbaRegisteredServer -SqlInstance nonProdServers, prodServers | Test-DbaSqlBuild -Latest\nGet-DbaDiskSpace This is a great command to have handy. Pass in one or many server names and it returns the current size, amount of space available and blocksize for all drives and mount points on that server. There are also switches to look for any drives that have SQL files on, or to check the filesystem fragmentation levels. One great use of this command is to pass in a list of servers and filter for drives under 5% free space. This is a great proactive check of where action may be needed soon.\nGet-DbaDiskSpace -ComputerName $servers | Where-Object {$_.PercentFree -lt 5}\nCopy-DbaDatabase This command is used to move databases from one instance to another. You can use either a backup/restore method or by detaching and re-attaching the files. Check out my post on the dbatools blog for a more detailed look on how I used this to save a lot of time on a recent project at work.\nCopy-DbaDatabase -Source SourceServer -Destination DestinationServer ` -Database MigratingDatabase -BackupRestore -NetworkShare \\\\fileshare\\\nRepair-DbaOprhanedUser This command is pure wizardry and Steve Jones has a more extensive post on this command, but it does exactly what it says. It syncs up the SIDs for SQL logins that you’ve migrated from one instance to another. This is not a difficult task, however this one line fixes any orphaned logins on your whole instance. Extremely useful if you are migrating a lot of databases to new servers.\nRepair-DbaOrphanUser -SqlInstance serverName\nWrite-DbaDataTable The final command for my top five takes an object in PowerShell and uses SQL bulk copy to insert it into a table in a SQL Server database. Using the -AutoCreateTable switch will do just that, if the table doesn’t exist it will be created. One thing to watch is this will be a heap so if you’re going to use this table going forward building the table ahead of time with appropriate indexes and keys is probably advisable. However, this one line can be very useful for quickly throwing results into a table to save or analyze further.\n$results | Write-DbaDataTable -SqlInstance serverName -Database databaseName -Table tableName -AutoCreateTable\nThe Second reason dbatools is my favorite tool is all the other things I’ve gained and learnt from this module. It’s been almost one year since my first pull request to dbatools, and at that point I had a decent handle on PowerShell but git was a foreign language. Guided by Chrissy LeMaire (t) and some other folks from the slack channel I got the repo forked, created my own branch and then submitted a PR to get my contributions merged in. Since then I’ve contributed multiple more PRs, everything from small fixes to the command based help, to writing a brand new command (Get-DbaDbCompression will be released soon!).\nThis tool not only gives you hundreds of commands to make your job easier, it encourages you to branch out and get involved in a truly special community. You will meet some brilliant people to bounce ideas off, learn new skills like github, integration tests or even continuous integration and development, all while giving back to the amazing community that surrounds the SQL Server ecosystem. This blog is the start of my attempt to give back while furthering my understanding of certain topics. In June I’ll be stepping even further outside of my comfort zone by presenting at my local user group on data compression, and of course that’ll feature some dbatools related demos.\n","date":"2018-04-10T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#101-the-multitool-of-my-dba-toolbox/","title":"T-SQL Tuesday #101 - The Multitool of my DBA toolbox"},{"content":"Folks, there is something fantastic coming from the creators of dbatools!\nChrissy LeMaire (blog|twitter) and Rob Sewell (blog|twitter) just announced something big at SQLBits 2018, a new PowerShell module that combines dbatools with Pester to ensure your environment is \u0026ldquo;as expected\u0026rdquo;. I\u0026rsquo;ve been lucky enough to get to know both Chrissy and Rob by contributing to dbatools and when they introduced me to this new module I instantly saw a lot of potential.\nI\u0026rsquo;m going to start off with just a small way to gain some value from dbachecks, ensuring your backups are completing with the frequency you expect.\nThe module is hosted on GitHub which means you can fork and contribute to it just as you would with any other open source project. There is also a lot of useful information out there including the readme. Reviewing this readme is an important first step as there are a couple of prerequisites and some potential caveats when you go to update the module.\nYou can download a copy of the module from the PowerShell Gallery (if this doesn\u0026rsquo;t work for you due to corporate firewalls, PowerShell version etc. head back to the readme for more ways to get the module):\nInstall-Module dbachecks\nFirst off let\u0026rsquo;s take a look at Get-DbcCheck to look for checks we may want to implement: Each check has one unique tag which basically names the check and then a number of other tags that can also be used to call a collection of checks.\nFor this example we are going to use several checks to ensure that we meet the following requirements:\nFull backup once a week - using LastFullBackup Differential backup once a day - using LastDiffBackup Log backup every hour - using LastLogBackup Since each of the three checks we want to run also have the LastBackup tag we can use that to call the collection of checks at once.\nThere are many ways to point dbachecks at your instances, for this simple example we\u0026rsquo;ll just pass in one server name to check.\n$server = \u0026ldquo;ServerName\u0026rdquo; Invoke-DbcCheck -SqlInstance $server -Check LastBackup\nAs you can clearly see from the test results there is a lot of red, meaning I\u0026rsquo;m not meeting backup requirements. However looking closer at the context we can see that the check is not configured for my specific needs \u0026ldquo;StackOverflow full backups on Server should be less than 1 days\u0026rdquo;, but I only require a full backup within 7 days.\nThe checks are set up in a way that make them extremely flexible. You can configure them to meet your needs exactly. We can use Get-DbcConfig to review the backup configurations. Here you can see we\u0026rsquo;re looking for full backups every 1 day (policy.backup.fullmaxdays), differentials every 25 hours (policy.backup.diffmaxhours) and log backups every 15 minutes (policy.backup.logmaxminutes).\nLet\u0026rsquo;s change these configuration properties to match our requirements of a full backup within the last 7 days and a log backup in the last 60 minutes.\nSet-DbcConfig -Name policy.backup.fullmaxdays -Value 7 Set-DbcConfig -Name policy.backup.logmaxminutes -Value 60\nNow that the configuration is setup correctly we can rerun and confirm our environment backups are in the green.\nValidating your backups are running is just one small example of how you can utilize dbachecks to keep your environment in line. In fact this is just the tip of the iceberg, there are 80 checks as of writing this post as well as multiple ways to display the results (including a pretty impressive PowerBi dashboard - Cláudio Silva has a great post on that).\nI hope this has peaked your interest in dbachecks, I suggest heading over to the readme to learn more or download a copy and get checking right away!\nUseful links:\ndbachecks on github dbachecks blog Introducing dbachecks - including links to a plethora of blog posts by other contributors ","date":"2018-02-22T00:00:00Z","permalink":"https://jpomfret.github.io/p/checking-backups-with-dbachecks/","title":"Checking backups with dbachecks"},{"content":"First off, welcome to my first T-SQL Tuesday which seems like the perfect first blog post to introduce myself and my non-SQL Server life. Starting this blog and becoming more involved with the SQL Server community (read speaking) is my goal for 2018 so here goes nothing. Thanks to Aaron Bertrand for hosting and picking a great topic.\nI was born in Oxford, England before my family moved to the small market town of Chippenham, Wiltshire where I was raised. Leading to my main passion being proper football. We can argue about why this is the proper football in a later post perhaps (hint: you use your feet to kick a ball, no hands or egg-shaped objects here).\nMy earliest memories involve playing and watching football. When I was in primary school (equivalent of elementary school in the USA) my mum fought with the headmistress until I was allowed to play on the boy’s team. The boys weren’t particularly happy until they realized I could hold my own.\nI then played for Chippenham Town Ladies for many seasons while I was in Secondary School and earned appearances for the Wiltshire County team. Next I joined the Bristol Academy of Sport at Filton College for the 2003/2004 season and played for Bristol Rovers Ladies. This set up my big move across the pond as I was recruited to join The University of Akron Zips in 2005. After four fantastic years of playing football every day and traveling around the United States, I graduated in 2009 with a degree in Information Systems and eBusiness, followed by an MBA. Then in 2011 accidentally became a DBA for a direct marketing company in Akron.\nI have since moved employers but still reside in Northeast Ohio as a SQL Server DBA. My days of playing football are behind me, but I now spend more time than I probably should watching the English Premier League and worrying about my fantasy (proper) football team. I also enjoy watching US sports, hiking, Crossfiting and travelling with my partner, Kelcie.\n[gallery type=\u0026ldquo;rectangular\u0026rdquo; link=\u0026ldquo;none\u0026rdquo; size=\u0026ldquo;medium\u0026rdquo; columns=\u0026ldquo;2\u0026rdquo; ids=\u0026ldquo;52,53,54\u0026rdquo;]\n","date":"2018-02-13T00:00:00Z","permalink":"https://jpomfret.github.io/p/t-sql-tuesday-#99-door-%231/","title":"T-SQL Tuesday #99 - Door #1"}]